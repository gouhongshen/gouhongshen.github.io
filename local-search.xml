<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>算法：动态规划专题</title>
    <link href="/2022/07/07/%E7%AE%97%E6%B3%95%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%93%E9%A2%98/"/>
    <url>/2022/07/07/%E7%AE%97%E6%B3%95%EF%BC%9A%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B8%93%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h3 id="力扣-1235-规划兼职工作"><a href="#力扣-1235-规划兼职工作" class="headerlink" title="力扣 1235-规划兼职工作"></a><a href="https://leetcode.cn/problems/maximum-profit-in-job-scheduling/">力扣 1235-规划兼职工作</a></h3><p>题目简介</p><blockquote><p>每个任务包括三个参数：开始时间、结束时间、报酬。现在有一堆任务，你可以从中选择任务来做，使得报酬最大，但所选择的任务的工作时间不能有重叠，如下图，选择最底下三个：<br><img src="/img/leetcode/1.png" alt="图 1"></p></blockquote><br><p>题目解析</p><blockquote><p>这个题目一看就知道用动态规划，但是状态转移方程没那么简单，最开始的想法很简单：<br>对于某个任务，它可以从在它之前的、与它不冲突的任务转移过来，那么只需要记录从这些任务转移来的最大报酬，再加上该任务的报酬，那么就能得出，做完该任务时取得的最大报酬，如下图，未连线的任务之间表示冲突：<br><img src="/img/leetcode/2.png" alt="图 2"><br>F 任务可以分别由 C、D、E 任务转移过来。那么将这些任务以开始时间从小到大排序后，就能写出如下的代码：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// job = &#123; startTime, endTime, profits &#125;</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">foo</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; jobs)</span> </span>&#123;<br>    <span class="hljs-built_in">sort</span>(jobs.<span class="hljs-built_in">begin</span>(), jobs.<span class="hljs-built_in">end</span>()); <span class="hljs-comment">// 以开始时间递增排序</span><br>    vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; <span class="hljs-built_in">from</span>(jobs.<span class="hljs-built_in">size</span>());<br><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; jobs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>        <span class="hljs-comment">// 因为 job 以开始时间排序，所以可以二分查找，</span><br>        <span class="hljs-comment">// 找到第一个大于等于任务 i 结束时间的任务</span><br>        <span class="hljs-comment">// 查找到第一个大于等于任务 i 结束时间开始的任务后，后面的都可以添加到 i 到 from 了</span><br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = i + <span class="hljs-number">1</span>; j &lt; jobs.<span class="hljs-built_in">size</span>(); j++) &#123;<br>            <span class="hljs-comment">// 如果任务 j 在任务 i 结束后开始，</span><br>            <span class="hljs-comment">// 那么任务 j 可以从任务 i 转移过来</span><br>            <span class="hljs-keyword">if</span> (jobs[i][<span class="hljs-number">1</span>] &lt;= jobs[j][<span class="hljs-number">0</span>]) &#123;<br>                from[j].<span class="hljs-built_in">push_back</span>(i);<br>            &#125;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// dp 过程</span><br>    <span class="hljs-type">int</span> dp[jobs.<span class="hljs-built_in">size</span>()] = &#123; <span class="hljs-number">0</span> &#125;;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; jobs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> f: from[i]) &#123;<br>            dp[i] = <span class="hljs-built_in">max</span>(dp[i], dp[f]);<br>        &#125;<br>        dp[i] += jobs[i][<span class="hljs-number">2</span>];<br>    &#125;<br><br>    cout &lt;&lt; *<span class="hljs-built_in">max_element</span>(dp, dp + jobs.<span class="hljs-built_in">size</span>()) &lt;&lt; endl;<br>&#125;<br></code></pre></td></tr></table></figure><blockquote><p>上面代码正确但不够快，时间复杂度未 $O(n^2)$，过不了该题。<br><u>在一次分析可知，里面存在重复计算</u>，比如从 E 转移到 F 就已经计算了从 C，D 转移来的报酬，不需要再单独计算从它们转移来的报酬了，因为比如是小于从 E 转移来的。<br><u>事实上这个问题类似背包问题</u>，对于一个任务 i，你可以选择做也可以选择不做，若选择不做，那么截止到任务 i 时的报酬 dp[i] 就需要从 dp[i-1] 转移过来，如果选择做，那么 dp[i] 需要从最近一次且与任务 i 不冲突的任务，假设为 j，转移过来，即 dp[i] &#x3D; dp[j] + profit[i]，最后在两者中取最大即可。找最近一次且不冲突的任务，可以采用二分，只需要将任务以结束时间递增排序，然后在其中找第一个小于等于当前任务开始时间的任务即可，所以可以写出如下的代码：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// job = &#123; startTime, endTime, profits &#125;</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">foo</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; jobs)</span> </span>&#123;<br>    <span class="hljs-comment">//以结束时间递增排序</span><br>    <span class="hljs-built_in">sort</span>(jobs.<span class="hljs-built_in">begin</span>(), jobs.<span class="hljs-built_in">end</span>(), <br>    [](<span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; a, <span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; b) -&gt;<span class="hljs-type">bool</span> &#123;<br>            <span class="hljs-keyword">return</span> a[<span class="hljs-number">1</span>] &lt; b[<span class="hljs-number">1</span>];<br>            &#125;);<br><br>        <span class="hljs-comment">// dp[i] = p 截至任务 i 时的最大报酬</span><br>        unordered_map&lt;<span class="hljs-type">int</span>, <span class="hljs-type">int</span>&gt; dp;<br>        dp[<span class="hljs-number">0</span>] = jobs[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>];<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; jobs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>            vector&lt;<span class="hljs-type">int</span>&gt; tmp = &#123; jobs[i][<span class="hljs-number">1</span>] &#125;;<br>            <span class="hljs-keyword">auto</span> last = <span class="hljs-built_in">upper_bound</span>(jobs.<span class="hljs-built_in">begin</span>(), jobs.<span class="hljs-built_in">end</span>(), tmp,<br>                [](<span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; a, <span class="hljs-type">const</span> vector&lt;<span class="hljs-type">int</span>&gt;&amp; b) -&gt; <span class="hljs-type">bool</span> &#123;<br>                    <span class="hljs-keyword">return</span> a[<span class="hljs-number">0</span>] &lt; b[<span class="hljs-number">0</span>];<br>                &#125;);<br><br>            <span class="hljs-comment">// 找到的是第一个大于目标的值，prev(last) 表示取前一个</span><br>            <span class="hljs-comment">// 就是最后一个小于等于目标值的了</span><br>            dp[i] = <span class="hljs-built_in">max</span>(dp[<span class="hljs-built_in">prev</span>(last) - jobs.<span class="hljs-built_in">begin</span>()] + jobs[i][<span class="hljs-number">2</span>], dp[i - <span class="hljs-number">1</span>]);<br>        &#125;<br><br>        cout &lt;&lt; dp[jobs.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>] &lt;&lt; endl;<br>&#125;<br></code></pre></td></tr></table></figure><p>其他知识点</p><blockquote><p>std::upper_bound(begin, end, val, pred) 函数<br>该函数在序列中二分查找，返回第一个大于目标值 val 的迭代器，用法可以看上面</p></blockquote><blockquote><p>另外，还可以自己写类似上面的二分算法：</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">// 返回第一个大于目标值 val 的元素的下标</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">BinarySearch</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; jobs, <span class="hljs-type">int</span> val)</span> </span>&#123;<br>        <span class="hljs-comment">// find the smallest v so that v &gt; val</span><br>        <span class="hljs-type">int</span> l = <span class="hljs-number">0</span>, r = jobs.<span class="hljs-built_in">size</span>(), mid;<br>        <span class="hljs-keyword">while</span> (l &lt; r) &#123;<br>            mid = l + (r - l) / <span class="hljs-number">2</span>;<br>            <span class="hljs-keyword">if</span> (jobs[mid][<span class="hljs-number">0</span>] &lt;= val) l = mid + <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">else</span> r = mid;<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> l;<br>    &#125;<br></code></pre></td></tr></table></figure><blockquote><p><u>个人决定二分算法最麻烦的地方在于，应该返回什么，这里可以这样分析</u>：<br>从 while 循环退出有两种可能:</p><blockquote><p>l &#x3D;&#x3D; r. 那么到达该种局面有两种情况：<br>A. $V_{mid} &lt;&#x3D; val$ 即 mid &#x3D; r - 1，很明显应该返回 r&#x2F;l<br>B. $V_{mid} &gt; val$ 即 mid &#x3D; l，很明显，应该返回 l<br>综上，返回 l</p></blockquote></blockquote><blockquote><blockquote><p>l &#x3D;&#x3D; r + 1. 那么达到该种局面有两种情况：<br>A. $V_{mid} &lt;&#x3D; val$ 即 mid &#x3D; r，不可能<br>B. $V_{mid} &gt; val$ 即 mid &#x3D; l - 1，不可能</p></blockquote></blockquote><h3 id="力扣-894-所有可能的真二叉树"><a href="#力扣-894-所有可能的真二叉树" class="headerlink" title="力扣 894-所有可能的真二叉树"></a><a href="https://leetcode.cn/problems/all-possible-full-binary-trees/submissions/">力扣 894-所有可能的真二叉树</a></h3><p><a href="https://leetcode.cn/problems/unique-binary-search-trees-ii/">相似题目</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * Definition for a binary tree node.</span><br><span class="hljs-comment"> * struct TreeNode &#123;</span><br><span class="hljs-comment"> *     int val;</span><br><span class="hljs-comment"> *     TreeNode *left;</span><br><span class="hljs-comment"> *     TreeNode *right;</span><br><span class="hljs-comment"> *     TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125;</span><br><span class="hljs-comment"> *     TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125;</span><br><span class="hljs-comment"> * &#125;;</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br><br>    <span class="hljs-comment">// 加一个哈希，就变成了自顶向下的动态规划了（记忆化搜索）</span><br>    unordered_map&lt;<span class="hljs-type">int</span>, vector&lt;TreeNode*&gt;&gt; history;<br><br>    <span class="hljs-function">vector&lt;TreeNode*&gt; <span class="hljs-title">allPossibleFBT</span><span class="hljs-params">(<span class="hljs-type">int</span> n)</span> </span>&#123;<br>        <span class="hljs-keyword">if</span> (history.<span class="hljs-built_in">count</span>(n) != <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> history[n];<br>        vector&lt;TreeNode*&gt; roots;<br>        <span class="hljs-comment">// 如果只有一个节点，就不需要拆分了，只有一种情况</span><br>        <span class="hljs-comment">// 老实说，当有三个节点时，也不用拆分，也只有一种情况</span><br>        <span class="hljs-comment">//   *</span><br>        <span class="hljs-comment">//  / \</span><br><span class="hljs-comment">        // *   *</span><br>        <span class="hljs-keyword">if</span> (n == <span class="hljs-number">1</span>) &#123;<br>            roots.<span class="hljs-built_in">push_back</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">TreeNode</span>(<span class="hljs-number">0</span>));<br>            <span class="hljs-keyword">return</span> roots;<br>        &#125;<br><br>        <span class="hljs-comment">// 因为一个节点的子节点个数要么为 0, 要么为 2，这是个递归定义</span><br>        <span class="hljs-comment">// 所以该子树（加上该节点）的节点个数为奇数</span><br>        <span class="hljs-comment">// i 表示以当前节点为根（总结点个数为参数 n）时，左子树节点个数</span><br>        <span class="hljs-comment">// allPossibleFBT(i) 也就表示总结点个数为 i 时，满足题目条件的所有二叉树集合</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; n - <span class="hljs-number">1</span>; i += <span class="hljs-number">2</span>) &#123;<br>            <span class="hljs-keyword">auto</span> leftRoots = <span class="hljs-built_in">allPossibleFBT</span>(i);<br>            <span class="hljs-keyword">auto</span> rightRoots = <span class="hljs-built_in">allPossibleFBT</span>(n - i - <span class="hljs-number">1</span>);<br>            <br>            <span class="hljs-comment">// 当前节点的左右子树的所有可能都求出来了，</span><br>            <span class="hljs-comment">// 那么以当前节点（下面的 new）为根的所有可能的二叉树，就可以通过排列组合计算了</span><br>            <span class="hljs-comment">// 由于左右子树的结构都符合题目，组合出来自然也符合题目</span><br>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> left: leftRoots) &#123;<br>                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> right: rightRoots) &#123;<br>                    <span class="hljs-keyword">auto</span> node = <span class="hljs-keyword">new</span> <span class="hljs-built_in">TreeNode</span>(<span class="hljs-number">0</span>);<br>                    node-&gt;left = left;<br>                    node-&gt;right = right;<br>                    roots.<span class="hljs-built_in">push_back</span>(node);<br>                &#125;<br>            &#125;<br>        &#125;<br><br>        history[n] = roots;<br>        <span class="hljs-keyword">return</span> roots;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="力扣-剑指-Offer-II-091-粉刷房子"><a href="#力扣-剑指-Offer-II-091-粉刷房子" class="headerlink" title="力扣 剑指 Offer II 091. 粉刷房子"></a><a href="https://leetcode.cn/problems/JEj789/">力扣 剑指 Offer II 091. 粉刷房子</a></h3><p>题目简介<br><img src="/img/leetcode/3.png"></p><p>先上题解</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">dynamicProgramming</span><span class="hljs-params">(vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt;&amp; costs)</span> </span>&#123;<br>        <span class="hljs-comment">// dp[i][0], dp[i][1], dp[i][2]</span><br>        <span class="hljs-comment">// 分别表示使用 0，1，2 三种颜色刷完房子 i 时的最小开销</span><br>        <span class="hljs-comment">// dp[i][0] = min&#123;dp[i-1][1], dp[i-1][2]&#125; + cost[i][0]</span><br>        <span class="hljs-comment">// ...</span><br>        vector&lt;vector&lt;<span class="hljs-type">int</span>&gt;&gt; dp;<br>        <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">tmp</span><span class="hljs-params">(<span class="hljs-number">3</span>)</span></span>;<br>        dp.<span class="hljs-built_in">resize</span>(costs.<span class="hljs-built_in">size</span>(), tmp);<br>        dp[<span class="hljs-number">0</span>] = costs[<span class="hljs-number">0</span>];<br><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; costs.<span class="hljs-built_in">size</span>(); i++) &#123;<br>            dp[i][<span class="hljs-number">0</span>] = <span class="hljs-built_in">min</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">1</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">2</span>]) + costs[i][<span class="hljs-number">0</span>];<br>            dp[i][<span class="hljs-number">1</span>] = <span class="hljs-built_in">min</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">0</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">2</span>]) + costs[i][<span class="hljs-number">1</span>];<br>            dp[i][<span class="hljs-number">2</span>] = <span class="hljs-built_in">min</span>(dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">1</span>], dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">0</span>]) + costs[i][<span class="hljs-number">2</span>];<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">min</span>(dp.<span class="hljs-built_in">back</span>()[<span class="hljs-number">0</span>], <span class="hljs-built_in">min</span>(dp.<span class="hljs-built_in">back</span>()[<span class="hljs-number">1</span>], dp.<span class="hljs-built_in">back</span>()[<span class="hljs-number">2</span>]));<br>    &#125;<br></code></pre></td></tr></table></figure><p>为什么可以这样做呢？看下图分析：<br><img src="/img/leetcode/4.png"><br>可以看到房子乙无论染哪种颜色，都有两种可能的开销，总共有六种情况，如果只有两个房子，那么比较这六种就能得出最小开销。现在增加一个房子丙，对于丙的任一一种染色方案，都有四种可能的开销，总共一十二种。</p><p><u>事实上，这里存在重复的计算</u>，假如房子乙染了 A 颜色，房子丙染了 B 颜色，在计算最小开销时，由于房子丙-B 的成本已知，那么乙-A1，乙-A2 两种方案只有一种会被采用，即最小的那一种，也就是说，房子乙没必要为每种染色保留所有可能的开销，只需要保留最小的即可。对房子丙同理。真好的剪枝啊。</p><h3 id="区间DP"><a href="#区间DP" class="headerlink" title="区间DP"></a>区间DP</h3>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>索引篇零：索引总论</title>
    <link href="/2022/07/04/%E7%B4%A2%E5%BC%95%E7%AF%87%E9%9B%B6%EF%BC%9A%E7%B4%A2%E5%BC%95%E6%80%BB%E8%AE%BA/"/>
    <url>/2022/07/04/%E7%B4%A2%E5%BC%95%E7%AF%87%E9%9B%B6%EF%BC%9A%E7%B4%A2%E5%BC%95%E6%80%BB%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本篇文章介绍一些索引的基础内容，了解完基础内容后，可以单独阅读如下内容：</p><a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/" title="索引篇一：哈希索引">索引篇一：哈希索引</a>   <br /><a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a> <br /><a href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/" title="索引篇三：LSM Tree">索引篇三：LSM Tree</a>    <br /><br><p>下文所涉及到的表 instructor 具有如下的视图：<br><img src="/img/introduction_of_indices/1.jpg"></p><h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><p>索引技术有很多，一般衡量一种索引技术有如下的几个标准：</p><ul><li>Access Type. access type 包括高效地查找包含特定属性值的记录和包含的属性值落在某一特定范围的记录。</li><li>Access Time. 找到特定数据或一组数据所花费的时间。</li><li>Insert Time. 插入一条新记录的时间，包括查找和更新索引结构的时间。</li><li>Deletion Time. 删除一条特定记录的时间，包含查找和更新索引结构的时间。</li><li>Space Overhead. 索引结构所占用的额外空间。</li></ul><p>通常一个文件不会只有一个索引结构，如查找图书时，既可以根据分类查找、也可以根据作者查找，还能根据时间查找。一个索引结构只能建立在某个 <font color=red>search key</font> 上面，search key 由某个属性或某组属性构成。search key 是属性（组）的名字，如 name，age，而具体的属性取值，如 Tom，22，被称为 search-key value 或 search key 值。</p><p>一般某个文件在存储记录时，可能依照某种顺序存储，比如依照表的自增主键从小到大，如果某个索引的 search key 恰好也定义了这种顺序，那么称该 index 为 <font color=red>clustering index 或者 primary index</font>. 注意，primary key 和 primary index 没有必然的联系，因为 primary index 可以建立在任何的 search key 上面。如果某个 index 的 search key 所指定的顺序和文件记录的存储顺序不同，那么这样的 index 被称为 <font color=red>nonclustering index 或者 secondary index</font>. 两种 index 如下：<br><img src="/img/introduction_of_indices/2.jpg" alt="图 1"><br><img src="/img/introduction_of_indices/3.jpg" alt="图 2"></p><p>假设 instructor 表的文件以 ID 为顺序存储记录，那么图 1 的索引结构是 primary index，因为它的 search key 为 ID，注意，若以 primary key（数据库自动增加的）为 search key，该索引结构同样是 primary index. 而图 2 索引结构的 search key 是 dept_name（第三列），该 search key 定义的顺序和文件本身存储的顺序不一致，所以是 secondary index.</p><p>如果索引结构是 primary index，search key 定义的顺序和记录在文件中存储的顺序一致，<font color=red>那么索引结构既可以采用稀疏索引（sparse index），也可以采用稠密索引（dense index）</font>：</p><ul><li>在稠密索引中，对一个文件中的每一个 search key 值都有一条索引条目（index entry）与之对应。每条索引条目包含 search key 值和一个指针，指针指向具有相同 search key 值记录的第一条记录，如图 4. 而如果是 secondary index 的稠密索引，那么必须记录所有具有相同的 search key 值的记录的指针。 </li><li>在稀疏索引中，文件中若干个相邻的记录共享一条索引条目，条目中的指针指向第一条记录。查找过程就是先找最大的小于等于目标的索引条目，然后遍历这些相邻的记录。如图 5.</li></ul><p><img src="/img/introduction_of_indices/4.jpg" alt="图 4 - 稠密索引"><br><img src="/img/introduction_of_indices/5.jpg" alt="图 5 - 稀疏索引"></p><p>想象这样一种场景，假设要为有 1,000,000 个元组的表构建一个稠密索引。一般索引条目要小于文件记录（元组），假设一个 4KB 的 block，能够容纳 100 个索引条目，那么总共需要 10,000 个 block. 如果元组的数量为 100,000,000，那么索引条目就要占 1,000,000 个 block，即 4GB. 一般索引条目也很大，需要存储在文件中。当查找记录时，需要先查找索引，而索引文件很大，全部读入内存需要非常多的 I&#x2F;O. 当然可以采用二分查找，对于上面所说的 10,000 个 block 需要 14 次磁盘 I&#x2F;O（block 为单次传输单位）。</p><p><font color=red>为减少磁盘读写，可以采用多级索引结构（multilevel indices）</font>。在多级索引结构中，像对待普通记录那样对待索引条目，再为它们建立一层索引，如下图：<br><img src="/img/introduction_of_indices/6.jpg" alt="图 6 - 两层稀疏索引"></p><p>图中，第一层索引（inner index）既可以稀疏也可以稠密，第二级索引（outer index）是在 inner index 条目（以索引条目为 search key）上面建立的稀疏索引，因为 inner index 必然是按索引条目顺序存储的。当查找一条记录时，首先在 outer index 中找到最大的且小于等于索引条目，然后读出（可能）包含目标索引（inner index 条目）的 block，再在该 block 中查找（取决于采用哪种索引方式）。这样加上读取文件记录的 block，总共只需要三次磁盘读写，远远小于二分查找。</p>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：代价衡量</title>
    <link href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E4%BB%A3%E4%BB%B7%E8%A1%A1%E9%87%8F/"/>
    <url>/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E4%BB%A3%E4%BB%B7%E8%A1%A1%E9%87%8F/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：交集</title>
    <link href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E4%BA%A4%E9%9B%86/"/>
    <url>/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E4%BA%A4%E9%9B%86/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>查询处理篇：排序</title>
    <link href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%8E%92%E5%BA%8F/"/>
    <url>/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%8E%92%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据库缓冲池（buffer pool）</title>
    <link href="/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%EF%BC%88buffer-pool%EF%BC%89/"/>
    <url>/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%86%B2%E6%B1%A0%EF%BC%88buffer-pool%EF%BC%89/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据库系统分类</title>
    <link href="/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%88%86%E7%B1%BB/"/>
    <url>/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>索引篇三：LSM Tree</title>
    <link href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/"/>
    <url>/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>考虑这样一种场景，有大量的数据需要通过 B+ 树索引写入（update&#x2F;insert&#x2F;delete）数据库，相邻的待写入数据很可能处在不同的叶节点上，而该索引结构又很大，buffer pool 容纳不下所有的叶子节点，那么完成这样的写入，文件 I&#x2F;O 次数很可能与数据量成正比。<u>若是使用 magnetic disk（磁盘）</u>，当前最快的磁盘能做到 0.05ms 每 block（4KB）的传输速度，seek time（寻道时间）大概是 4ms 每次，也就是单单考虑寻道时间，每秒也只能做 250 次磁盘 I&#x2F;O. <u>若是使用 flash memory（如 SSD）</u>，因为 flash 支持随机读写，可以不需要寻道，大概 0.01ms 每页（4KB），但是 flash 不支持原地更新（in-place update），更新单位为 erase block（256KB-1MB），通常需要 3ms，就算是只更新一条记录也必须如此，更何况 flash 还有寿命限制，最多能擦除 1,000,000 次。</p><p>所以，<u>在每秒存在大量随机 insert&#x2F;update&#x2F;delete 场景中</u>，B+ 树并不是十分理想的结构。为此许多 write-optimized 索引结构被提出，LSM（log-structed merge tree）便是其中一种。LSM 有许多变种，本文只介绍其中一种，stepped-merge index LSM，以下简称 LSM.</p><h2 id="Stepped-Merge-Index-LSM"><a href="#Stepped-Merge-Index-LSM" class="headerlink" title="Stepped-Merge Index LSM"></a>Stepped-Merge Index LSM</h2><p>LSM 包含若干个 B+ 树，它们分为两部分：一个在内存中的 B+ 树索引结构和若干个在磁盘上的 B+ 树索引结构，如下图 1：<br><img src="/img/LSM/1.jpg" alt="图 1"><br>在内存中的 B+ 树称为 $L_0$，在磁盘上的 B+ 树称为 $L_i,\ \ i &gt;&#x3D; 1$. 磁盘中的每一层包含 $k$ 个相同的 B+ 树，且 $L_{i+1}$ 层的 B+ 树的大小（叶子节点数量）是 $L_i$ 的 $k$ 倍，也就是说磁盘上的每一层的所有 B+ 树合并后刚好构成下一层的一个 B+ 树，但 $L_0$ 的 B+ 树和 $L_1$ 的 B+ 树一样大。 </p><br><p><font color=dark-green>LSM 的插入操作</font><br>当插入数据时，数据首先被插入内存中的 $L_0$ 树，当 $L_0$ 树容量达到限制时，将其写入磁盘并清空，也就是 $L_1$。当 $L_0$ 再次达到上限后，再将其写入磁盘，以此往复，磁盘中就可能存在多个 $L_1:\ L_1^1,\ L_1^2,\ …$ 当磁盘中存在 $k$ 个 $L_1$ 时，就可以将这些树合并成一个 $L_2$ B+ 树，当磁盘中存在 $k$ 个 $L_2$ 时，又可以合并成 $L_3$. 这个过程可以一直执行下去。</p><blockquote><p><strong>为什么要合并呢？</strong>考虑查询操作，当要某条数据时（借助 search key），按照上面所述的步骤，查询过程应该是这样的：先查询 $L_0$，不中，再依次查询 $L_1^1,\ L_1^2,\ …$ 若还是不中，继续查询 $L_2^1,\ L_2^2,\ …$ 也就是说查询一条数据可能会搜索多颗 B+ 树。<u>为了限制查找开销，有两点优化，其一便是合并</u>，将若干个上一层的树合并成下一层的一个较大的树，这样原本会查询多颗小树的操作就只需要查询一颗更矮更胖的大树。<u>另外一个优化是使用布隆过滤器（bloom filter）</u>。尽管合并在一定程度上降低了查询开销，但仍旧不能避免在某一层需要检索多颗 B+ 树。布隆过滤器能够给出某个集合（这里就是一颗 B+ 树）中包含包含某一个 search key 的概率：若判断包含，则有一定概率不存在（假阳性），若判断不存在，那一定不存在。所以，可以为每颗 B+ 树分配一个布隆过滤器，查询时可以跳过一些树。布隆过滤器建立在 search key value 上，如果一棵树包含 $n$ 个 search key value，布隆过滤器大小为 $10n\ bits$，使用 7 个哈希函数，那么假阳性约为 $\frac{1}{100}$，也就是能跳过大量无效查询，开销略微高于常规的 B+ 树索引。<u>但是，对于范围查询</u>，布隆过滤器无能为力，还是需要检索每棵树判断是否有在范围内的数据。</p></blockquote><p>在合并 $L_i$ 层时，先顺序读取 $L_i$ 层每棵树的叶子节点，然后将这些数据按照 search key value 的顺序合并，最后在上面以 bottom-up construction 的方式构建一颗 $L_{i+1}$ 的树。很明显，合并过程没有多余的随机 I&#x2F;O 读写，$L_i$ 的每棵树是按顺序从左至右读取每个叶子节点，合并时采用的排序算法（见 <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%8E%92%E5%BA%8F/" title="查询处理篇：排序">查询处理篇：排序</a>）也没有多余的随机读写，构建下一层树时，也是顺序读写。</p><p><u>可以使用写放大（write amplification）指标来衡量 LSM 和 B+ 树的磁盘写开销</u>。当插入一条数据时，需要写磁盘的总数据量（bytes）除以单条数据的大小，其值就是写放大。写放大衡量的主要是，当更新一条数据时，需要多少额外的写开销，若写放大为 10，那么更新一条数据，平均需要引起 10 条数据的写磁盘。</p><p><u>对于 B+ 树索引</u>，假设每个叶子节点写回磁盘前平均有 $x$ 次更新，那么写放大为，叶子节点的字节数（block 大小）除以 $x$ 条数据的字节数。假设一个 block 能够容纳 100 条数据，那么写放大为 $\frac{100}{x}$. <u>对于 LSM</u>，若 $k &#x3D; 5$，$L_0$ 的数据条数为 $M$，所有层总共的数据条数为 $I$，且 $100 &#x3D; \frac{I}{M}$. 那么该 LSM 大概有 $log_5(\frac{I}{M}) &#x3D; 3$ 层，写放大也就为 3，因为同一条数据只会在一层写入一次：写入 $L_1$ 层的一次来自 $L_0$，写入 $L_2$ 层的一次来自 $L_1$ 层的合并……</p><p><font color=dark-green>LSM 的删除操作</font><br>一般来说，删除一条数据首先是找到它，然后再删除它，LSM 并不采用这种方式，而是通过<u>插入删除标记（deletion entry）</u>的方式来标记某条数据已经被删除了。因此 LSM 的删除操作过程和插入过程一致，只不过插入的是一条 deletion entry. </p><p>插入 deletion entry 后，LSM 的查找和合并操作有些不一样，先看原文：</p><blockquote><p>However, lookups have to carry out an extra step. As mentioned earlier, lookups retrieve entries from all the trees and merge them in sorted order of key value. If there is a deletion entry for some entry, both of them would have the same key value. Thus, a lookup would find both the deletion entry and the original entry for that key, which is to be deleted. If a deletion entry is found, the to-be-deleted entry is filtered out and not returned as part of the lookup result.</p></blockquote><blockquote><p>When trees are merged, if one of the trees contains an entry, and the other had a matching deletion entry, the entries get matched up during the merge (both would have the same key), and are both discarded.</p></blockquote><p>需要解释下，可以确定，$L_0$ 中不会同时存在 deletion entry 和原数据，因为内存中删除操作很快也简单，当 $L_0$ 满了写入磁盘变成 $L_1$ 的 1 棵树，那么在 $L_1$ 的每棵树中也不会同时存在删除标记和纪录，在不同的树中可能同时存在。同一层中，每棵树的数据新旧程度不一样，后写入的更新一些，当合并时，当遇到 search key value 相同的删除标记和记录（可能有多条）时，若删除标记更加新，那么丢弃记录即可，若记录更新，忽略删除记录即可。这样，在下一层中，同一颗树中，同样不会同时存在删除标记和原记录。</p><p><font color=dark-green>LSM 的更新操作</font><br>LSM 的更新操作和删除操作过程类似，并不是找到记录然后更新它，而是插入一条<u>更新标记</u>。查找、合并时的过程和上面叙述的类似。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>此小节分为两个部分，先讨论 LSM 相比 B+ 树有什么样的特点，然后讨论 LSM 的缺点。</p><p>由上面的叙述可知，LSM 在插入数据时（删除、更新一样）不会涉及到将数据读出磁盘然后再写入，它采用只追加（顺序写）的方式，拥有非常好的写性能。这是它最主要的优点。</p><p>然而，虽然借助合并和布隆过滤器，但它的查找开销还是稍微高于 B+ 树；而且 $L_0$ 写磁盘、合并操作可能会阻塞正常的数据读写；最后，因为采用只追加的方式，同一条记录可能会同时保留多个新旧版本，对内存有一定消耗。</p><p>另外，LSM 的写放大还需要计算上预写日志 WAL(write-ahead log)，因为 $L_0$ 装满后才会写入磁盘，这期间可能出现故障，整个 $L_0$ 的数据就会丢失，为了防止这样的事情发生，写 LSM 之前，要先将数据以指令的方式写入 log 文件中（stable storage），log 写成功之后才会将数据写入 LSM 中。</p>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关系型数据库磁盘存储</title>
    <link href="/2022/06/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%A3%81%E7%9B%98%E5%AD%98%E5%82%A8/"/>
    <url>/2022/06/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%A3%81%E7%9B%98%E5%AD%98%E5%82%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文主要介绍关系型数据库的一张表在磁盘中是如何存储的。</p><p>绝大多数数据库系统将一张表（relation）存储在一个或多个文件中（file），由于 file 可能很大，为方便元组（tuple，也称 record）的增删改查，file 内部又被分为了多个 page（或 block，一般 4KB-8KB），真正的元组存储在 block 中。同时 block 也是内存和磁盘之间的数据传输单位。</p><p>以上的结构都是逻辑结构，存储在磁盘时，通常会由多个物理块（比如扇区）连续存储，来表示一个 block.</p><p>后面会依次介绍元组如何组织各个属性值，block 如何存储元组，以及 file 对元组的管理。</p><h2 id="属性值在元组中的存储"><a href="#属性值在元组中的存储" class="headerlink" title="属性值在元组中的存储"></a>属性值在元组中的存储</h2><p>假设创建如下的表：<br><img src="/img/storage_in_disk/1.jpg"><br>该表的四个属性有三个是变长的，最简单的一种方式是给每个属性值都分配其声明的最大值，在该例子中为 53 bytes。很多时候，属性值并不能达到其要求的上限，而且还可能存在 null 值，十分浪费空间。</p><p>为满足属性值变长特性，通常采用如下的方式组织一个元组内部的值：<br><img src="/img/storage_in_disk/2.png"><br>整个元组分为两部分：头部（header）和实际的数据存储部分，header 用来存储一些元数据，若表的模式确定，其头部大小也就固定了：</p><ul><li><u>空值位图（null bitmap）</u>。除了主键外，其他属性值可能会为 null，空值位图用每一位来表示某一属性值是否为空，若为空该位置 1. 在一些实现中，若属性值为空，则不分配空间，适合空值较多的表；一些实现中，若为空，依然分配空间，读取时忽视即可，如例子（例子中空值位图没有放置在最前面）。</li><li><u>各属性值的偏移与大小数组</u>。由于属性值可能是变长的，所以需要同时记录这些属性值在该元组中的偏移量和大小，如例子（例子中，内存占用固定的 salary 属性值放置在该数组后面）。</li><li><u>可见性标识</u>。用于并发控制，一般是锁标志。</li></ul><p>数据部分的 a, b, c, d 就是实际的属性值。对于大对象的存储，比如图片、视频等，属性值会保存一个指针（如指向另外一个 file 或者 page），然后将大对象存储在其他地方。</p><p><strong>注意</strong>，头部并没有存储表模式信息（schema），因为如何解析一条元组的数据是由数据库上层决定的。当一张表被创建时，其模式信息就保存在了数据库元信息中，当读取一条元组时，就依据这些元信息来解析元组中各属性值。</p><h2 id="元组在-block-中的存储"><a href="#元组在-block-中的存储" class="headerlink" title="元组在 block 中的存储"></a>元组在 block 中的存储</h2><p>在 block 中存储元组（tuple&#x2F;record），slotted-page structure 是用得最多的一种结构，如下图：<br><img src="/img/storage_in_disk/3.png"></p><p>可以看出也分为头部 header 和数据部分，头部包含三个部分：</p><ul><li><u>Record 的数量</u>。也就是元组的数量。</li><li><u>空闲空间的结束位置</u>。在插入元组时，由该位置能快速找到空闲空间放置元组。</li><li><u>记录 Record 偏移和大小的 Entry 数组</u>。由于元组的属性值变长，所以尽管属于同一张表，元组同样是变长的，该数组能方便的提取元组。</li></ul><p>当往一个 block 中插入一条 record 时，首先检查空闲空间能否放下该 record（由图很容易计算），若有，则从空闲空间的最后一个字节开始<strong>往头部移动</strong>分配空间给该 record，同时插入一个 entry.</p><p>当从一个 block 中删除一条 record 时，会将指向该记录的 entry 设置为删除（比如将 size，图中 y，设置为 -1），然后移动那些后面插入的 record 覆盖删除的 record。由于总的 record 数量不算太多，移动的代价还能接受。</p><h2 id="元组在文件中的组织方式"><a href="#元组在文件中的组织方式" class="headerlink" title="元组在文件中的组织方式"></a>元组在文件中的组织方式</h2><p>一张表包含一系列元组，由单个或多个文件（如分表）组成，而一个文件又可能存在多个 block，上一节是介绍元组在 block 中如何存储，这一节则是介绍如何为元组选取合适的 block.</p><h3 id="堆文件组织方式"><a href="#堆文件组织方式" class="headerlink" title="堆文件组织方式"></a>堆文件组织方式</h3><p>堆文件（heap file）组织方式中，总是将 record 放置到有空闲空间的 block 中（如第一个有空闲空间的 block）。在寻找具有空闲的 block 时，为了避免按顺序扫描所有的属于该文件的 block，绝大多数数据库采用了 free-space map 的结构。</p><p>free-space map 通常是一个整数数组，一个数组元素对应表的一个 block，元素的值与 block 空闲空间比例有关系，该数组存储在某个 block 中，一般在一开始就读进了内存中。如下例子：<br><img src="/img/storage_in_disk/4.jpg"></p><p>假设某个表具有 16 个 block，因此 free-space map 具有 16 个元素。假如使用 3-bit 来表示一个数组元素，那么将某个元素存储的值除以 $2^3$ 就能得到该 block 中空闲空间（至少）占据的比例，如元素值为 7 表示该 block 至少有 7&#x2F;8 的比例是空闲的。为何是这样？如果直接存储 $f &#x3D; \frac{block\_free\_space}{block\_total\_space}$ 那么都会为零，所以，存储的实际是 $f*2^n$，$2^n$ 表示该元素能表示的最大的数，如 PostgreSQL 就采用的 8-bit free-space map.</p><p>如果表很大，那么 block 数量也可能很大，free-space map 也就可能很大，为了进一步减少扫描的时间，可以采用多级 free-space map 结构，如下，采用两级：<br><img src="/img/storage_in_disk/5.png"><br>很好理解，其实就是将第一级分组，然后选出其中最大的构成第二级，类似折半查找，如果第二级数量依旧庞大，可以继续分下去。</p><p>每当插入元组后，block 的空闲空间比例可能会变小，free-space map 结构也就需要更新，更新的 free-space map 存在于内存中，所以需要将其写入磁盘。然而，每次更新都写磁盘代价高昂，一般采用延迟写（周期性），这样就会导致磁盘的文件过时，若未来得及写磁盘就发生了故障，重启后，free-space map 就记录了过时的数据。不过，这没啥大不了的问题，数据过时导致查找的 block 与实际不符，多来几次也就将过时的信息更新了，当然也可以周期性的扫描 block 来更新该结构。</p><h3 id="序列文件组织方式"><a href="#序列文件组织方式" class="headerlink" title="序列文件组织方式"></a>序列文件组织方式</h3><p>考虑到这样一种场景，某些记录总是以某种方式频繁检索，如某个年龄区间 + 性别，为了实现这样的高效检索，记录一般以检索键（search key）顺序存储。search key 由任何属性或一组属性构成，不一定是主键。</p><p>为了使得它们按 search key 顺序存储，每条记录还会存储下一条记录的位置（指针），检索时按照该指针就能快速检索相关记录；插入时，需要找到该记录的前一条记录，然后修改指针即可；删除时也需要修改指针。这些操作和链表一致。例子如下图：<br><img src="/img/storage_in_disk/6.jpg"></p><p>比如，若插入一条记录，则可以按照如下方式：</p><ul><li>定位待插入记录的前一条记录所在的 block. 可以通过记录每个 block 中最大最小 search key 做到。</li><li>如果该 block 有空闲空间，插入，并修改相应的指针，若无，则插入一个 overflow block（就如图片展示的那样），并修改指针。</li></ul><h3 id="散列文件组织方式"><a href="#散列文件组织方式" class="headerlink" title="散列文件组织方式"></a>散列文件组织方式</h3><p>哈希结构通常用在内存中存储数据，它的特点是能够快速定位记录的位置。在内存中，哈希通常是一个指针数组，每个指针是一个链表头，该链表存储定位到该数组元素的实际数据（链地址法），比如数据库在内存中的哈希索引结构。而在 disk-based 哈希中，哈希结构一个 bucket（可以理解为 block） 数组，每个 bucket 存储实际的记录。</p><p>当插入一条记录时，计算该记录的哈希值，$buket\_idx &#x3D; h(r\_key)\ \%\ buket\_num$，然后再将记录插入该 bucket，若 bucket 满了，那么就会新分配一个 bucket 链接在后面作为 overflow bucket，将记录插入新的 bucket 中。</p><p>当查找某条记录时，采用同样的方式计算 bucket_idx，然后在 bucket 及其 overflow bucket 中查找（一般是顺序检索）是否存在。具体结构如下：<br><img src="/img/storage_in_disk/7.png"></p><p>之所以会出现 overflow bucket，有以下原因：record_key 分布不均匀，或者是哈希函数选择得不够好，导致某些 bucket 记录特别多，而有些又特别少。一般通过选取好一点的哈希函数，或者通过分配更多 bucket 缓解（如 bucket_num &#x3D; $(n_r &#x2F; f_r) * (1 + d)$，$n_r, f_r$ 分别表示总的记录数和每个 bucket 能够存储的记录数，d 表示一个经验系数，一般取 0.2）。而 record 的数量很多时候根本无法预估，固定分配的哈希结构（static hashing），就可能不能适应记录数量的增减，一般有以下方式解决：</p><ul><li>rehashing. 此方法需要重新计算所有的记录，比较耗时。</li><li>dynamic hashing. 具体见<a href="#">Post not found: 索引篇一：可拓展哈希</a></li><li>采用其他结构。</li></ul><h3 id="B-tree-文件组织方式"><a href="#B-tree-文件组织方式" class="headerlink" title="B+ tree 文件组织方式"></a>B+ tree 文件组织方式</h3><p>B+ tree 文件组织方式，见 <a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a></p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据库文件存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>索引篇二：B-plus Tree</title>
    <link href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/"/>
    <url>/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/</url>
    
    <content type="html"><![CDATA[<h2 id="B-Tree-的结构"><a href="#B-Tree-的结构" class="headerlink" title="B+ Tree 的结构"></a>B+ Tree 的结构</h2><p><img src="/img/B_plus_tree/1.jpg" alt="图 1 B+ tree 节点结构"><br>如图 1 所示，B+ 树的每个节点包含 $n-1$ 个 search-key value，$K_1, K_2, …, K_{n-1}$，和 $n$ 个指针，$P_1, P_2, …, P_n$. 这些 search-key value 在节点中按序存储，即若 $i &lt; j$, 有 $K_i &lt; K_j$. 在实际中 search-key value 是可能存在重复的，<font color=red>目前只考虑不重复的情况，后面再介绍重复的情况</font>。</p><p>B+ 树的节点分为三类，叶子节点（leaf node）、非叶子节点（non-leaf node），下面一个一个介绍。<br><img src="/img/B_plus_tree/2.jpg" alt="图 2 叶子节点"></p><p><strong>首先是叶子节点</strong>。如图 2，是一个 n &#x3D; 4 的叶子节点，search-key 为 instructor 表的 name 属性. 对于 $i&#x3D;1, 2, …, n-1$，指针 $P_i$ 指向存储在文件中的，search-key value 等于 $K_i$ 的记录，$P_n$ 指向右边相邻的同级节点或者为 null，如果右边不存在节点。</p><p>每一个叶子节点<u>能够容纳的指针</u>的数量为 $[\lceil \frac{n}{2} \rceil,\ n]$，对于 n &#x3D; 4，其区间为 [2, 4]. </p><p>对于两个叶子节点 $L_i, L_j$，若 $i &lt; j$，即 $L_i$ 在 $L_j$ 的左边，那么对于所有在 $L_i$ 中的 search-key value，都小于 $L_j$ 中的每一个 search-key value. </p><p>B+ 树是一个多级索引结构，叶子节点常采用稠密索引，也就是说，出现在文件记录中的每一个 search-key value，也会出现在叶子节点中（一个指针对应一条记录）。<br><img src="/img/B_plus_tree/3.jpg" alt="图 3"><br><strong>然后是非叶子节点</strong>，非叶子节点是对叶子节点建立的多级稀疏索引。非叶子节点和叶子节点的不同是，非叶子节点的指针指向的是树节点而非记录，而且非叶子节<u>能够容纳的指针</u>数量也是 $[\lceil \frac{n}{2} \rceil, n]$. </p><p>在一个非叶子节点中，$P_i\ \ 1 &lt; i &lt; n$，指向一个子树，该子树包含的 search-key value 小于 $K_i$，但大于等于 $K_{i-1}$；指针 $P_1$ 指向包含小于 $K_1$ 的 search-key value 的子树；而 $P_n$ 指向其右边的相邻同级节点或者为 null，如果右边不存在节点。</p><p>图 3 展示了一个在 instructor relation 文件上建立的完整 B+ 树，其中 n &#x3D; 4.</p><p><u>B+ 树是一颗平衡多叉树，即从根节点到每个叶节点的路径长度总是一样的，正是如此，才提供了良好的增删改查性能，而且每个节点至少要半满</u>。</p><p><font color=red>对于 search key value 存在重复的情况</font>，可以有如下的方式解决：</p><ul><li>叶子节点存储每一个出现的 search key value，无论它是否重复，那么 对于 $i &lt; j$, 就应该有 $K_i &lt;&#x3D; K_j$.</li><li>在叶子节点上，对于每一个 search key value，不再存储单个指针，而是一组指针，指向具有相同 search key value 的记录，由于每组指针的数量不确定，节点中的 search key value 个数也就不一定了。</li></ul><p>这两种方式都会增加操作（尤其是删除）的复杂度，不常使用。而绝大多数的数据库系统通过组合多个属性来构成不重复的 search key. 假如某个索引的 search key 为 $a_i$，并且它可能重复（不是主键或候选键），这时系统选择一个不可能重复的属性，假设为 $A_p$，将它们两个组合在一起构成新的 search key $(a_i, A_p)$. </p><h2 id="在-B-树上的查询"><a href="#在-B-树上的查询" class="headerlink" title="在 B+ 树上的查询"></a>在 B+ 树上的查询</h2><p>假设现在要查询一个 search-key 值为 $v$ 的记录，下图 4 展示了其伪代码，比较简单就不解释了：<br><img src="/img/B_plus_tree/4.jpg" alt="图 4"></p><p>由于每个节点有一个 $P_n$ 指针指向下一个同级节点，所以 B+ 树支持范围查询（range queries），其伪代码如下图 5：<br><img src="/img/B_plus_tree/5.jpg" alt="图 5"></p><p>在范围查询的伪代码中，先采用单值查询 find(lb) 的方式找到落在 [lb, ub] 中且最小的记录，然后向后遍历（可能跨节点）找到所有合适的记录。<u>需要注意</u>，伪代码中 n 表示的是 $K$ 的个数，和上文中的 n 意义不一样，上文中 n 表示指针的个数，在伪代码中，节点的最后一个指针应该是 $P_{n+1}$.</p><p><strong>现在计算下 B+ 树查询的时间复杂度</strong>。假设文件中共有 $N$ 条记录，从根节点到叶子节点的路径长度不超过 $\lceil log_{\lceil\frac{n}{2}\rceil}(N)\rceil$. </p><p>一般情况下，B+ 树节点的大小和 block 的大小一致，同为 4KB-8KB，假设 search key 的大小为 32B（一般没这么大），指针的大小为 8B，那么一个节点能存储 100 个 search key value. 如果文件中的记录总共包含 1,000,000 个 search key value，查找某个特定的记录最多只需要访问 4 个节点，即最多读 4 次磁盘，远远少于平衡二叉树的次数，最后只需要根据指针的值再读一次磁盘就能得到目标记录。</p><p><u>而范围查询代价更高一些</u>，假设最后返回的指针个数为 M，这 M 个指针在叶子节点上必然是连续的，所以要读取这 M 个指针，最多需要读取 $\lceil\frac{M}{n&#x2F;2}\rceil + 1$ 次磁盘，因为一个叶子节点最少有 $\frac{n}{2}$ 个指针。为了获取这些指针所指向的记录，还需要计算一些开销，<u>如果是 secondary index</u>，尽管记录的 search key value 是相邻的，但这些记录在文件中却不一定相邻，所以最多需要 M 次磁盘 I&#x2F;O. <u>如果是 primary index</u>，那么这些记录在文件中也是相邻的，所以一次磁盘 I&#x2F;O 读取一个 block，其中包含多条记录，磁盘 I&#x2F;O 大大减少。</p><p><font color=red>假如 search key value 存在重复，如何查找指定的记录？</font>使用上面所说的使用组合 search key 的方式去重，假设组合后的 search key 为 $(a_i, A_p)$，当给定 $a_i &#x3D; v$ 时，如何在组合索引中找到所有包含 search key 为 $v$ 的记录？使用范围查找，其范围为 $[lb, ub]$，且 $lb &#x3D;(v, min(A_p)),\ ub &#x3D;(v, max(A_p))$.</p><h2 id="B-树文件组织方式"><a href="#B-树文件组织方式" class="headerlink" title="B+ 树文件组织方式"></a>B+ 树文件组织方式</h2><p>本节讨论，当采用 B+ 树为文件建立索引时，文件应如何组织。<br><img src="/img/B_plus_tree/6.jpg" alt="图 6"></p><p>如上图 6，在 B+ 树文件组织方式中，叶子节点（叶子 block）存储实际的记录，而不是指向记录的指针，而非叶子节点还和 B+ 树的定义一样。因为记录要比指针大很多，这样一来叶子节点存储的记录就会大大减少，但依旧要求至少半满。</p><p>查找、插入和删除操作与在 B+ 树上定义的一样。</p><p><strong>当要插入一条 search key value 为 $v$ 的记录时</strong>，搜索该 B+ 树，找到 $&lt;&#x3D; v$ 的最大的 search key value（或者最小的 $&gt;&#x3D; v$ 的 search key value），同样也就找到了待插入的 block（叶子节点）。<u>如果该 block 还有空间</u>，那么按 search key value 的顺序插入该记录（可能涉及到记录在 block 中的移动，参考数组的插入操作）。<u>如果没有空间</u>，执行分裂操作（split），将待插入纪录和原记录按照 search key value 顺序平均分成两部分，同时创建一个新的 block（叶子节点），将前一半放在原 block 中，后一半放入新 block中。还需要索引新 block，即放入一条记录 $(key, P_i)$ 到分裂 block 的父节点中，其中 $key$ 是新 block 中最小的 search key value，$P_i$ 则是指向新 block 的指针。很明显，这是一个递归操作，父节点的插入依然可能引起分裂。</p><p><strong>当要删除一条记录时</strong>，同样需要定位记录的 search key value，然后删除该记录（如果存在多个相等的 search key value，需要遍历它们直到找到目标记录），删除后，需要移动后面的记录填补删除记录的空白。由于每个节点都需要满足半满，如果删除一条记录后，节点不满足该要求，就需要从兄弟节点借一些记录，如果两个兄弟都不够，就只能释放 block，合并兄弟，同样地，这些操作也会影响到父节点。</p><p>B+ 树文件组织方式还能用于大文件，具体地，将大文件划分成多记录，再采用 B+ 树组织。</p><h2 id="一些其他的内容"><a href="#一些其他的内容" class="headerlink" title="一些其他的内容"></a>一些其他的内容</h2><p><font color=dark-green>Secondary Indices And Record Relocation</font><br>假设在 instructor 表文件上建立了两个索引，$A_{idx}$ 建立在 dept_name 属性上的，$B_{idx}$ 建立 ID 上，很明显 $A_{idx}$ 是一个 secondary index，而 $B_{idx}$ 是 primary index。不妨假设 $B_{idx}$ 采用 B+ 树索引结构，$A_{idx}$ 采用何种索引结构都无所谓，但一个文件只能有一种索引结构能够改变记录的存储位置，所以 $A_{idx}$ 的索引项只能存储指向记录的指针。当 $B_{idx}$ 节点分裂时，会有部分的记录移动到新的磁盘位置，这时候，那些 secondary index 就需要更新（记录的位置变了，之前的指针也就失效了），包括 $A_{idx}$. 这样的索引结构可能很多，可能涉及很多的记录，那么就可能需要非常多次的磁盘读写。</p><p><font color=red>一个比较广泛的解决方法是</font>，在 secondary index 中，对于某个 search key value，不再存储指向对应记录的指针，而是存储 primary index 的 search key 在该记录中的值。当使用 secondary index 查找记录时，就需要两个步骤：</p><ul><li>先在 secondary index 中找到该记录对应的 primary index 的 search key value.</li><li>然后借助该 value 再到 primary index 中查找具体的记录。</li></ul><p>这样就避免了更新大量 secondary index 中指向记录指针。</p><br><p><font color=dark-green>Search Key Value 长度可变</font><br>如果 B+ 树的 search key 为字符串，长度可变，那么每个节点的容量就不一定，出度不一定。同时，插入、删除等过程也会发生变化，不再根据 search key value 的个数判断是否指向分裂还是合并，而是 block 是否能够容纳新的内容。为了增加每个节点能够容纳的 search key value 的数量（尽量使树又矮又胖），<u>一种前缀压缩（prefix compression）的技术被提出来</u>。</p><p>对于每个非叶子节点，在索引子树时（或叶子节点），不需要存储完整的 search key value，而是只存储一部分前缀，只要在节点内部，能够区分各个子树即可，如下图 7。<br><img src="/img/B_plus_tree/7.jpg" alt="图 7"></p><p><font color=dark-green>B+ 树的高效构造</font><br>考虑这样这种场景，在一个非常大的表上（很多记录）建一个 secondary index 类型的 B+ 树索引，表和索引都很大，内存是放不下的。<u>按照最简单的方式构建</u>，扫描表的每一条记录，然后往 B+ 树中插入条目，可能是 search key value 和指向记录的指针，或者是前面提到的 search key value 和 primary index 中该记录对应的 search key value. 由于是 non-clustering index，所以相邻的记录可能要插入到不同的节点中（block），也就是可能存在大量的磁盘读写（表文件可以顺序扫描，每个 block 只需要一次磁盘 I&#x2F;O，而 B+ 树的各个节点 block 可能需要多次磁盘 I&#x2F;O，因为不同的记录需要插入到不同的节点，而 buffer pool 的容量有限，这些节点已经替换出了内存）。</p><p>像上面这样的大量插入条目的操作被称为该索引的 bulk loading，<u>通常采用的高效的方法是</u>，创建一个临时的索引条目文件，扫描表文件，为每条记录创建一条索引条目，追加到该临时文件，然后对该临时文件排序（见 <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%8E%92%E5%BA%8F/" title="查询处理篇：排序">查询处理篇：排序</a>），最后扫描已经排好序的文件（非此临时文件，具体见连接），将这些索引条目插入到 B+ 树中。</p><p><u>先排序再插入有什么好处？</u>上面提到的大量的磁盘 I&#x2F;O 是因为，这些在表文件中相邻的记录的索引条目不一定在索引结构中相邻（不在同一个节点 block 中），那如果先将这些索引条目按照 search key value 排好序，那么插入时，属于同一个节点的条目就能一次性插入该节点，也就是说每个节点 block 只需要一次磁盘 I&#x2F;O 即可。</p><p><u>性能上还能更近一步</u>，如果索引条目要插入的 B+ 树本身是空的，那么在排完序后，不需要采用插入的方式，而是直接在排完序后的文件上构建。一个文件包含多个 block，这些 block 就是叶子节点，然后取出这些 block 中的最小的 search key value，加上一个指向 block 的指针，就可以构成上一层的条目（分配新的 block，将这些条目写入），继续下去直到创建根节点。<u>这种方式称为 bottom-up B+ tree construction</u>，能够减少大量的磁盘读写（全部的叶节点也就创建好了，也写好了）。 </p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这里总结下 B+ 树的各种操作的时间复杂度，假设，文件中共有 $N$ 条记录，B+ 树的每个节点能存储 $n$ 个 search key value（也就是 $n+1$ 个指针），并且 search key value 不重复：</p><p><u>对于单个记录的查询操作</u>，找到目标记录的（如果存在）search key value 最多需要 $\lceil log_{\lceil \frac{n}{2}\rceil}(N)\rceil$ 次磁盘 I&#x2F;O，如果是采用 B+ 树组织文件，那么不需要额外的磁盘读写，如果指针或 primary index 的 search key value 则还需要其他磁盘读写，这里考虑最简单的情况，采用 B+ 树组织文件，下同。</p><p><u>对于单个记录的插入操作</u>，找到待插入的节点（block），最多需要 $\lceil log_{\lceil \frac{n}{2}\rceil}(N)\rceil$ 次磁盘 I&#x2F;O，插入后，这些 block 还需要写回磁盘，所以一共需要 $\lceil log_{\lceil \frac{n}{2}\rceil}(N)\rceil + 1 + x$ 次磁盘 I&#x2F;O，其中 1 表示写回更新的 block，$x$ 表示可能分裂引起的 block 更新的数量（包括新分配的 block）。</p><p><u>对于单个记录的删除操作</u>，同插入操作。</p>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>索引篇一：哈希索引</title>
    <link href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/"/>
    <url>/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%80%EF%BC%9A%E5%93%88%E5%B8%8C%E7%B4%A2%E5%BC%95/</url>
    
    <content type="html"><![CDATA[<h2 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h2><p>本节内容是，如何选择一个好的哈希函数。一个理想的哈希函数能够将 search key 的值均匀地映射到各个 bucket 中，很多时候无法提前预知 search key 会取哪些值，所以功夫主要在哈希函数的选择上，一般就可能要具有如下的特质：</p><ul><li><u>映射均匀</u>。尽管无法精确知道 search key 会取哪些值，但可以能预知所有可能的取值（比如具有长度上限的字符串，整数等），那么只要保证该哈希函数能够将这一集合均匀地映射到每一个 bucket 即可。需要明白，这个集合中各元素出现的频率并不一定是相同的，需要考虑到频率。比如大学里的人年龄区间为 [14-80]，但 [20-30] 的频率远远大于其他区间。</li><li><u>映射随机</u>。哈希值的计算不能跟 search key 取值的任何外部可见性，比如字符顺序等，产生关联，这个不好理解，具体看下面例子。</li></ul><p>如图 1 是一个静态哈希的例子：<br><img src="/img/hash_indices/1.jpg" alt="图 1"></p><p>上图中，索引的 search key 为 ID，哈希函数为 ID 各位数之和模 8，共有 8 个哈希 bucket，每个的大小为 2，而且该 instructor 表是以 dept_name 顺序存储的，所以这是一个 secondary index 结构。图中展示的是已经映射完成情况，其中 bucket 5 被映射了 4 个值，超出了容量，所以分配了一个 overflow bucket.</p><p><font color=red>现在重新映射 instructor 表，看看如何选取哈希函数</font>。假设以 dept_name 为 search key，哈希函数为取 dept_name 的第一个字符，它处于 26 个字母的第几位，就将它映射到第几个 bucket 中（当然本例 bucket 数量需要增加到 26）。该哈希函数虽然简单，<strong>却并不满足映射均匀</strong>，因为在 dept_name 所有可能的取值中，以 B、R 开头的 dept_name 要多于 Q 和 X，这会导致 bucket 2 和 18 的元素远多于 bucket 17 和 24 中的元素。再假设，以 salary 为 search key，哈希函数为将薪水区间分为 10 份，比如 [30001, 40000], [40001, 50000], …, <strong>该函数均匀（均分了薪水区间），但却不满足随机</strong>，因为薪水处于区间 [60001, 70000] 的人数远多于 [30001, 40000] 的人数。</p><h2 id="可拓展哈希"><a href="#可拓展哈希" class="headerlink" title="可拓展哈希"></a>可拓展哈希</h2><p>表的记录数量在一段时间内既可能变得很大，也可能很小，静态哈希索引结构不适应这样的变化。单单考虑表的大小增长，静态哈希索引将有如下方式可以应对：</p><ul><li>不额外操作。那么随着记录的增多，overflow bucket 的数量剧增，各种操作将会变得很慢。</li><li>提前分配空间。该表的增长范围不太可能预知，那么很可能只能是扬汤止沸或者过犹不及（浪费空间）。</li><li>rehashing. 非常费时，很可能导致索引在一段时间内不可用。</li></ul><p>因此，哈希表能够随着记录的增长而增长，缩小而缩小的技术被设计出来，称为 dynamic hashing. 其中一种广泛使用的动态哈希是可拓展哈希（extendible hashing）。</p><p>总的来说，可拓展哈希通过合并某些空的 bucket 或者分裂某个溢出的 bucket 来应对 search key 值得减少和增长。可拓展哈希依然会存在 rehashing，但每一次只有一个 bucket 中的记录会被重新映射，开销可以接受。</p><br><p><font color=dark-green>可拓展哈希的数据结构</font><br><img src="/img/hash_indices/2.jpg" alt="图 2"><br>我们依然按照上面的原则选取哈希函数，可拓展哈希的哈希函数将 search key 的值映射为二进制串（一般 32 位），如上图，search key 为 dept_name. <strong>那如何将具体的哈希值与 bucket 绑定呢？</strong><br><img src="/img/hash_indices/3.jpg" alt="图 3"><br>可拓展哈希采用这样的方式，如上图，左边的 bucket address table 又可以称为目录页，其中存储的是指向 bucket 的指针，右边的 bucket 是实际存储记录的结构。总的来说，可拓展哈希根据哈希值的某几位二进制来决定将某个 search key 值映射到哪一个 bucket 中。<strong>如何决定使用哪几位呢？</strong>目录页使用一个全局的 $i$ 来决定使用的二进制串长度（global length），图中展示的是前缀，从最高位开始，而在实现中，为了方便，常常使用后缀，从最后一位开始，后面统一使用前缀。同时每个 bucket 还记录本地实际使用的位数 $i_j$（local length），满足 $i &gt;&#x3D; i_j$。在同一个 bucket 中的记录，其 search key 值的高 $i_j$ 位都相同，为什么这样先不管，后面会逐步清晰。初始情况下 $i &#x3D; 1, i_j &#x3D; 1$.</p><br><p><font color=dark-green>可拓展哈希的查询和更新操作</font><br>下面通过查询（lookup）、插入（insertion）和删除（deletion）几个操作，来理解可拓展哈希的运作机理。</p><p><strong>首先是查询</strong>。第一步是通过哈希函数计算哈希值 $val &#x3D; h(Key)$，然后取该值的高 $i$ 位，<font color=red>得到目录页的下标（十进制）</font>，再根据该下标得到目录条目，该条目含有指针指向具体的 bucket. 假设当前的 global length 为 2，key &#x3D; biology，如图 2，最高两位为 00，所以目录页的下标为 0，其指向 bucket 1. 然后具体的查找过程在 bucket 中执行。</p><p><strong>然后是插入</strong>。先执行上面描述的查询过程，找到具体的 bucket，不妨设为 $j$，如果 bucket 还有剩余的空间，则将记录插入到该 bucket，如果 bucket 已经满了，将要执行分裂操作（split），将该 bucket 中的记录 rehash. <font color=red>分裂操作做了什么？</font>当 bucket $j$ 满了，说明 $i_j$ 太小了，search key 中具有相同前缀（高 $i_j$ 位）的太多，区分度不够，需要增加位数，以将部分记录散列到其他的 bucket 中，有如下两种可能：</p><ul><li><u>如果当前 $i&#x3D;i_j$</u>，需要先增加目录页。通过 $i&#x3D;i+1$，增加新的一位参加映射，目录页数量就能加倍（ $2^{i+1}$）。增加的一倍的目录项都是空的（没有指向具体的 bucket），所以，会将这些新增的目录项指向与他们高 $i$ 位相同的目录项所指向的 bucket，如图 3 的 01 和 00 都指向同一个 bucket（即表示新增的一位在大多数目录项中还未用上）。<font color=red>此时，可以安全地 rehashing 了</font>。假设 bucket $j$ 中记录的 search key 都有前缀 $b_1b_2b_3$，当 $i_j&#x3D;i_j+1$ 后，新的前缀有两种可能，$b_1b_2b_30$ 和 $b_1b_2b_31$，我们让具有前者前缀的 search key 依旧映射到 bucket $j$，而后者则会被映射到新增的目录项所指的 bucket，如上面上述，该目录项依旧指向 bucket $j$，所以，需要分配一个新的 bucket，让该目录项指向它。所以 bucket $j$ 中的记录会有一部分被映射到新分配的 bucket 中。然后再次哈希待插入的记录，将其插入哈希值所指的 bucket 中。最后增加这两个 bucket 的 local length.</li><li><u>如果当前 $i &gt; i_j$</u>，若是该种情况，则不需要再增加目录项，可以直接 rehashing bucket $j$，和上面描述的过程一样（红色字体开始）。</li></ul><p>下面通过一个例子阐述其插入过程，search key 如图 2 所示。<br><img src="/img/hash_indices/4.jpg" alt="图 4"><br>假设可拓展哈希的初始情况如图 4，此时 $i, i_1, i_2$ 都为 1，，已经有三个记录映射到了 bucket 中。现在插入记录（22222, Einstein, Physics, 95000），因为 $h(Physics)$ 的最高位为 1，需要将其插入 bucket 2 中。而现在 bucket 2 已经满了，且 $i_2 &#x3D;&#x3D; i$，所以需要先增加 $i$，让目录项翻倍，然后 rehash bucket 2 中的记录，最后插入新记录，结果如图 5：<br><img src="/img/hash_indices/5.jpg" alt="图 5"><br>可以看到，11 01 是新增的目录项，原二进制串 ‘1’ 指向的 bucket 2，被分裂了，一部分记录被映射到新分配的 bucket 3 中（由 11 指向），新记录插入了 bucket 2 中。而对于 01，新增的 1 位还没用上，所以指向了与它具有相同前缀的 00 目录项所指向的 bucket.</p><p><strong>最后是删除</strong>，同样先通过查找操作找到 bucket，假设位 $j$，然后在 bucket $j$ 中查找指定的记录，若未找到，不做任何操作。若找到了，删除该记录。<font color=red>删除后，bucket 可能为空，若为空，则可以执行合并操作（coalescing）</font>. 合并操作是当 $i&gt;j$ 时对 bucket $j$ 执行分裂操作的逆操作，即回收（释放）bucket $j$，让指向 bucket $j$ 的目录项指向其他 bucket，设为 $p$. 寻找 bucket $p$ 是一个递归的过程，如图 6：<br><img src="/img/hash_indices/6.jpg" alt="图 6"></p><p>图中黄色所示的 bucket 删除后为空。左边和右边是两种不同的情况。<font color=red>左边</font>是该变空的 bucket 释放后，目录项指向新的 bucket，这个 bucket 所具备的特点是，local length 比原先小 1，并且，这两个目录项拥有相同的前缀，前缀长度刚好是这个新 bucket 的 local length. <font color=red>右边</font>，bucket 变空后，释放，然后按照左边那样找新的 bucket，但发现它们都已经变空释放了，所以就一路找到了 local length 为 1 的 bucket，两个目录项的前缀当然还是一样的，只不过前缀的长度为 1 了。所以这是一个递归的过程。</p><p>另外，当太多的的 bucket 变空后会被释放，所有剩余的 bucket 的 local length 的长度都小于目录页的 global length，就可以将目录页减半，即 global length 减少 1，释放内存，如图 6 右边。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可拓展哈希既可以用在内存中存储数据，也可以用作磁盘数据索引结构。当用作磁盘索引结构时，bucket 是一段连续的空间，比如 4KB，里面插入了若干条记录，整个 bucket 会作为一个整体写入磁盘，读出时，再将这段空间解析为 bucket 数据结构（内存中），进行增删改查：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs C++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bucket</span> &#123;<br>    <span class="hljs-comment">// function members (not occupy space)</span><br><br>    <span class="hljs-comment">// data members</span><br>    Pair&lt;KeyType, ValType&gt; records[MAX_NUM]<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">WriteToDisk</span><span class="hljs-params">(Bucket* bucket)</span> </span>&#123;<br>    <span class="hljs-keyword">return</span> io.<span class="hljs-built_in">write</span>((<span class="hljs-type">char</span>*)bucket, <span class="hljs-built_in">sizeof</span>(bucket))<br>&#125;<br><br><span class="hljs-type">void</span> <span class="hljs-built_in">ReadFromDisk</span>(Bucket* bucket, <span class="hljs-type">uint32_t</span> bucket_id) &#123;<br>    <span class="hljs-type">char</span> buffer[<span class="hljs-number">4096</span>];<br>    io.<span class="hljs-built_in">seek</span>(bucket_id * <span class="hljs-number">4096</span>)<br>    io.<span class="hljs-built_in">read</span>(buffer);<br>    bucket = <span class="hljs-built_in">reinterpret</span>&lt;Bucket*&gt;(buffer)<br>&#125;<br></code></pre></td></tr></table></figure><p>下面是书中对可拓展哈希的评价，不想翻译了，直接看原文吧：</p><p><strong>Static Hashing versus Dynamic Hashing</strong></p><blockquote><p>We now examine the advantages and disadvantages of extendable hashing, compared with static hashing. The main advantage of extendable hashing is that performance does not degrade as the file grows. Furthermore, there is minimal space overhead. Although the bucket address table incurs additional overhead, it contains one pointer for each hash value for the current prefix length. This table is thus small. The main space saving of extendable hashing over other forms of hashing is that no buckets need to be reserved for future growth; rather, buckets can be allocated dynamically.</p></blockquote><blockquote><p>A disadvantage of extendable hashing is that lookup involves an additional level of indirection, since the system must access the bucket address table before accessing the bucket itself. This extra reference has only a minor effect on performance. Although the hash structures that we discussed in Section 24.5.1 do not have this extra level of indirection, they lose their minor performance advantage as they become full. A fur- ther disadvantage of extendable hashing is the cost of periodic doubling of the bucket address table.</p></blockquote><blockquote><p>The bibliographical notes also provide references to another form of dynamic hash- ing called linear hashing, which avoids the extra level of indirection associated with extendable hashing, at the possible cost of more overflow buckets.</p></blockquote><br><p><strong>Comparison of Ordered Indexing and Hashing</strong></p><blockquote><p>We have seen several ordered-indexing schemes and several hashing schemes. We can organize files of records as ordered files by using index-sequential organization or B+- tree organizations. Alternatively, we can organize the files by using hashing. Finally, we can organize them as heap files, where the records are not ordered in any particular way.</p></blockquote><blockquote><p>Each scheme has advantages in certain situations. A database-system implementor could provide many schemes, leaving the final decision of which schemes to use to the database designer. However, such an approach requires the implementor to write more code, adding both to the cost of the system and to the space that the system occupies.</p></blockquote><blockquote><p>Most database systems support B+-trees for indexing disk-based data, and many databases also support B+-tree file organization. However, most databases do not sup- port hash file organizations or hash indices for disk-based data. One of the important reasons is the fact that many applications benefit from support for range queries. A second reason is the fact that B+-tree indices handle relation size increases gracefully, via a series of node splits, each of which is of low cost, in contrast to the relatively high cost of doubling of the bucket address table, which extendable hashing requires. Another reason for preferring B+-treesisthefactthatB+-trees give good worst-case bounds for deletion operations with duplicate keys, unlike hash indices.</p></blockquote><blockquote><p>However, hash indices are used for in-memory indexing, if range queries are not common. In particular, they are widely used for creating temporary in-memory indices while processing join operations using the hash-join technique, see <a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E4%BA%A4%E9%9B%86/" title="查询处理篇：交集">查询处理篇：交集</a>.</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>索引</category>
      
    </categories>
    
    
    <tags>
      
      <tag>可拓展哈希</tag>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇五：两阶段提交协议</title>
    <link href="/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/"/>
    <url>/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这一篇文章主要是介绍如何在分布式数据库中保证事务的原子性和数据库的一致性。<font color=red>注意，两阶段提交协议只是用于协调在多个节点执行的事务（同一个事务被分成了多个子事务，并执行在多个节点上）的提交过程，并没有限制各个子事务在本地执行时使用的并发协议</font>。为方便叙述，给出一个分布式数据库的简化模型。<br><img src="/img/two-phase-commit/1.jpg" alt="系统架构"><br>其中 TC 表示 transaction coordinator，负责协调在本地节点发起的本地或全局事务（若是全局事务，则会将其分成若干个子事务并分发给其他节点）。 TM 表示 transaction manager，负责本地或全局（子）事务在本地节点的执行（包括数据访问、锁等）。</p><h2 id="两阶段提交协议（Two-phase-Commit-Protocol）"><a href="#两阶段提交协议（Two-phase-Commit-Protocol）" class="headerlink" title="两阶段提交协议（Two-phase Commit Protocol）"></a>两阶段提交协议（Two-phase Commit Protocol）</h2><p><font color=dark-green>协议执行步骤</font><br>假设节点 $N_i$ 的协调器 $C_i$ 发起了一个全局事务 T，当 $C_i$ 收到了<strong>所有</strong>执行 T （子）事务的节点的消息 —— <strong>T 已经执行完成，$C_i$ 开始 2PC 协议</strong>：</p><ul><li><u>第一个阶段</u>。$C_i$ 向 log 文件中追加一条 &lt;prepare T&gt; 记录并将其 <em>output</em> 到 stable storage。 然后，它发送一条 prepare T 消息给所有执行事务 T 的节点。当某个节点收到该消息后，它的 TM 将决定是否提交事务 T（在该节点执行的部分）。若不同意提交，该节点追加一条 &lt;no T&gt; 记录到 log 文件中，并回复一条 abort T 消息 给 $C_i$. 若同意提交，该节点追加一条 &lt;ready T&gt; 记录到 log 文件中，并将其 <em>output</em> 到 stable storage 中，然后回复一条 ready T 消息到 $C_i$.</li><li><u>第二个阶段</u>。只有当 $C_i$ 收到来自所有节点的 ready T 消息后，T 才能被提交，否则事务 T 必须被中止。如果 $C_i$ 收到了所有的 ready T 消息，它向 log 文件追加一条 &lt;commit T&gt; 记录，否则追加一条 &lt;abort T&gt; 记录，并将记录 <em>output</em> 到 stable storage 中。取决于上一步操作， $C_i$ 将发送一条 commit T 或者 abort T 消息给所有参与的节点，当其他节点收到该条消息后，追加 &lt;commit T&gt; 或 &lt;abort T&gt; 记录到 log 中，并执行相应的提交或中止操作。</li></ul><p>下图展示了，三个节点共同参与事务，并同意提交的情况：<br><img src="/img/two-phase-commit/2.jpg" alt="系统架构"></p><p><font color=dark-green>协议的一些细节</font><br>为防止参与者故障离线，协调器 $C_i$ 会在发送 prepare 消息后开启一个定时器，如果定时器超时后还存在没有回复的节点，那么 $C_i$ 就可以决定中止该事务，并发送 abort T 消息给所有节点。</p><p>在执行两阶段提交协议时，任一一个参与者，可以在发送 ready T 消息之前无条件的中止事务，一旦节点将 &lt;ready T&gt; 追加到日志文件中，事务在该节点就处于 ready state. <strong>事实上， ready T 消息是一个承诺：</strong>该节点对事务的处理将完全按照协调器 $C_i$ 的指令。节点为了在故障恢复后已经信守承诺，所以将必要的日志记录到 stable storage 中。而且，事务所获取到的锁也必须持有直到事务结束。需要注意，协调器所在的节点也会执行事务的一部分，因此 $C_i$ 可以单方面的决定事务的提交还是中止，尽管它收到了所有节点的 ready T 消息。</p><p>当事务的命运被决定后，协调器会发送 commit T 或者 abort T 消息给所有参与的节点，为防止节点未收到该消息，<strong>一些实现中会增加一个步骤：</strong>协调器发送完消息后，会等待节点回复 acknowledge T 消息，当收到所有回复后，协调器追加一条 &lt;complete T&gt; 到日志中。该步骤未完成前，协调器需要记录对事务的决定，因为可能一些未收到消息的节点会询问，该步骤完成后，即可丢弃该事务的信息。</p><h2 id="协议中的故障处理"><a href="#协议中的故障处理" class="headerlink" title="协议中的故障处理"></a>协议中的故障处理</h2><p>假设某次全局事务 T 的发起节点为 $N_i$，协调器为 $C_i$.</p><p><font color=dark-green>非协调器节点 $N_k$ 故障离线</font><br>事务两阶段提交协议执行中，协调器 $C_i$ 检测到节点 $N_k$ 发生故障，会做出如下反应：</p><ul><li>如果故障发生在协调器收到 ready T 消息之前，$C_i$ 会假定 $N_k$ 拒绝提交该事务。</li><li>如果故障发生在协调器收到 ready T 消息之后，$C_i$ 走正常流程，忽视故障。</li></ul><p>当该故障节点 $N_k$ 重新上线后，该节点会检查 log 文件进入恢复流程（详见<a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="事务篇四：Log-Based Recovery System">事务篇四：Log-Based Recovery System</a>），假设事务 T 存在于日志文件中：</p><ul><li>如果日志包含了 &lt;commit T&gt; 记录，$N_k$ 对事务 T 执行 redo 操作。</li><li>如果日志包含了 &lt;abort T&gt; 记录，$N_k$ 对事务 T 执行 undo 操作。</li><li>如果日志只包含了 &lt;ready T&gt; 记录，说明节点故障发生在节点做出承诺之后，事务的后续结果如何无从得知，所以该种情况下，节点必须询问协调器 $C_i$. <u>如果协调器在线</u>，会将结果回复给 $N_k$，该节点根据回复的结果执行 redo 或 undo；<u>如果协调器故障离线</u>，$N_k$ 会向系统中的所有节点询问，收到该询问的节点会查询 log 文件，若有相关的记录则回复，否则忽略，如果没有任何节点能告知 $N_k$，则 $N_k$ 必须周期地发送询问给所有节点直到有答案产生（$C_i$ 必然有相关的记录）。</li><li>如果日志中不包含任何关于 T 的记录，即 abort、commit、ready 都没有，那么可以肯定，$N_k$ 在回复 prepare T 消息之前就发生故障了。结合协调器 $C_i$ 对该种情况的处理，$N_k$ 必须对事务 T 执行 undo.</li></ul><br><p><font color=dark-green>协调器节点 $C_i$ 故障离线</font><br>如果协调器 $C_i$ 在执行提交协议过程中故障离线，剩下的节点就必须决定事务 T 的命运，可能存在以下可能：</p><ul><li>如果某个节点包含了 &lt;commit T&gt; 记录，事务 T 必须提交。</li><li>如果某个节点包含了 &lt;abort T&gt; 记录，事务 T 必须中止。</li><li>如果存在节点不包含 &lt;ready T&gt; 记录（还未回复&#x2F;收到 prepare T 消息），说明这种局势下，就算 $C_i$ 没有发生故障，也不能决定事务的提交与否。比起等待 $C_i$ 恢复，中止 T 更有好处（因为不确定的因素太多）。</li><li>如果上面情况都不满足，说明所有的节点都包含 &lt;ready T&gt; 记录，但是没有额外的记录（即 commit 或 abort）。这种情况 $C_i$ 既有可能已经决定了事务 T 的命运，也可能没有，<font color=red>因此必须要等待协调器 $C_i$ 恢复</font>。</li></ul><br><p><font color=dark-green>网络分区故障（Network Partition）</font><br>网络分区故障是指，整个分布式系统因为网络连接问题被划分成了几个子系统（partitioned），这几个子系统互相之间没有网络连接。单个子系统中可能存在多个节点，也可能只存在一个节点。</p><p>提交协议执行中，当网络分区故障发生后，有如下两种可能：</p><ul><li>协调器 $C_i$ 和其他参与者仍旧处于同一个分区，该种情况下，网络分区故障对该次提交协议没有任何影响。</li><li>协调器 $C_i$ 和其他参与者属于多个分区。不妨假设两个分区，第一个分区包含协调器和部分参与者，第二个分区包含其他参与者。第一个分区内的处理方式和正常情况下处理非协调器故障离线方式一样；第二个分区内的处理方式和正常情况下处理协调器故障离线的方式一样。</li></ul><h2 id="2PC-协议存在的问题"><a href="#2PC-协议存在的问题" class="headerlink" title="2PC 协议存在的问题"></a>2PC 协议存在的问题</h2><p>协调器的失败可能使得某个事务处于模糊状态（remain in doubt）。这段时间内（hours or days），事务需要持持有系统的资源，比如事务持有某些数据的排它锁，在参与该事务提交的节点上，其他新的事务就可能必须等待。因此数据不仅在失败的节点上不可用，而且在活跃的节点上也变得不可用。</p><p>由以上内容可知，<font color=red>2PC 协议最大的缺点是，可能导致多个节点阻塞（blocking），即对某个事务的决策（提交或者中止）需要推迟到协调器重新上线。</font></p><p>另外，引入分布式提交后，故障节点重启后的恢复过程也有些不一样了。这些不一样体现在恢复算法（见<a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="事务篇四：Log-Based Recovery System">事务篇四：Log-Based Recovery System</a>）对待处于不确定状态的事务，即对于该事务，存在 &lt;ready T&gt; 记录在 log 文件中，但不存在 &lt;commit T&gt; 或 &lt;abort T&gt; 记录，因此该节点必须询问其他节点后才能决定该事务提交与否。<font color=red>要知道询问过程可能维持很长一段时间，这段时间内该节点将变得不可用。</font>为了避免该问题，节点收到来自协调器的 prepare T 消息并同意提交后，不再记录 &lt;ready T&gt;，而是 &lt;ready T, L&gt; 到日志文件中，L 表示该事务持有的排它锁。等到重启恢复时，如果存在状态不确定的分布式事务，节点便重新获取这些锁，然后就可以开始正常的其他事务处理了，不需要再等待那些不确定状态的分布式事务，但需要注意，与这些排它锁冲突的事务依然需要等待。</p><h2 id="2PC-协议问题的解决"><a href="#2PC-协议问题的解决" class="headerlink" title="2PC 协议问题的解决"></a>2PC 协议问题的解决</h2><p><font color=dark-green>解决方案一：共识算法</font><br>通过上面的讨论，分布式事务的提交决议过程不能避免等待，问题出在等待的时长。如果协调器故障离线，那么这样的阻塞等待可能持续数小时或数天，使得大规模的节点不可用。在<a href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/" title="Raft 算法问答录">Raft 算法问答录</a>一文中介绍的共识算法就可以用来处理协调器（可以理解为leader）失效的情况。只要大部分的参与者可用，那么等待时间就不会太长。</p><p>采用共识算法处理 2PC blocking 问题的建议早在 1980s 就已经提出了，在 Google Spanner 分布式数据库系统中就采用了该算法。</p><br><p><font color=dark-green>解决方案二：3PC 协议</font><br><a href="https://blog.csdn.net/qq_31960623/article/details/116429261">见博客</a></p><br><p><font color=dark-green>解决方案三：Persistent Messaging</font><br>未完待续</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇三：事务的并发控制</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a>中介绍了事务基本概念和具有特性，在<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a>中介绍了调度要具备何种性质，才能在并发事务执行后使系统处于一致性状态和事务失败后能让系统回到安全状态，还论证了隔离性与一致性的关系。</p><p><strong>并发控制的工作就是，在事务并发时，确保只会产生具备这种性质的调度方案</strong>，具体地，并发控制策略的目标是同时保证两点：</p><ul><li>使系统具有较高的事务并发度</li><li>确保生成的调度方案，同时是冲突可串行化（conflict serializable），可恢复（recoverable）和非失败连锁式（cascadeless）的。</li></ul><p>并发控制策略有许多，<font color=red>本文主要集中在两阶段加锁（two-phase locking）和快照隔离（snapshot isolation）</font>。介绍完这两个并发控制协议后，再讨论各个隔离级别的实现，如何避免事务总论中提到的现象。</p><h2 id="Two-Phase-Locking-Protocol"><a href="#Two-Phase-Locking-Protocol" class="headerlink" title="Two-Phase Locking Protocol"></a>Two-Phase Locking Protocol</h2><p>为了简单，目前讨论的事务只有两种操作：访问（read）和更新（write），实际上还有如：插入（insert）、删除（delete）和含条件的读取（predicate read），它们会放在以后慢慢讨论。</p><p>同时，现在只讨论操作针对的是单个数据（如元组），这其实是锁的粒度问题，后面会讨论大粒度锁的情况。</p><p>为此，最简单的锁具有两种模式：</p><ul><li>共享锁（shared-mode lock: S） </li><li>排他锁（exclusive-mode lock: X）</li></ul><p>在基于锁的的协议中，执行 read 操作时会对数据加共享锁，执行 write 操作时会对数据加排他锁，两种锁的相容性（compatibility）如下：</p><table><thead><tr><th align="center"></th><th align="center">S</th><th align="center">X</th></tr></thead><tbody><tr><td align="center">S</td><td align="center">true</td><td align="center">false</td></tr><tr><td align="center">X</td><td align="center">false</td><td align="center">false</td></tr></tbody></table><p><img src="/img/concurrency-control/txn-3.jpg" alt="事务3"></p><h3 id="两阶段加锁协议"><a href="#两阶段加锁协议" class="headerlink" title="两阶段加锁协议"></a>两阶段加锁协议</h3><p><font color=dark-green>基础版两阶段加锁协议</font><br>该协议要求一个事务请求加锁和释放锁分别集中在两个阶段：</p><ul><li>growing phase. 该阶段事务可以申请加锁，而不能释放任何锁</li><li>shrinking phase. 该阶段事务可以释放锁，但不能申请新的锁</li></ul><p>如图事务3满足两阶段加锁要求，需要注意，基本的两阶段加锁协议并不要求所有的释放锁操作都必须放在最后，如事务3将 unlock(B) 可以移到 lock-X(A) 后面，仍然满足两阶段加锁的定义。</p><br><p><font color=dark-green>基础版两阶段加锁协议的特点</font>：</p><ul><li><u>保证了 conflict serializability</u>. 假设一个调度内的事务都遵守该两阶段假设协议，同时将事务 growing phase 获取到的最后一个锁的位置定义为 lock-point，那么各个事务就可以根据其 lock-point 排序，该顺序一定是可串行化的。（证明：可以构造该调度的先行图，先行图有两种可能，要么无环，要么有环。若无环调度冲突可串行化；若有环，事务必然是相互等待对方到达 shrinking phase 释放冲突的锁，这是死锁。）</li><li><u>可能存在死锁</u>。如上</li><li><u>不能保证 recoverable 和 cascadeless rollback</u>.</li></ul><p>先行图、recoverable 和 cascadeless 的定义见<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a></p><br><p><font color=dark-green>如何保证 recoverable 和 cascadeless？</font><br>这里引入两种升级版两阶段加锁协议，一步到位之间使得满足这些协议的调度满足 cascadeless（自然就满足了 recoverable）。</p><p><strong>Strict Two-Phase Locking Protocol</strong>:<br>在基础版两阶段加锁协议上，增加了这样一条要求，所有的 exclusive-mode lock 必须要等到事务提交之后再释放。</p><p><strong>Rigorous Two-Phase Locking Protocol</strong>：<br>在基础版两阶段加锁协议上，增加了这样一条要求，所有的 lock 必须要等到事务提交之后再释放。</p><p>依据 cascadeless 的定义，它们显然满足。这两种方式被广泛用于商业数据库系统中，也就是锁的释放通常要等到事务完成（提交或终止）。</p><br><p><font color=dark-green>如何处理死锁？</font><br>事务存在循环等待即进入了死锁，处理死锁一般有两种思路，这两种思路都会涉及到事务的回滚：</p><ul><li><u>死锁避免</u>。该思路是让系统永远不会进入死锁状态，一般用于死锁发送频率高的场景中。</li><li><u>死锁检测 + 恢复</u>。该思路是在死锁发生后在人为干预，解除死锁状态，一般用于死锁发生频率较低的场景。</li></ul><h3 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h3><p>实现死锁避免也有许多思路：</p><ul><li><u>破除循环等待条件</u>。实现方法有 1)通过给所有数据规定一个次序，加锁只能按该顺序加锁；2)事务开始之前完成对所有数据的加锁。最大的问题在于，如何准确的知道事务会涉及哪些数据。</li><li><u>基于抢占和回滚事务方式</u>。抢占很好理解，当两个事务发生冲突了，优先级高的事务可以让优先级低的事务回滚，下面主要介绍这种方式。</li></ul><p><font color=dark-green>基于抢占和回滚事务方式的死锁避免算法</font><br>抢占需要确定优先级，下面的两个算法采用时间戳（timestamp）的方式决定优先级的大小。每个事务在其开始时得到一个时间戳，时间戳越小，代表事务越老，若事务回滚重新开始，那么它保持时间戳不变。注意，该时间戳只用于两个事务发生锁冲突时：</p><ul><li><u>wait-die 算法</u>。当事务 TA 对某个数据加锁时，发现事务 TB 已经已经持有该数据的排它锁，若 TA 时间戳小于 TB 的时间戳，那么 TA 更老，选择等待；若 TA 的时间戳更大，则 TA 回滚。很明显，该算法不涉及抢占。该算法为何能避免死锁？因为它避免了循环等待的条件，假设形成了环，环首持有环尾需要的锁，那么环尾会直接回滚，矛盾。</li><li><u>wound-wait</u>。和上个算法不同的是，该算法涉及抢占，若 TA 的时间戳更小（TA 更老），那么 TA 会直接抢占该锁，TB 则回滚；若 TA 的时间戳更大，则 TA 等待。其避免死锁的证明同上。</li></ul><p>以上两个算法最大的缺点在于可能会造成不必要的回滚，一个简单的优化是在回滚之前先等待一段时间，不过这段时间对每个事务最好是随机的，而且其长度难以确定。</p><p>死锁检测与恢复不想写了，有时间再写吧。</p><h3 id="更丰富的锁粒度"><a href="#更丰富的锁粒度" class="headerlink" title="更丰富的锁粒度"></a>更丰富的锁粒度</h3><p>一个简单的例子，当锁粒度只有元组时，一个事务想更新一张表，那它就必须对表的所有元组加锁，显然不合适，反之，若只更新几个元组，用不上对整张表加锁。</p><p>多粒度一般采用分级结构实现，该结构可以称为 multiple-granularity tree，如下图：<br><img src="/img/concurrency-control/multiple-granularity.jpg"></p><p>（未完待续）</p><h2 id="Snapshot-Isolation"><a href="#Snapshot-Isolation" class="headerlink" title="Snapshot Isolation"></a>Snapshot Isolation</h2><p>快照隔离（snapshot isolation）属于多版本并发控制技术的一种。多版本并发技术通过维护一份数据的多个版本，让事务可以访问（read）数据的上一个版本（当前版本可能正在被修改），而不是当前未提交事务正在修改的版本（该未提交事务可能更老，即开始于当前事务之前；也可能发生在未来，即如果按照串行执行，当前事务本应该看不到后面事务的更新，但并发执行，就有可能了）。</p><p>版本的概念并不容易理解，后面详细介绍 snapshot 后，再理解它在 percolator 的实现，这样就清楚得多了。</p><br><p><font color=dark-green>基本概念</font><br>当事务开始执行时，数据库给该事务一个 snapshot，里面包含的是该事务需要的、已被提交的数据，之后该事务对数据的所有操作都在该 snapshot 上操作（该 snapshot 暂存在事务私有内存中，其他事务不可见）。对于只读事务，操作完就可以结束了（提交或者失败），不需要等待，也不会因为并发而被回滚；对于更新事务，它还需要将更新写入数据库（写入应当是一个原子操作），因此还涉及到一个验证步骤，后面会详细结束。</p><p>在快照隔离中，只读事务不需要等待，也不会 abort，<font color=red>那会不会有不可重复读的可能呢？毕竟多次读取，可不可能在某一次就读到了刚刚被更新的值呢？</font>不会的，因为读取到快照后，该快照被该事务独享（或者说被只读事务共享），不会涉及到更新。</p><br><p><font color=dark-green>实现细节</font><br>该协议给进入系统的每个事务两个时间戳：</p><ul><li><u>startTS</u>. 在事务开始时获取</li><li><u>commitTS</u>. 在事务准备写入更新到数据库的时间（或者是开始验证阶段的时间）</li></ul><p>每个被更新的数据都带有一个时间戳（即数据的版本号），即执行该更新事务的 commitTS，也就是说同一份数据可能存在多个版本。时间可以是系统时间，也可以是逻辑时间（计数器），只要保证不会存在相同的时间戳即可。</p><p>当一个事务 TA 读取一个数据时，读取到的数据具有这样的特点：在所有版本号小于等于 startTS(TA) 的数据中具有<strong>最大的版本号</strong>。从事务角度理解，一个事务看不到，任何在该事务开始之后提交的事务所作的更新。</p><br><p><font color=dark-green>更新验证阶段</font><br>按照上面所说，事务读取数据后各自操作，其他事务感知不到，那么就可能会出现两个并发的事务更新同一个数据，如果允许这两个事务都修改数据库，那么就会发生值覆盖，也就是更新丢失（lost update）。如何解决冲突，是验证阶段的工作。</p><p>一般有两种方法可以防止更新丢失，介绍之前先看一下 snapshot isolation 中的并发定义，当下面情况中任一发生时，就说事务 TA 和 TB 并发：</p><ul><li>startTS(TA) &lt;&#x3D; startTS(TB) &lt;&#x3D; commitTS(TA)</li><li>startTS(TB) &lt;&#x3D; startTS(TA) &lt;&#x3D; commitTS(TB)</li></ul><p><strong>第一种方法称为 first committer wins</strong>. 假设同一时间只有一个事务在验证。当事务 TA 准备写入更新时，执行如下步骤：</p><ol><li><u>检测是否存在冲突</u>。检查 TA 更新的每一个数据，看是某个数据否存在一个版本号落在 [startTS(TA), commitTS(TA)].</li><li><u>若有</u>，则 TA abort.</li><li><u>若无</u>，正常写入更新和后续步骤，比如提交。</li></ol><p><strong>第二种方法称为 first updater wins</strong>. 该方法会应用到锁机制，当 TA 准备写入更新时，会申请需要写入数据的写锁，根据锁是否申请成功，有两种情况：</p><ul><li><u>写锁申请成功</u>，若当前数据已经被并发的事务更新了（检测方法同上），则 TA abort，否则执行正常的写入更新和后续步骤，比如提交。</li><li><u>写锁申请失败</u>，假如该锁被 TB 占有，TA 等待 TB abort 或者 commit。若 TB abort，TA 获取到该锁，后续操作同上一种情况。若 TB commit，那么 TA 只能 abort.</li></ul><br><p><font color=dark-green>Snapshot Isolation 问题与解决</font><br>snapshot isolation 吸入的地方在于，读写分离，互不干扰。<strong>但它有一个致命的缺陷：不保证可串行化（serializability）</strong>，如下例子：<br><img src="/img/concurrency-control/snapshot-isolation-1.jpg"></p><p>如果构造该例子的先行图，会发现存在环：该调度非冲突可串行化。在两阶段加锁协议中该调度会产生死锁，不可能顺利执行下去。然而，在快照隔离策略中却可以执行、提交。如果这两个事务以可串行化的调度方案并发，那么 A、B 的值是一样的，具体的值取决于谁先执行，而快照隔离的结果却是将它们的值交换。</p><p><strong>这种现象被称为写偏斜（write skew）</strong>：两个事务读取了对方会更新的数据，但这两个事务更新的数据却没有交集，如 Ti 读取了会被 Tj 更新的 B，Tj 读取了会被 Ti 更新的 A，而它们分别更新 A 和 B，没有交集，所以在验证阶段无法被检测出来。</p><p><font color=red>发生写偏斜的根本原因是什么？snapshot isolation 不能有效追踪防止 read-write conflict，验证阶段的连个方法都只限制了 write-write conflict</font>. 借助上面例子可以发现，发生写偏斜时，调度方案的先行图存在两条 read-write 冲突边，一条入边，一条出边：对于事务 Ti 写入了 A 的一个新版本，而 Tj 读取了 A 之前的一个版本（Tj -&gt; Ti, read-write edge）；对于事务 Tj 写入了 B 的一个新版本，而 Ti 读取了 B 之前的一个版本（Ti -&gt; Tj, read-write edge）：<br><img src="/img/concurrency-control/snapshot-isolation-2.jpg"></p><p><strong>由此，一种新的技术称为 Serializable Snapshot Isolation(SSI) 被提出以保证快照隔离可串行化</strong>。SSI 会追踪并发事务之间的所有 read-write 冲突，检查是否同时存在出边和入边，如果存在，则其中一个事务会被回滚。</p><p>趁热打铁读一读 snapshot isolation 的工业级实现：<a href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/" title="事务篇六：Percolator 随笔">事务篇六：Percolator 随笔</a>呀。</p><h2 id="隔离级别的实现"><a href="#隔离级别的实现" class="headerlink" title="隔离级别的实现"></a>隔离级别的实现</h2><p><font color=dark-green>如何实现读已提交级别？</font><br>读已提交有如下特点：</p><ul><li>从数据库中读时，只会读到已经提交了的数据，即<u>没有脏读（dirty read）</u></li><li>写入数据库时，只会覆盖已经写入的数据，即<u>没有脏写（dirty write）</u></li><li>可能发送不可重复读异常</li></ul><p><strong>何为脏读？</strong>能读取到尚未 commit&#x2F;abort 事务所做的更新，就叫脏读。脏读可能引发只能看到部分更新的问题，比如转账，很可能会出现账户余额蒸发的怪现象。</p><p><strong>何为脏写？</strong>当两个事务更新同一对象时，通常后者会覆盖前者所做的更新，但若是覆盖的是尚未 commit&#x2F;abort 事务的更新，就叫做脏写。脏写同样会产生问题，考虑自动发送邮件例子，一个事务刚刚填入收件人地址，还未提交，另一个事务却覆盖了这个邮件地址，邮件就会发往错误的地方。</p><p>一般数据库通过行锁（row-level lock）<strong>防止脏写</strong>。当事务想要修改特定对象时，它必须首先获得该对象的排它锁，持有到事务提交或中止。可以通过读锁<strong>防止脏读</strong>，然而该方法中，写事务会阻塞读事务，所以数据库通常会为数据保留一个最近的旧值，和正在更新的新值，若新值还未提交，任何读事务都会拿到旧值，若新值提交了，就会读到新值（发生不可重复读异常）。</p><br><p><font color=dark-green>如何实现读未提交级别？</font><br>读未提交有如下特点：</p><ul><li>写入数据库时，只会覆盖已经写入的数据，即<u>没有脏写（dirty write）</u></li><li>可能发生脏读</li><li>可能发生不可重复读异常</li></ul><p>防止脏写的实现方式和读已提交级别一致。</p><br><p><font color=dark-green>如何实现可重复读级别？</font><br>可重复读有如下特点：</p><ul><li>没有脏读</li><li>没有脏写</li><li>不会发生不可重复读异常</li><li>若使用快照隔离提供稳定的视图，则不会出现幻读</li></ul><p>读已提交维护数据的两个版本，但可能会读取到已提交的新值，而对于可重复读级别，可以使用快照隔离，让事务始终读取最开始读取的那一个版本，维持稳定的视图，来<strong>防止不可重复读异常，同时也解决了幻读异常</strong>。</p><p>这里再解释下幻读。下面是一张教师（instructor）薪资表：</p><table><thead><tr><th align="center">ID</th><th align="center">name</th><th align="center">salary</th></tr></thead><tbody><tr><td align="center">001</td><td align="center">Jery</td><td align="center">1000</td></tr><tr><td align="center">002</td><td align="center">Peter</td><td align="center">2000</td></tr><tr><td align="center">003</td><td align="center">Tim</td><td align="center">3000</td></tr></tbody></table><p>现在执行如下的 SQL 语句：</p><p><strong>select</strong> <em>ID</em>, <em>name</em><br><strong>from</strong> <em>instructor</em><br><strong>where</strong> <em>salary</em> &gt; 1000</p><p>假设当前系统采用可重复读隔离级别，一个用户执行了如上的查询，同时另外一个用户执行了如下的插入语句（删除语句也有如下的效果）：<br><strong><center>insert</strong> <strong>into</strong> <em>instructor</em> <strong>values</strong> (004, James, 4000)</center><br>查询事务读取到多少条的数据取决于它和插入事务的先后顺序，若查询事务多次读取，就有可能发生某次读到的内容比上次多，即发生了幻读。</p><br><p><font color=dark-green>如何实现可串行化级别？</font><br>采用 SSI 和 两阶段加锁都可以实现。</p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
      <tag>mvcc</tag>
      
      <tag>snapshot</tag>
      
      <tag>two-phase-locking</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇二：事务的可串行化</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该文章的内容来自 database system concepts 17.5-7，建议先看文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a></p><p>为什么会有事务的串行化？因为数据库系统会允许事务的并发（concurrency），事务并发有如下两个优点：<br><strong>提升吞吐量和资源利用率</strong>。事务通常包含多个操作（步骤），一些操作需要更多的 CPU 计算，而另外一些需要更多的 I&#x2F;O. CPU 和 I&#x2F;O 设备通常可以并行（parallel），如果同时允许多个事务并发，能减少 CPU 和 I&#x2F;O 的空闲时间，那么可以提升吞吐量和资源利用率。</p><p><strong>减少等待时间</strong>。如果所有事务只能串行，那么短事务只能等待长事务结束才能执行。如果事务能够并发，则能在一定程度上共享 CPU 和磁盘资源，减少响应时间。</p><p><strong>并发很有好处，但它可能会破坏事务的隔离性，破坏数据的一致性</strong>。因此有必要研究各个并发的事务需要满足什么关系，才能保证隔离性和数据库的一致性。并发的具体实现方案，参见文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a>。</p><h2 id="Schedules"><a href="#Schedules" class="headerlink" title="Schedules"></a>Schedules</h2><p><strong>考虑一个例子</strong>：有 A 和 B 两个账户，其初始值分别为 1000 和 2000，事务 T1 从账户 A 转账 50 到账户 B，而事务 T2 从账户 A 转账 10% 到账户 B。这两个事务包含许多指令（instructions），当它们并发时，这些指令可能以各种顺序执行（但属于同一个事务的的各指令间相对顺序一定，而属于不同事务的指令间顺序可能变化），这样具体的一个顺序称为一个调度（schedule），如下面的调度1和调度2<br><img src="/img/schedule-serializable/schedule1.jpg" alt="schedule 1"><br><img src="/img/schedule-serializable/schedule2.jpg" alt="schedule 2"></p><p><strong>串行调度（serial schedule）</strong>：在调度中，属于同一个事务的指令出现在一起。如 schedule 1 是串行的调度，而 schedule 2 不是。对于 n 个事务，可以组成最多 n! 个不同的串行调度。</p><p>很明显，串行的调度方案一定能保证隔离性和一致性，但在并发时，不同事务的指令可能交叉，调度并不总是串行的，但如果能保证一个调度方案对数据库的修改与某个串行调度方案的修改结果一致，那么该调度方案就保证了一致性，如上图调度1执行后，A+B&#x3D;3000，调度2执行后，A+B&#x3D;3000，这样的调度等价于串行调度，被称为<strong>可串行化调度（serializable schedule）</strong>。</p><h2 id="Conflict-Serializability"><a href="#Conflict-Serializability" class="headerlink" title="Conflict Serializability"></a>Conflict Serializability</h2><p>但我们不能将并发事务的调度方案执行后，根据结果来判断这样的并发是否能保证数据库的一致性。需要采取其他办法判断。</p><p>这里先介绍冲突可串行化（conflict serializability）的概念。</p><p>冲突的定义为：假设，指令 I 和 J 属于不同的事务且对相同的数据执行操作，当 I 和 J 之中至少有一个是修改操作（write）时，I，J 冲突。</p><p>对于非冲突的指令，我们可以交换它们的顺序而不会影响调度最终的结果（如交换调度2中 T1 的 read(B) 和 T2 的 write(A)），而对于冲突的指令，交换它们则会产生影响。如果一个调度方案 S1 能够通过交换一系列不冲突指令后，变成调度方案 S2，那么 S1 和 S2 冲突等价（conflict equivalent），如果 S2 恰好是一个串行调度方案，那么 S1 就可以称为<strong>冲突可串行化</strong>（与串行调度冲突等价）。</p><p>由上可知，如果一个调度方案冲突可串行化，那么它能保证数据的一致性，现在问题是如何判断一个调度是否冲突可串行化？构造<strong>先行图（precedence graph）</strong>后可以很容易的判断。</p><p>先行图是一个有向图，其顶点表示并发的事务，假设在一个调度方案 S 中有 TA 和 TB 两个并发事务，Q 表示某个数据。当且仅当以下任何一种情况发生时，TA 到 TB 有一条边 （TA -&gt; TB）：</p><ul><li>TA 执行 write(Q) 之后 TB 会执行 read(Q)</li><li>TA 执行 read(Q) 之后 TB 会执行 write(Q)</li><li>TA 执行 write(Q) 之后 TB 会执行 write(Q)</li></ul><p>如果边 TA -&gt; TB 存在，那么在任何与 S 等价的串行调度方案中，TA 都要先于 TB 执行。</p><p><strong>假如某个调度方案的先行图含有环，那么，该调度就不是冲突可串行化的，若没有环，则该调度是冲突可串行化的</strong>。</p><p><font color=red>调度可串行化就万事大吉了吗？</font>上面讨论的内容都没有提到事务失败的情况，见下图：<br><img src="/img/schedule-serializable/schedule3.jpg" alt="schedule 3"></p><p>很明显，调度3是冲突可串行化的。如果 T6 执行完 read(B) 后提交前失败了，按照原子性定义，T6 需要回滚，需要注意，T7 读取的数据正是 T6 更新的数据，而 T6 回滚时，T7 已经提交了。</p><p>一个允许事务并发的系统，为保证原子性，若一个事务失败了，依赖于这个事务的其他事务（即这些事务读取了失败事务所做的更新）都需要失败终止和回滚。在调度3中，T7 依赖于 T6，所以 T7 也需要回滚，但已经不可能了。</p><p>像调度3这样的调度方案属于不可恢复调度（nonrecoverable schedule）。</p><p><strong>可恢复调度（recoverable schedule）</strong>是指，在一个调度中，对于任一两个事务 TA, TB，若 TA 读取了 TB 所做的更新，那么 TB 的提交操作必须出现在 TA 的<strong>提交</strong>操作之前。若 T7 的提交操作延迟到 T6 提交之后，那么调度3就是可恢复的。</p><p>为了提升性能，还需要介绍一种调度：cascadeless schedule (不知道怎么翻译，暂译作非失败连锁式调度)</p><p>考虑下面调度4的情况：<br><img src="/img/schedule-serializable/schedule4.jpg" alt="schedule 4"></p><p>调度4中，T9 依赖 T8，T10 依赖 T9，若 T8 失败终止，那么 T9 和 T10 都需要失败终止。调度4同时满足冲突可串行化和可恢复，但单个事务的失败引起太多的事务失败，这显然会降低系统的性能，这样的调度被称为失败连锁式调度（cascade schedule）。</p><p>这里给出 <strong>cascadeless schedule 的定义</strong>：在一个调度中，对于任一两个事务 TA, TB，若 TA 读取了 TB 所做的更新，那么 TB 的提交操作必须出现在 TA 的<strong>读</strong>操作之前，显然，所有的 cascadeless 调度都是可恢复的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>为了保证事务并发执行和单个按次序执行之间结果的一致性，提出了可串行化调度的定义。为了更加容易判断一个调度是否可串行化，引出了冲突可串行化的定义。可串行化调度能保证事务都成功执行后的一致性，但未能保证事务失败后的原子性，为了保证原子性，提出了可恢复调度的概念。再进一步，为了减少事务失败所引起的回滚操作，在可恢复之上再做限定，给出了 cascadeless 调度的定义。</p><p><font color=red>文首提到，并发很有好处，但它可能会破坏事务的隔离性，破坏数据的一致性，但通篇只提到了一致性，并没有涉及隔离性，难道是事务并发破坏了事务的隔离性？</p><p>假设系统初始处于一致状态，事务编写正确，即每个事务执行后，系统仍处于一致状态，那么事务的隔离性实际上就保证了：如果所有事务按照串行执行，最终系统仍处于一致性状态。</p><p>事务的一致性和隔离性的区别在哪？一致性需要满足系统初始处于一致状态，事务的逻辑正确，那么保证了隔离性就能保证事务的一致性。</p><p>再次回忆，可串行化实际上就是按照事务并发后其系统所处的状态是否和事务按串行顺序执行后的状态一致来定义的，也就是说，满足了可串行化，即满足了隔离性！！！</font></p><p>在文章 <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a>，会介绍采取何种方法，能生成满足这些定义的调度方案。</p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇四：Log-Based Recovery System</title>
    <link href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/"/>
    <url>/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><img src="/img/log-based_recovery_system/1.png" alt="磁盘交互模型"><br>事务的原子性和一致性（数据库数据一致性）的保证，除了在事务代码的编写（逻辑）、事务的调度（并发）上面下功夫外，还需要考虑系统故障的发生。当系统从故障中恢复后，应当正确处理这些异常，以保证事务的原子性和数据的一致性。</p><p><font color=dark-green>一个例子</font></p><p>考虑银行有两个账户 A 和 B，初始账户分别为 1000 和 2000，一个事务负责从账户 A 转账 50 到账户 B。假设故障发生在事务执行过程中，那么有如下可能发生：</p><ol><li>A &#x3D; 950, B &#x3D; 2050</li><li>A &#x3D; 950, B &#x3D; 2000</li><li>A &#x3D; 1000, B &#x3D; 2050</li><li>A &#x3D; 1000, B &#x3D; 2000</li></ol><p>根据磁盘交互模型图可知，事务需要先将需要的数据从 buffer pool 读入自己的私有内存，等到操作完成再将更新后的数据写入 buffer pool，至于这些数据何时会写入磁盘与 buffer pool 页面置换算法有关。上面情况 1 表示一切正常；情况 2 表示含有 A 新值的 block 被正常写入磁盘后系统崩溃；情况 3 表示含有 B 新值的 block 被正常写入磁盘后系统崩溃；情况 4 表示两个 block 都还未写入磁盘就发生故障（当然 A 和 B 可能存在于同一个 block 中）。在情况 2 和 3 中数据库一致性都已被破坏，事务的原子性也未能保证。</p><h2 id="Log-Based-Recovery-System"><a href="#Log-Based-Recovery-System" class="headerlink" title="Log-Based Recovery System"></a>Log-Based Recovery System</h2><p>为了保证事务的原子性和数据的一致性，需要一种方法从故障中恢复被部分修改的数据，目前最广泛使用的方式是：<strong>在更新数据库之前，先将描述修改的信息记录到 stable storage 中的 log 文件里</strong>，等到系统重启后，根据这些信息来恢复数据。这些信息被称为 log records，这个方法被称作 log-based recovery.</p><h3 id="Log-Records-amp-事务流程"><a href="#Log-Records-amp-事务流程" class="headerlink" title="Log Records &amp; 事务流程"></a>Log Records &amp; 事务流程</h3><p>当事务开始时，会向日志中追加一条事务<strong>开始记录</strong> &lt;$T_i\ $start&gt;，$T_i$ 表示事务标识符；在将更新写入数据库之前，会向日志中追加一条<strong>更新记录</strong> &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt;，$X_i,\ V_1,\ V_2$ 分别表示数据标识符（通常用磁盘块号和偏移量）、该数据的旧值和新值；等到事务提交时，会向日志中追加一条<strong>提交记录</strong> &lt;$T_i\ $commit&gt;，如下面例子：<br><img src="/img/log-based_recovery_system/2.png" alt="图 2"><br>一旦这些记录被写入了日志文件中，系统就可以将事务的修改应用到数据库中了，就算故障发生，也能根据日志文件重放（replay）这些操作，恢复它们，正因为如此，日志文件必须持久化（写入 stable storage，定义见<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a>），以保证永久不丢失。需要注意，$T_0,\ T_1$ 的日志记录可能是相互交叉的。</p><p>现在在细节上更进一步，所谓的将记录追加到日志文件中、更新写入数据库中这些操作实际上包含两个步骤：</p><ol><li>将事务私有内存空间的数据写入某个 buffer block；</li><li>将 buffer block 写到磁盘上。</li></ol><p>第二步由数据库系统执行，发生的时机是不定的，<strong>所以当第一步发生后，就算是对文件和数据库做出了修改</strong>。需要明白，buffer block 中的数据可能会在内存中存在很长一段时间才会被写入磁盘（当然可以让第二步即时生效，但磁盘 I&#x2F;O 代价昂贵，非必须，一般是推迟写磁盘），在这段时间内系统完全有可能发生故障，所以事务提交流程中有一些<font color=red>原则（write-ahead logging, WAL, rule）必须要遵守，以保证事务的原子性</font>：</p><ul><li>事务已提交是指（可以回复客户端了），提交记录 &lt;$T_i\ $commit&gt; 已经被 <em>output</em> 到 stable storage 中。</li><li>在上一步执行之前，该事务的所有其他记录（存在于 log buffer 中）都被 <em>output</em> 到 stable storage 中。</li><li>在数据被 <em>output</em> 到数据库之前，与这些数据更新相关的所有日志记录都已经被 <em>output</em> 到 stable storage 中。</li></ul><p>只要遵守了以上原则，无论是系统崩溃后恢复还是事务的正常 abort，系统都可以根据这些记录将系统恢复到一致状态，事务自然也就保证了原子性。因此日志记录在 log 文件（stable storage）中的<strong>顺序十分重要</strong>，必须和写入 log buffer（buffer block）的顺序一致。</p><h3 id="Recovery-Algorithm"><a href="#Recovery-Algorithm" class="headerlink" title="Recovery Algorithm"></a>Recovery Algorithm</h3><p>在介绍恢复算法之前，先看看事务在正常情况下的流程，第一种是事务因失败需要 abort 回滚；第二种是事务正常提交。</p><p><strong>Case 1: 事务正常提交</strong>。这种情况下事务不需要回滚，日志文件就像图 2 所示那样。</p><p><strong>Case 2：正常 abort，事务回滚</strong>。当事务 $T_i$ 执行到一半时因为某些原因失败需要 abort 回滚（rollback 之后才能算 aborted）。回滚也就是将该事务对数据库所做的修改撤销，系统做如下操作（如图 3 示）：</p><ul><li>系统向后扫描 log 文件（注意 log 文件是只追加的，向前是指追加的方向），对于每一条属于 $T_i$ 的更新记录 &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt;，系统将使用旧值 $V_1$ 更新 $X_i$ （即撤销修改），同时向日志追加一条 redo-only 记录 $&lt;T_i,\ X_j,\ V_1$&gt;。</li><li>当遇到记录 &lt;$T_i\ $start&gt;，系统向日志追加一条 &lt;$T_i\ $abort&gt; 记录，对于该事务的回滚也就结束了。</li></ul><p>系统回滚所添加的这些日志记录又被称为 compensation log records. 其实可以这样理解，事务中止处于未完成状态，系统创建一个互补事务，负责撤销事务所做的更改，并补充完整日志记录。</p><p><img src="/img/log-based_recovery_system/3.png" alt="图 3"></p><br><p><font color=dark-green>恢复算法思想</font><br>系统重启后是很懵逼的，它不清楚执行过的事务的具体情况，所以它需要查看 log 文件，根据日志记录的完整性，可以将事务分为两种：</p><ul><li><p><u>事务执行完成（committed 或 aborted）</u>。当系统重启后，发现某事务在 log 文件中包含了完整的日志记录，同时具有 &lt;$T_i\ $start&gt; 和 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt;。根据 WAL rule，尽管日志表明该事务已经完成，无论是事务自己正常提交还是系统所做的互补操作，但它不能确定这些操作已经写入数据库中（很可能系统在将 log buffer <em>output</em> 到 stable storage 之后就失败了，还未来得及将 data buffer <em>output</em> 到数据库），<strong>因此它必须依据日志 redo 这些操作</strong>。</p></li><li><p><u>事务执行未完成</u>。当系统重启后，发现某事务在 log 文件中没有包含完整的日志记录，即缺少了 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt; 记录，为保证事务的原子性和数据的一致性，该事务所做的操作必须撤销，<strong>也就是必须 undo 这些操作</strong>。当然，系统有可能在 rollback 未完成时就发生了故障，也就是日志中含有不完整的 redo-only 记录，undo 这些记录实际上就是 redo 它们。</p></li></ul><p>上面简单描述了恢复算法的的思想，很朴素，<font color=red>但却存在很大的性能问题</font>，试想在系统重启之前可能存在成千上百的事务执行大量的操作，这些操作都记录在了 log 文件中，log 文件可能变得十分巨大，若系统重启后对这些操作全都执行 redo 或 undo，系统可用性就会很差，何况，有很多操作已经实实在在写入了数据库，不需要再做额外的操作。为了减少恢复时间，数据库系统通常采用一种简单的方法，称作 checkpoints. </p><br><p><font color=dark-green>Checkpoints 思想</font><br>checkpoint 是一组如下的操作：</p><ol><li>从 log buffer <em>output</em> 所有的日志记录到 stable storage 中。</li><li>从 data buffer <em>output</em> 所有的数据修改到数据库磁盘文件中。</li><li>系统向日志文件追加一条 &lt;checkpoint L&gt; 记录，并 <em>output</em> 到 stable storage 中。L 是一组事务标识符的集合，表示执行 checkpoint 时，还处于活跃状态的事务集合。</li></ol><p><font color=red>那么有了 checkpoint 后，又当如何缓解上述问题呢？</font>假设存在一个事务 $T_i$，它在执行 checkpoint 操作之前已经完成（committed 或 aborted），那么该事务的相关日志记录要么在 checkpoint 之前 <em>output</em> 到日志文件了，要么在 checkpoint 过程中 <em>output</em> 到日志文件，总之这些记录出现在 &lt;checkpoint L&gt; 之前，即 L 中不包含 $T_i$，系统重启后，也自然没有必要 redo $T_i$ 的操作了。随着日志文件越来越大，有了 checkpoint 记录，就可以丢弃以前的记录了，反正也用不上了。</p><br><p><font color=dark-green>恢复算法详细步骤</font><br>有了以上的知识背景，现在可以提出完整的 log-based recovery algorithm 步骤了，当数据库系统从崩溃中重启后，会完成如下两个部分的工作：</p><p><strong>重放阶段（Redo Phase）</strong>. 该阶段，系统从最后一条 checkpoint 记录开始<font color=red>向前扫描</font>（追加方向），重新执行具有完整日志记录（含有 commit 或 abort 记录）的事务所做的操作。另外，该阶段在扫描时，还会记录 undo-list，包含那些没有完整记录（不含有 commit 或 abort 记录）的事务标识符，这些事务要么出现在最后一条 checkpoint 记录的 L 中，要么在最后一次 checkpoint 操作后才开始。在扫描过程中，有如下步骤：</p><ol><li>使用最后一条 checkpoint 记录中的 L 集合初始化 undo-list</li><li>当扫描到一条更新记录 &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt; 或者 redo-only 记录 &lt;$T_i,\ X_j,\ V_1$&gt; 时，系统重新执行记录表示的操作。</li><li>当扫描到一条 &lt;$T_i\ $start&gt; 记录时，将 $T_i$ 添加到 undo-list 中。</li><li>当扫描到一条 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt; 记录时，将 $T_i$ 从 undo-list 中移除。</li></ol><p><strong>撤销阶段（Undo Phase）</strong>. 上一阶段结束后，undo-list 包含了所有处于未完成状态的事务标识符。系统从日志文件结束位置开始<font color=red>向后扫描</font>，撤销这些未完成事务的所有操作，扫描中，包含如下步骤：</p><ol><li>当扫描到属于 undo-list 中事务的日志时，执行 undo 操作，和正常情况下事务失败回滚的操作一样。</li><li>当扫描到属于 undo-list 中事务的 &lt;$T_i\ $start&gt; 日志后，系统向日志文件追加一条 &lt;$T_i\ $abort&gt;，并将该事务从 undo-list 中移除。</li><li>当 undo-list 为空时，扫描停止，该阶段结束。</li></ol><p>在 redo 阶段可能存在重复的操作，比如正常情况下的失败事务的日志记录和回滚时的记录，但正是这少许的重复，使得整个流程简单了不少，只需要两遍扫描就能完成恢复。上述两个过程如图 4 所示：<br><img src="/img/log-based_recovery_system/4.jpg" alt="图 4"></p><br><p><font color=dark-green>一些细节</font><br>checkpoint 阶段应该也是原子操作，也就是说，在执行 checkpoint 时，不允许事务向 buffer 中写更新，这个要求可以通过 fuzzy checkpoint 技术放松，这里不谈。</p><p>而且，当 buffer block 向磁盘 <em>output</em> 时，也不允许事务向 buffer block 执行写操作，不然可能会违法 WAL 规则。</p><p>保证以上两个要求，可以通过特殊的锁：</p><ul><li>当事务执行 <em>write</em> 时，应当先获取数据所在 buffer block 的排它锁，当更新执行完后，立即释放锁。</li><li>当 <em>output</em> data buffer block 时，应该先获取 该 block 的排它锁，然后 <em>output</em> 与该 block 数据相关的所有 log records 到 stable storage 中，再然后 <em>output</em> 该 block 的数据到数据库磁盘文件中，最后释放该锁。</li></ul><p>可以看到，这里的锁持有的时间很短，一般将这种锁称为 latch.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然写了这么多，但这些却是最简单的知识，还有很多高级的话题没写，等着以后学习深入了再来补充吧。</p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>事务篇一：事务总论</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文内容来自 database system concepts 第七版第十七章，主要是大概介绍事务的基本概念。更进一步的内容会引用其他文章，所以该文相当于一篇索引。</p><p><strong>单机事务部分（local transactions）：</strong></p><a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a> <br /> <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a> <br /><a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="事务篇四：Log-Based Recovery System">事务篇四：Log-Based Recovery System</a> <br /><br /><p><strong>分布式事务部分（global transactions）：</strong></p><a href="/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/" title="事务篇五：两阶段提交协议">事务篇五：两阶段提交协议</a> <br /><a href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/" title="事务篇六：Percolator 随笔">事务篇六：Percolator 随笔</a> <br /><br /><p><font color=red>分布式事务部分和单机事务部分的不同主要在于如何在多个节点保证事务的原子性。</font></p><h2 id="Transaction-Basic-Concept"><a href="#Transaction-Basic-Concept" class="headerlink" title="Transaction Basic Concept"></a>Transaction Basic Concept</h2><p>事务是一个逻辑单元，它包含了一组操作，这些操作可能访问或修改不同的数据。对用户来说，这组操作（事务）是一个单一的、不可分割的部分（比如，用户发起转账请求，这个请求对用户来说单一的操作，而在实际执行时分为多个步骤：安全性检查、出账、入账等），也就是说事务要么完成（其包含的所有步骤全部执行完成）或者失败（所有步骤皆失败），这个 all-or-none 特性被称为<font color=red>原子性（atomicity）</font>。</p><p>因为事务是一个不可分割的单元，所以它内含的步骤不能被数据库的其他操作分隔开。即使多个事务并发（concurrence），数据库系统也需要保证如此，即对于每两个事务 TA，TB，TA 要么开始于 TB 结束之后，要么结束于 TB 开始之前。这个特性被称为<font color=red>隔离性（isolation）</font>。</p><p>当事务执行完成提交后，它对数据库的修改不会丢失，即使系统从崩溃中恢复。这个特性被称为<font color=red>持久性（durability）</font>。</p><p>另外，事务还需要保证数据库的<font color=red>一致性（consistency）</font>，即，事务开始之前，数据库满足一致性，事务结束之后，数据库仍满足一致性。一致性的要求无法完成由数据库系统本身保证，它可能与上层逻辑有关（如出账入账，总账平衡）。</p><p>以上四个特性被简称位 ACID。</p><h2 id="Storage-Structure"><a href="#Storage-Structure" class="headerlink" title="Storage Structure"></a>Storage Structure</h2><p>这里需要清楚一些存储的概念，比如易失性存储（volatile storage）、非易失性存储（non-volatile storage）、稳定性存储（stable storage）。</p><p><strong>主要是 stable storage 的概念</strong>：只要数据写入 stable storage，那么它们就“永远”不可能丢失，要实现这样的存储，需要将数据备份（replicate）到多个非易失性存储介质上，这些非易失性存储分别独立，有自己的错误处理模块。<a href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/" title="关于多从节点如何安全的备份，参考分布式一致性协议 raft">关于多从节点如何安全的备份，参考分布式一致性协议 raft</a>。</p><h2 id="Transaction-Atomicity-and-Durability"><a href="#Transaction-Atomicity-and-Durability" class="headerlink" title="Transaction Atomicity and Durability"></a>Transaction Atomicity and Durability</h2><p><strong>需要明白，原子性的挑战在哪里？</strong></p><p>事务并不总是成功执行。结束的事务（terminated txn）有两种可能：</p><ul><li>committed，事务成功完成了所有操作，所有更新都已经写入了数据库；</li><li>aborted，事务因各种原因失败，事务造成的修改都已经恢复（rolled back），数据库回到了事务开始之前的状态。</li></ul><p>对失败的事务造成的修改如何恢复，是保证原子性的一个难点。通常采用日志的方式实现（事务对数据库的每一个修改都先写入日志中），维护日志不仅可以重新事务的修改操作（保证原子性和持久性），还能在事务失败后撤销修改，以保证原子性。数据库的 recovery system 负责保证原子性和持久性，<a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="参见文章：数据库恢复系统">参见文章：数据库恢复系统</a>。</p><p>对于，原子性和持久化还有一个部分需要注意：外部可见更新（observable external writes），如更新显示到屏幕、发送邮件或者网上购物等场景。如果事务半途中断，这些更新难以撤回。一般的解决方法是，先将更新存储到数据库的某个地方，等到事务提交后，在将这些更新应用到外部，另外，如果系统在事务提交后，应用更新到外部之前崩溃，那么等到系统重启后依然可以应用更新到外部。</p><h2 id="Transaction-Isolation"><a href="#Transaction-Isolation" class="headerlink" title="Transaction Isolation"></a>Transaction Isolation</h2><p>SQL 标准将隔离级别（isolation level）分为四类：</p><p><strong>可串行化-serializable</strong>，即事务之间的执行顺序可串行化，其结果等价于串行执行，能够保证数据库的一致性。该隔离级别允许一定程度的并发，属于最高的隔离级别。一般，数据库为了提升性能，实现时不会完全遵循其标准。关于串行化的定义，可以参见文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务的串行化定义">事务的串行化定义</a>。</p><p><strong>可重复读-repeatable read</strong>，该级别规定了两点：1）只读已经提交了的数据；2）在事务执行期间多次读取一个数据之间，不允许其他任何事务更新该数据。该隔离级别保证了在<u>一个事务中，多次读取同一个数据，总会得到同样的值</u>。但注意，这里只是规定了不允许更新已存在的数据，对于其他事务插入新的数据却未做规定，这就导致了，遵循该级别的事务在两次读取中，有可能第二次读取的数据中有一些不存在于第一次读取中的新数据（共同拥有的数据还是相同的），即幻读，若使用快照隔离提供稳定的视图，则不会出现幻读。</p><p><strong>读已提交-read committed</strong>，该级别比上一级别更弱，由上一级别的叙述可知，遵循该级别的事务在两次读取同一数据，这两次的数据可能不同，因为其他事务在这段时间内更新了该数据，即不可重复读。</p><p><strong>读未提交-read uncommitted</strong>，这是最低的级别了，该级别甚至允许一个事务读取另外一个事务的中间结果，即脏读。</p><p>解释下读已提交&#x2F;未提交中的提交的含义：提交是指事务的提交。假设有两个事务： TA 读取数据 S，TB 修改数据 S。读未提交允许 TA 读取被 TB 修改了的数据 S，尽管 TB 还未提交。这里存在的可能隐患是，若 TB 失败终止了，所有修改都会回滚，也就是说，TA 读取到了无效的值。</p><p>区分下幻读和不可重复读现象的区别：幻读是指，本次读出的数据中，有一部分在之前读取的结果中不存在，幻读存在于范围读取中；不可重复读是指，本次读出的数据和之前的值不相等。</p><p>总结下，各种隔离级别可能发生的<font color=red>现象</font>：</p><table><thead><tr><th align="center"></th><th align="center">脏读</th><th align="center">幻读</th><th align="center">不可重复读</th><th align="center">脏写</th></tr></thead><tbody><tr><td align="center">可串行化</td><td align="center">禁止</td><td align="center">禁止</td><td align="center">禁止</td><td align="center">禁止</td></tr><tr><td align="center">可重复读</td><td align="center">禁止</td><td align="center">？</td><td align="center">禁止</td><td align="center">禁止</td></tr><tr><td align="center">读已提交</td><td align="center">禁止</td><td align="center">允许</td><td align="center">允许</td><td align="center">禁止</td></tr><tr><td align="center">读未提交</td><td align="center">允许</td><td align="center">允许</td><td align="center">允许</td><td align="center">禁止</td></tr></tbody></table><h2 id="Implementation-of-Isolation-Level"><a href="#Implementation-of-Isolation-Level" class="headerlink" title="Implementation of Isolation Level"></a>Implementation of Isolation Level</h2><p>该部分内容广且复杂，在文章 <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务的并发控制中有详细介绍。">事务的并发控制中有详细介绍。</a></p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇六：Percolator 随笔</title>
    <link href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/"/>
    <url>/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf">Precolator 论文</a><br>建议先阅读事务并发 snapshot isolation 部分: <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a></p><p>precolator 这个系统是怎么产生的呢？论文中介绍了下背景，谷歌的爬虫应用会周期性的爬取新网页，同时更新网页的索引、排名等信息，这些更新操作往往并发，在庞大的数据之上做一些小的、独立的更改，而且修改跨越多个行、多个表。谷歌现有的系统中，MapReduce 也能完成相同的工作，但由于设计理念，MapReduce 需要扫描整个数据库才能完成这些更新；而且 BigTable 也只支持单行事务，不能提供上面所说的需求，所以就在 BigTable 之上构建了 precolator.</p><p>在极其庞大的数据集之上（谷歌是数十PB），每次高效地完成少量的更新被称为<strong>增量处理（incremental processing）</strong>，precolator 便是完成这样的工作。precolator 每次采用事务的方式执行这样的更新，在本地使用快照隔离并发技术，分布式提交时使用 2PC.</p><p><font color=red>precolator 包含很多的内容，本文主要集中在事务系统的设计，主要是快照隔离的实现</font>。</p><h2 id="事务系统的设计"><a href="#事务系统的设计" class="headerlink" title="事务系统的设计"></a>事务系统的设计</h2><p>snapshot isolation 有两个阶段：</p><ol><li>本地阶段，读取在 [0, startTS] 时间内提交的最新的 snapshot，即开始时间戳之前提交的，并在事务的本地空间对数据进行操作。</li><li>提交阶段，该阶段需要先验证是否有冲突产生，snapshot isolation 要求不能有 write-write conflict.</li></ol><p>要实现这两个阶段，不仅需要依据时间戳读取数据，还需要辨别出并发事务，precolator 是怎么做的呢？它通过记录一些列信息做到：</p><ul><li>data column. 实际的数据将记录在该列，格式一般为 $startTS(T_i) + data$.</li><li>lock column. 当某个数据处于提交阶段时，该更新还不可见，格式一般为 $startTS(T_i) + point\_to\_data$.</li><li>write column. 当写入一条 write 数据时，表示某个数据已提交，其他事务可见，格式一般为 $commitTS(T_i) + point\_to\_data$.</li></ul><p>下面分别针对读和写两个操作介绍它们是如何工作的：<br><strong>当事务 $T_i$ 读取数据时</strong>，它先检查是否存在 [0, startTS($T_i$)] 区间内的同一数据的写锁，若存在，说明存在某个与 $T_i$ 并发的事务正在修改该数据，这时，$T_i$ 等待该事务完成。当锁不存在后，$T_i$ 就在 write column 中寻找 [0, startTS($T_i$)] 区间内同一数据最新的 write record，最后依据该记录读取到真正的数据。</p><p><font color=red>这里有一些想不明白，为何 $T_i$ 需要等？</font>该事务只能看到其开始之前提交的数据，很明显，上面占据写锁的事务所做的更新对 $T_i$ 是不可见的，既然如此，为何 $T_i$ 还必须等待呢？</p><br><p><strong>写操作相对复杂一些，因为它涉及提交过程</strong>。precolator 采用两阶段提交事务，这里略去了 2PC 的细节，只事务在本地节点与 snapshot isolation 有关的操作：<br><u>第一阶段称为 Prewrite.</u> 在协调节点提交事务 $T_i$ 前，先获得所有被修改数据的写锁。这其实相当于验证阶段，查找冲突是这一阶段的重要过程。冲突来自于与之并发的其他更新事务（假设为 $T_j$），存在两种冲突，如图片所示:<br><img src="/img/precolator/conflict.png" alt="冲突示意"></p><ol><li>并发的事务已完成提交。即提交时间（write record 的时间戳）在当前事务开始时间戳之后。</li><li>并发的事务正在提交。即并发事务已经对数据加锁了，锁的时间戳为事务的开始时间戳。</li></ol><p>对需要修改的每一个数据，当事务 $T_i$ 遇到这两种冲突时会 abort，即满足了 snapshot isolation 对避免 write-write conflict 的保证。如果没有冲突发生，事务 $T_i$ 会结合事务开始时间戳 startTS($T_i$) 分别将修改的数据（data）和锁记录（lock）写入相应的列（这是一个原子操作，通过 BigTable 的单行事务完成）。</p><p><u>如果没有冲突，将来到第二个阶段</u>。假设参与 2PC 的所有节点都同意提交。在该阶段，事务首先获取一个提交时间戳 commitTS($T_i$)，然后对修改的每一个数据，移除其 lock 记录，同时写入 write 记录（同样是一个原子操作），这时，该更新就对外部可见了。</p><p>以一个例子阐述上述过程，Bob 和 Joe 账户初始分别有 10 美元和 2 美元，一个事务从 Bob 账户转 7 美元到 Joe 账户。下面几幅图展示了修改过程（加粗表示新写入的记录，x:data 表示 data 是在时间戳为 x 时写入的，primary 后面会提到）：</p><p><img src="/img/precolator/example_1.jpg" alt="example_a"><br><img src="/img/precolator/example_2.jpg" alt="example_b"><br><img src="/img/precolator/example_3.jpg" alt="example_c"><br><img src="/img/precolator/example_4.jpg" alt="example_d"><br><img src="/img/precolator/example_5.jpg" alt="example_e"></p><h2 id="Precolator-对可用性的考虑"><a href="#Precolator-对可用性的考虑" class="headerlink" title="Precolator 对可用性的考虑"></a>Precolator 对可用性的考虑</h2><p>上面提到事务准备提交前会对数据加锁，这些锁分布在不同的行、不同的表、甚至不同的节点。假如事务在提交过程中系统发生故障，只释放了部分的锁，那么就可能阻塞后面新的事务，如果加锁的是热点数据，那系统的可用性就会大大降低，甚至不可用。另外，如果由系统来贸然释放这些遗留的锁，则可能造成数据的不一致，因为事务可能只提交了部分更新。因此锁必须是可恢复的（保存在 stable storage），而且还必须判断这些锁能不能安全的释放。Precolator 提出了如下的解决方法。</p><p>锁是通过 lock 记录写入 BigTable 中的，能提供 stable storage 的功能。同时，在获取写锁时，事务会指定一行（某个数据）的锁为主锁（primary，如 example_b），其他锁都会包含一个指针指向主锁。事务提交释放锁时，先从主锁开始释放所有锁。</p><p>若一新事务准备提交时发现数据已经被加锁了，<strong>可以猜想该锁是系统崩溃遗留下来的</strong>，此时它有两种选择符：</p><ul><li><u>帮助事务提交（roll forward）</u>。若持有该锁的事务在系统崩溃前已经提交了部分的更新，那么为维护数据的一致性，剩下的更新也应当被提交（其实数据已经写入 data 列了，只不过锁还在，新事务要做的不过是移除该锁，写入 write 记录，使该更新可见）。</li><li><u>回滚该事务（roll back）</u>。若持有该锁的事务在崩溃前还未提交，那么需要撤销事务所做的更新（写入 data 列的数据）。</li></ul><p>听起来是那么回事，但有两个问题还需要解决：</p><ol><li>如何判断事务是否已提交了部分更新？</li><li>如何判断该锁真的是系统崩溃后遗留的？</li></ol><p><strong>对于第一个问题</strong>（precolator 论文中没有提到 log 文件），primary lock 发挥了重要作用。因为释放锁总是先释放 primary lock，若 primary lock 已经不存在了，说明至少部分更新已经对外可见了，因此该事务必须被完全提交，所以需要 roll forward. 若 primary lock 依旧存在，说明该事务还未提交，roll back 也不会产生什么副作用，<font color=red>所以该 primary lock 是一个同步点（synchronize point）</font>。因为这一点，当事务释放锁时，<strong>应当检查其是否还持有主锁</strong>，看看自己是否已经被 roll back 了。</p><p>如果锁不是崩溃后留下来的，贸然 roll back 则会影响系统的性能。precolator 中，是由一系列 worker 执行事务，<strong>对于第二个问题</strong>，所以当新事务遇到锁时，发现该锁属于不活跃的 worker（dead or stuck）时，才会做如上猜想，否则 abort 或者等待。如何判断 worker 不活跃呢？论文中原文如下（偷个懒，不想写了）：</p><blockquote><p>…So, a transaction will not clean up a lock unless it suspects that a lock belongs to a dead or stuck worker. Percolator uses simple mechanisms to determine the liveness of another transaction. Running workers write a token into the Chubby lockservice [8] to indicate they belong to the system; other workers can use the existence of this token as a sign that the worker is alive (the token is automatically deleted when the process exits). To handle a worker that is live, but not working, we additionally write the wall time into the lock; a lock that contains a too-old wall time will be cleaned up even if the worker’s liveness token is valid. To handle long- running commit operations, workers periodically update this wall time while committing.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Precolator 论文中还包含很多其他内容，比如通知服务、时间戳、相关工作等等。本文主要讨论其事务的实现。</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式事务</tag>
      
      <tag>事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>与数据库的初见</title>
    <link href="/2022/06/16/%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%88%9D%E8%A7%81/"/>
    <url>/2022/06/16/%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%88%9D%E8%A7%81/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该篇文章是一篇索引目录，索引课程 CMU-15445 的笔记：</p><p><strong>基础内容：</strong></p><a href="/2022/07/01/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%88%86%E7%B1%BB/" title="数据库系统分类">数据库系统分类</a> <br /><a href="/2022/06/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%A3%81%E7%9B%98%E5%AD%98%E5%82%A8/" title="关系型数据库磁盘存储">关系型数据库磁盘存储</a> <br /><br /><p><strong>索引部分：</strong></p><a href="/2022/07/04/%E7%B4%A2%E5%BC%95%E7%AF%87%E9%9B%B6%EF%BC%9A%E7%B4%A2%E5%BC%95%E6%80%BB%E8%AE%BA/" title="索引篇零：索引总论">索引篇零：索引总论</a>   <br /><a href="#">Post not found: 索引篇一：可拓展哈希</a>   <br /><a href="/2022/06/29/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%BA%8C%EF%BC%9AB-plus%20Tree/" title="索引篇二：B-plus Tree">索引篇二：B-plus Tree</a> <br /><a href="/2022/06/30/%E7%B4%A2%E5%BC%95%E7%AF%87%E4%B8%89%EF%BC%9ALSM%20Tree/" title="索引篇三：LSM Tree">索引篇三：LSM Tree</a>    <br /><br /><p><strong>查询处理部分：</strong></p><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E4%BB%A3%E4%BB%B7%E8%A1%A1%E9%87%8F/" title="查询处理篇：代价衡量">查询处理篇：代价衡量</a>    <br /><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E6%8E%92%E5%BA%8F/" title="查询处理篇：排序">查询处理篇：排序</a>    <br /><a href="/2022/07/01/%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E7%AF%87%EF%BC%9A%E4%BA%A4%E9%9B%86/" title="查询处理篇：交集">查询处理篇：交集</a>    <br /><br /><p><strong>查询优化部分：</strong></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>课程的主要结构如图所示：<br><img src="/img/CMU15445/1.png"></p><p>各级存储设备的访问时间如下：<br><img src="/img/CMU15445/2.png"></p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CMU-15445</tag>
      
      <tag>关系型数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TinyKV Snapshot 流程探秘</title>
    <link href="/2022/06/16/TinyKV-Snapshot-%E6%B5%81%E7%A8%8B/"/>
    <url>/2022/06/16/TinyKV-Snapshot-%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写 TinyKV 时，有一个部分很难理解，那就 Snapshot 的收发过程。<br>为了防止内存中的日志条目无限扩张，Raft 会定时&#x2F;定量清理日志（如已经提交了的日志），一些节点可能由于是新加入或者网络等原因，其想要复制的日志已经被 leader 清理出内存了，此时，leader 会给该节点发送一份 Snapshot 使其快速跟上。在实现代码时，Raft 只是使用 Snapshot 的元数据来更新了一些状态，并没有涉及的日志的追加等操作，深感疑惑。而且，Snapshot 一般很大，虽然可以作为普通消息处理，但可能会阻塞正常的流程，所以对它的收发过程也很感兴趣。为了搞清楚这些问题，追踪代码调用，总算是搞清楚了。下面分为 Snapshot 的发送、接收、处理几个方面解密。</p><h2 id="Snapshot-流程总览"><a href="#Snapshot-流程总览" class="headerlink" title="Snapshot 流程总览"></a>Snapshot 流程总览</h2><p>这里先给出 snapshot 各个部分的流程示意图，下面会对各个部分详细分析<br><img src="/img/tinykv_snapshot/tinykv_arch.png" alt="TinyKV 的整体架构（代码层面）"><br><img src="/img/tinykv_snapshot/fig_for_create.png" alt="snapshot 创建流程"><br><img src="/img/tinykv_snapshot/fig_for_send.png" alt="snapshot 发送流程"><br><img src="/img/tinykv_snapshot/fig_for_recv.png" alt="snapshot 接收流程"><br><img src="/img/tinykv_snapshot/fig_for_apply.png" alt="snapshot 应用流程"></p><h2 id="Part-1：Snapshot-的创建"><a href="#Part-1：Snapshot-的创建" class="headerlink" title="Part 1：Snapshot 的创建"></a>Part 1：Snapshot 的创建</h2><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Raft)</span></span> sendAppend(to <span class="hljs-type">uint64</span>) &#123;<br>    term, err := r.RaftLog.Term(r.Prs[to].Next - <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// the peer left too far behind  (or newly join), </span><br>        <span class="hljs-comment">// send it a snapshot to catch-up</span><br>        r.trySendSnapshot(to)<br>        <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-comment">// something else</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Raft)</span></span> trySendSnapshot(to <span class="hljs-type">uint64</span>) &#123;<br>    snapshot, err := r.RaftLog.storage.snapshot()<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span><br>    &#125;<br><br>    r.msgs = <span class="hljs-built_in">append</span>(r.msgs, SnapshotMessage&#123;...&#125;)<br><br>&#125;<br></code></pre></td></tr></table></figure><p>第一个函数表明了 Raft 发送 Snapshot 的时机，第二个函数表明了 Snapshot 来自 storage。</p><blockquote><p>这个 storage 在 2A部分和之后的部分是不一样的，值得分析下。在 2A 中，storage 接口由 MemoryStorage 实现，这货存在于内存中，文档中写着，放入 storage 的日志是持久化的（stabled），当时很不理解，因为它也是存在内存中的啊，做到后面才发现，这里的 MemoryStorage 主要起着测试的作用，你只需要闭只眼假装它真的持久化了就行。而在后面的部分，storage 接口由 badger.DB（engines.Raft）实现，是实打实的写入磁盘。</p></blockquote><p>那么，调用 storage.snapshot() 实际做了什么？</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> Snapshot() (eraftpb.Snapshot, <span class="hljs-type">error</span>) &#123;<br>    <span class="hljs-keyword">var</span> snapshot eraftpb.Snapshot<br>    <span class="hljs-keyword">if</span> snapshot_is_generating &#123;<br>        snapshot &lt;- ps.snapState.Receiver<br>        <span class="hljs-keyword">return</span> snapshot, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// something else</span><br><br>    ch := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> *eraftpb.Snapshot, <span class="hljs-number">1</span>)<br>ps.snapState = snap.SnapState&#123;<br>StateType: snap.SnapState_Generating,<br>Receiver:  ch,<br>&#125;<br><span class="hljs-comment">// schedule snapshot generate task</span><br>ps.regionSched &lt;- &amp;runner.RegionTaskGen&#123;<br>RegionId: ps.region.GetId(),<br>Notifier: ch,<br>&#125;<br><br>    <span class="hljs-keyword">return</span> snapshot, raft.ErrSnapshotTemporarilyUnavailable<br>&#125;<br><br></code></pre></td></tr></table></figure><p>代码很清楚，如果当前正在生成 snapshot，那么就等待它生成完成并返回，否则，就创建一个 RegionTaskGen 任务发送给 regionSched 通道，并返回暂时不可用错误。那么是谁在接收该任务呢？</p><p>上面说到 Raft 调用生成 snapshot 的接口，该接口的实现（PeerStorage）会创建一个 RegionTaskGen 任务发送给 regionSched 通道。该通道的消费者实际上是 regionWorker：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(w *Worker)</span></span> Start(handler TaskHandler) &#123;<br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-keyword">for</span> &#123;<br>Task := &lt;-w.receiver<br><span class="hljs-keyword">if</span> _, ok := Task.(TaskStop); ok &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br>handler.Handle(Task)<br>&#125;<br>&#125;()<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *regionTaskHandler)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *RegionTaskGen:<br>task := t.(*RegionTaskGen)<br><span class="hljs-comment">// It is safe for now to handle generating and applying snapshot concurrently,</span><br><span class="hljs-comment">// but it may not when merge is implemented.</span><br>r.ctx.handleGen(task.RegionId, task.Notifier)<br><span class="hljs-keyword">case</span> *RegionTaskApply:<br>task := t.(*RegionTaskApply)<br>        <span class="hljs-comment">// apply received snapshot</span><br>r.ctx.handleApply(task.RegionId, task.Notifier, task.StartKey, task.EndKey, task.SnapMeta)<br><span class="hljs-keyword">case</span> *RegionTaskDestroy:<br>task := t.(*RegionTaskDestroy)<br>r.ctx.cleanUpRange(task.RegionId, task.StartKey, task.EndKey)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>regionWorker 创建一个协程，处理接收到的各种任务。其中有两种任务是本文需要关注的：<br>1）RegionTaskGen（生成 snapshot）<br>2）RegionTaskApply（应用从其他 peer 接收来的 snapshot）</p><p>继续看 RegionTaskGen 任务是如何执行的：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> handleGen(...) &#123;<br>snap, err := doSnapshot(...)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>notifier &lt;- <span class="hljs-literal">nil</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">// notify task done to task creator</span><br>notifier &lt;- snap<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">doSnapshot</span><span class="hljs-params">(...)</span></span> (*eraftpb.Snapshot, <span class="hljs-type">error</span>) &#123;<br>log.Debugf(<span class="hljs-string">&quot;begin to generate a snapshot. [regionId: %d]&quot;</span>, regionId)<br>    <span class="hljs-comment">// kvDB !!!</span><br>txn := engines.Kv.NewTransaction(<span class="hljs-literal">false</span>)<br>    <span class="hljs-comment">//...</span><br>err = s.Build(txn, ...)<br>    <span class="hljs-comment">//...</span><br><span class="hljs-keyword">return</span> snapshot, err<br>&#125;<br></code></pre></td></tr></table></figure><p>该 Build() 函数最终会调用 snapBuilder.build()，函数会扫描 PeerStorage.kvDB 的数据，创建一份快照：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(b *snapBuilder)</span></span> build() <span class="hljs-type">error</span> &#123;<br><span class="hljs-keyword">defer</span> b.txn.Discard()<br>startKey, endKey := b.region.StartKey, b.region.EndKey<br><br>    <span class="hljs-comment">// all data will store in b.cfFiles</span><br><span class="hljs-keyword">for</span> _, file := <span class="hljs-keyword">range</span> b.cfFiles &#123;<br>cf := file.CF<br>sstWriter := file.SstWriter<br><br>it := engine_util.NewCFIterator(cf, b.txn)<br><span class="hljs-keyword">for</span> it.Seek(startKey); it.Valid(); it.Next() &#123;<br>item := it.Item()<br>key := item.Key()<br><br>value, err := item.Value()<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br><br>cfKey := engine_util.KeyWithCF(cf, key)<br>            <span class="hljs-comment">// store data</span><br><span class="hljs-keyword">if</span> err := sstWriter.Add(cfKey, y.ValueStruct&#123;<br>Value: value,<br>&#125;); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br>it.Close()<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>自此，Snapshot 的创建过程分析完成</p><h2 id="Part-2：Snapshot-的发送"><a href="#Part-2：Snapshot-的发送" class="headerlink" title="Part 2：Snapshot 的发送"></a>Part 2：Snapshot 的发送</h2><p>当 sendAppend() 函数获取到创建的 snapshot 后，会将其封装在 pb.MessageType_MsgSnapshot 消息中，等待 RaftStorage 层调用 rawNode.Ready()：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(d *peerMsgHandler)</span></span> HandleRaftReady() &#123;<br><span class="hljs-comment">// Your Code Here (2B).</span><br>rd := d.RaftGroup.Ready()<br>d.sendMessageToPeers(rd.Messages)<br>    <span class="hljs-comment">// ...</span><br>d.RaftGroup.Advance(rd)<br>&#125;<br></code></pre></td></tr></table></figure><p>sendMessageToPeers() 最终会调用 WirteData() 函数：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *ServerTransport)</span></span> WriteData(...) &#123;<br><span class="hljs-keyword">if</span> msg.GetMessage().GetSnapshot() != <span class="hljs-literal">nil</span> &#123;<br>t.SendSnapshotSock(addr, msg)<br><span class="hljs-keyword">return</span><br>&#125;<br><span class="hljs-keyword">if</span> err := t.raftClient.Send(storeID, addr, msg); err != <span class="hljs-literal">nil</span> &#123;<br>log.Errorf(<span class="hljs-string">&quot;send raft msg err. err: %v&quot;</span>, err)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，在 WriteData 中对 snapshot 消息做了一个拦截，采用另外的方式单独处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *ServerTransport)</span></span> SendSnapshotSock(...) &#123;<br>t.snapScheduler &lt;- &amp;sendSnapTask&#123;<br>addr:     addr,<br>msg:      msg,<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这下明白了，由于 snapshot 比较大，会采用分块传输，对它的发送操作与普通的消息分开，由 sendSnapTask 异步完成。<br>继续探寻该任务是如何被执行的，该任务被 snapWorker 接收，并调用 Handle() 处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *sendSnapTask:<br>r.send(t.(*sendSnapTask))<br><span class="hljs-keyword">case</span> *recvSnapTask:<br>r.recv(t.(*recvSnapTask))<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> send(t *sendSnapTask) &#123;<br>t.callback(r.sendSnap(t.addr, t.msg))<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> sendSnap(...) <span class="hljs-type">error</span> &#123;<br>    <span class="hljs-comment">// ...</span><br>buf := <span class="hljs-built_in">make</span>([]<span class="hljs-type">byte</span>, snapChunkLen) <span class="hljs-comment">// snapChunkLen = 1024 * 1024</span><br><span class="hljs-keyword">for</span> remain := snap.TotalSize(); remain &gt; <span class="hljs-number">0</span>; remain -= <span class="hljs-type">uint64</span>(<span class="hljs-built_in">len</span>(buf)) &#123;<br>_, err := io.ReadFull(snap, buf)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> errors.Errorf(<span class="hljs-string">&quot;failed to read snapshot chunk: %v&quot;</span>, err)<br>&#125;<br>err = stream.Send(&amp;raft_serverpb.SnapshotChunk&#123;Data: buf&#125;)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到最终 snapshot 以 snapChunkLen 为单位分块发送出去的，后面的事情就是 gRPC 的工作了，探秘自此结束。 </p><h2 id="Part-3：Snapshot-的接收"><a href="#Part-3：Snapshot-的接收" class="headerlink" title="Part 3：Snapshot 的接收"></a>Part 3：Snapshot 的接收</h2><p>当使用 gRPC 发送 snapshot 时，对应 peer 也就进入了接收流程。上面提到的 snapWorker 也会处理接收操作，这里不再赘述。当所有的 snapshot 分块都接受完成后，就会给 raftWorker 监听的管道发送消息，最后调用 rawNode.Step() 让 raft 调用 handleSnapshot() 处理。 </p><h2 id="Part-4：应用来自其他-Peer-的-Snapshot"><a href="#Part-4：应用来自其他-Peer-的-Snapshot" class="headerlink" title="Part 4：应用来自其他 Peer 的 Snapshot"></a>Part 4：应用来自其他 Peer 的 Snapshot</h2><p>handleSnapshot() 接收到 snapshot 后只是更新了一些元数据，并将 snapshot 赋值给 pendingSnapshot，等待上层调用 Ready() 获取 pendingSnapshot：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(d *peerMsgHandler)</span></span> HandleRaftReady() &#123;<br><span class="hljs-comment">// Your Code Here (2B).</span><br>rd := d.RaftGroup.Ready()<br>applySnapResult, _ := d.peerStorage.SaveReadyState(&amp;rd)<br><span class="hljs-comment">//...</span><br>d.RaftGroup.Advance(rd)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> SaveReadyState(ready *raft.Ready) &#123;<br><span class="hljs-comment">// Your Code Here (2B/2C).</span><br>applySnapResult, err := ps.ApplySnapshot(&amp;ready.Snapshot, ...)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> ApplySnapshot(snapshot *eraftpb.Snapshot, ...) &#123;<br>    <span class="hljs-comment">// ...</span><br><span class="hljs-comment">// send runner.RegionTaskApply task to region worker through </span><br>    <span class="hljs-comment">// PeerStorage.regionSched and</span><br><span class="hljs-comment">// wait until region worker finishes</span><br>ch := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-type">bool</span>, <span class="hljs-number">1</span>)<br>ps.regionSched &lt;- &amp;runner.RegionTaskApply&#123;<br>RegionId: snapData.Region.GetId(),<br>Notifier: ch,<br>SnapMeta: snapshot.Metadata,<br>StartKey: snapData.Region.StartKey,<br>EndKey:   snapData.Region.EndKey,<br>&#125;<br><br><span class="hljs-comment">// waiting</span><br>&lt;-ch<br>    <span class="hljs-keyword">return</span> ...<br>&#125;<br></code></pre></td></tr></table></figure><p>上面代码很明显了，上层获取到 Ready.Snapshot 后，会创建 RegionTaskApply 任务通过 regionSched 通道发送给 RegionWorker -&gt; handle() -&gt; handleApply() 处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *regionTaskHandler)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *RegionTaskGen:<br>task := t.(*RegionTaskGen)<br><span class="hljs-comment">// It is safe for now to handle generating and applying snapshot concurrently,</span><br><span class="hljs-comment">// but it may not when merge is implemented.</span><br>r.ctx.handleGen(task.RegionId, task.Notifier)<br><span class="hljs-keyword">case</span> *RegionTaskApply:<br>task := t.(*RegionTaskApply)<br>r.ctx.handleApply(task.RegionId, task.Notifier, task.StartKey, task.EndKey, task.SnapMeta)<br><span class="hljs-keyword">case</span> *RegionTaskDestroy:<br>task := t.(*RegionTaskDestroy)<br>r.ctx.cleanUpRange(task.RegionId, task.StartKey, task.EndKey)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> handleApply(...) &#123;<br>err := snapCtx.applySnap(regionId, startKey, endKey, snapMeta)<br><span class="hljs-comment">// ...</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> applySnap(...) &#123;<br>applyOptions := snap.NewApplyOptions(snapCtx.engines.Kv, &amp;metapb.Region&#123;<br>Id:       regionId,<br>StartKey: startKey,<br>EndKey:   endKey,<br>&#125;)<br><span class="hljs-keyword">if</span> err := snapshot.Apply(*applyOptions); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，上面一步步调用，最后调用 snapshot.Apply()，注意这里传入的是 badger.kvDB。<br>snapshot.Apply() 和上面提到的 snapshot 创建过程的 snapBuilder.build() 执行的是相反的步骤，即，将 snapshot 中的内容写入到磁盘:</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *Snap)</span></span> Apply(opts ApplyOptions) <span class="hljs-type">error</span> &#123;<br>externalFiles := <span class="hljs-built_in">make</span>([]*os.File, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(s.CFFiles))<br><span class="hljs-keyword">for</span> _, cfFile := <span class="hljs-keyword">range</span> s.CFFiles &#123;<br>file, err := os.Open(cfFile.Path)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>log.Errorf(<span class="hljs-string">&quot;open ingest file %s failed: %s&quot;</span>, cfFile.Path, err)<br><span class="hljs-keyword">return</span> err<br>&#125;<br>externalFiles = <span class="hljs-built_in">append</span>(externalFiles, file)<br>&#125;<br>    <span class="hljs-comment">// write to DB</span><br>n, err := opts.DB.IngestExternalFiles(externalFiles)<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p>snapshot 的应用分析自此结束。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的分析，可以得到以下信息：</p><ol><li>snapshot 的创建、发送、接收和处理都与 Raft 无关，它无需关系具体数据（除了元数据）。</li><li>snapshot 的发送和接收都采取了单独的 RPC 异步处理。</li><li>生成 snapshot 需要从 kvDB 中读取数据，然后返回给 Raft，最后通过 Ready 交给上层发送。</li><li>snapshot 接收后，需要先交给 Raft 更新一些元数据，然后通过 Ready 交给上层写到 kvDB 中。</li></ol>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TinyKV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 算法问答录</title>
    <link href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/"/>
    <url>/2022/06/14/raft-%E6%9D%82%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前段时间学习了 CMU-15445 的课程，也写完了 project，了解了数据库内核的基本知识。这段时间在做 TinyKV，刚好看了 raft，细节很多，所以来总结下。</p><p>关于 raft 网上有很多资料：<br><a href="https://raft.github.io/raft.pdf">raft 小论文</a><br><a href="http://files.catwell.info/misc/mirror/2014-ongaro-raft-phd.pdf">raft 博士论文</a><br><a href="https://tanxinyu.work/raft/">raft 博客</a><br><a href="https://www.codedump.info/post/20180922-etcd-raft/">etcd raft 实现</a></p><p>所以，这里并不是对 raft 算法本身的细节记录（可能存在部分），而是自己阅读、实现时的一些疑问和解答。</p><h2 id="Q0-raft-算法有哪些优化？"><a href="#Q0-raft-算法有哪些优化？" class="headerlink" title="Q0: raft 算法有哪些优化？"></a>Q0: raft 算法有哪些优化？</h2><ol><li>PreVote 见 Q3.</li><li>Lease 见 Q6.</li><li></li></ol><h2 id="Q1：raft算法解决了什么问题？"><a href="#Q1：raft算法解决了什么问题？" class="headerlink" title="Q1：raft算法解决了什么问题？"></a>Q1：raft算法解决了什么问题？</h2><p>raft 是一个分布式共识协议（算法），其主要作用是让集群中的节点对某件事情达成一致，如客户端发起更新请求，为了保证多个节点的数据状态一致，就需要让该更新请求在所有节点上都应用成功，否则更新请求失败。raft 算法可以看作一个黑匣子，当某个节点接收到客户端的请求后，首先将该请求交给 raft 模块，由 raft 模块负责节点间的协商，最后将结果返回给节点，节点再反馈客户端。如下图的复制状态机所示。</p><p><img src="/img/raft/rsm.png"></p><p>图中的 consensus module 就相当于 raft 黑匣子，state machine 可以理解为 kv 键值数据库。还需要注意两点：1）日志记录和协商同步进行；2）图中是分层的，表示多个客户端和集群节点。</p><h2 id="Q2：在实际实现中，整个分布式系统的流程如何？"><a href="#Q2：在实际实现中，整个分布式系统的流程如何？" class="headerlink" title="Q2：在实际实现中，整个分布式系统的流程如何？"></a>Q2：在实际实现中，整个分布式系统的流程如何？</h2><p>这个问题实际是关于</p><ul><li>集群节点如何与客户端交互？</li><li>集群节点如何与 raft 模块交互？</li><li>raft 模块如何与其他 raft 模块交互？</li></ul><p>要解答这些问题，需要借助现有的成熟的工业系统，比如 etcd、tikv 等等，因为比较熟悉 TinyKV，所以以它为例。</p><p>TinyKV 的设计参照了 etcd，将 raft 模块设计成独立的部分，raft 需要的网络、存储服务由上层（非 raft)提供，比较具有灵活性，具体流程如下：</p><ul><li>客户端向节点发送请求（put&#x2F;get&#x2F;delete…)</li><li>节点准备 WAL(write-ahead-log)</li><li>节点将请求封装成 entry 发送给 raft 模块</li><li>节点获取 raft 算法的输出，主要有以下部分：<ul><li>raft 需要存储的日志记录（unstable entry 和一些 raft 自身状态信息）</li><li>committed entry（已经在大多数节点间达成一致的 entry）</li><li>messages，需要发往其他 raft 模块的消息</li></ul></li><li>节点获取到 raft 的输出后，按其类型做一些操作，对于 unstable entry 和状态信息，执行持久化操作；对于 committed entry，它们已处于安全状态，可以应用于数据库了，并且可以就这些 entry 中的请求向客户端反馈成功；对于messages，将其发送到对应的 raft 模块。完成以上操作后，通知 raft 模块。</li></ul><p><u>为什么 raft 会存储 unstable entry 和状态信息，比如 peers，是为了从崩溃中恢复</u></p><p><strong>总结一下</strong>，raft 模块被独立实现，其算法输入来自上层（这里的输入可能是客户端请求，也可能是其他 raft 模块的消息），其算法输出由上层负责处理（存储、发送等），从这里也能知道，raft 根本不关心 entry 中的具体请求，那是上层逻辑的责任，它只需要采取办法能够唯一标识一条 entry 即可（Term、Index）。</p><h2 id="Q3：当网络分区发生时，raft算法有什么表现？"><a href="#Q3：当网络分区发生时，raft算法有什么表现？" class="headerlink" title="Q3：当网络分区发生时，raft算法有什么表现？"></a>Q3：当网络分区发生时，raft算法有什么表现？</h2><p>该问题比较大，需要分类讨论：</p><ul><li>单个 follower 节点被隔离，恢复后，会发生什么？</li><li>网络分区发生时，leader 处在少数部分，恢复后会发生什么？</li><li>网络分区发生时，leader 处在多数部分，恢复后会发生什么？</li></ul><p><font color=red>对于第一个问题</font>，先看被隔离节点的表现：因为是 follower，它只能被动应答，在一段时间内没有异常发生。等到 election_timeout 后，它自增 Term，发起选举请求，由于网络问题，其他节点接收不到该请求，然后再次等到 election_timeout，再次自增 Term，发起选举请求……它重复该操作，直到从隔离中恢复。</p><p>该节点（记为 A）从隔离中恢复后，可能会先收到 leader 发来的 AppenEntriesRPC 或者 HeartBeatRPC，但 leader.Term &lt; A.Term，根据算法，这些 RPC 对 A 没有影响，但 leader 会受到影响（response 中的 Term 比 leader 的 Term 大），leader 会转变为 follower。集群中先超时的节点会率先发起选举请求，由于存在选举限制：<strong>要获取到大多数的选票，就必须具有最新的日志记录（通过比较最后一份日志的 Term 和 Index 判断）</strong>，这样的选举可能会持续多次，但无论如何节点 A 都不可能当选 leader，因为它被隔离，没有后面新追加的日志。也就是说，节点 A 的重新加入造成了系统不必要的抖动，其原因在于，节点 A 在隔离期间盲目地自增 Term。</p><p>etcd 是如何解决该问题的呢？采用 PreVote 机制，即当一个节点超时后，它并不急于自增 Term，而是先发起选举请求，如果能获取到大多数的选票，再自增 Term 重新发起选举。这样，当 follower 从隔离中恢复后，就不会因为 Term 过大干扰集群的正常流程。下面是 Raft 博士论文中的叙述：</p><blockquote><p>The Pre-Vote algorithm solves the issue of a partitioned server disrupting the cluster when it rejoins.</p></blockquote><p><u>然而，还有这样一种情况</u>，当 A 从隔离中恢复后，由于其未收到 heartbeat 超时，发起选举，虽然其 Term 没有增加（PreVote 的限制），但是它在大多数节点中仍然具有较新的日志（可能是隔离期间没有新的请求，也可能是隔离时间比较短，新的日志还没有复制到大多数的节点上），依然可以当选 leader。集群当然可以正常工作，但旧 leader 上还未来得及复制的新日志就会被覆盖，客户端也就需要超时重试，这实际上也算一个干扰，该问题的解决方式会在 Q6 中问题三提到（租约期-Lease）。</p><br><p><font color=red>对于第二个问题</font>，leader 处在少数节点分区部分，根据 raft 要求，一条 entry 能被提交，该entry 至少需要被 N&#x2F;2 + 1 个节点安全复制，因此上层交付的任务 proposal 都无法被提交，自然无法被应用到数据库和反馈客户端，客户端会出现请求超时。后面的客户端请求可能会被路由到另外一个节点，直到请求能够被正常执行。</p><p>当网络分区恢复后，该 leader 会接受来自新 leader 的 RPC 请求，转换成 follower，开始正常的日志复制。该种情况下，是否会出现第一个问题中的场景呢？是有可能的，比如四个节点，每个分区中存在两个节点，包含 leader 的分区不会触发新的选举，但另外一个分区会发起多次的选举（或预选举），<strong>这种情况下，整个系统瘫痪，无法对外服务</strong>。</p><br><p><font color=red>对于第三个问题</font>，这种情况相对较简单，系统可正常对外服务，少数分区可能存在多次选举，但分区恢复后，可以开始正常的日志复制，具体过程在前两个问题中已经提及。</p><h2 id="Q4：leader-commit-日志之前崩溃了，会发生什么？"><a href="#Q4：leader-commit-日志之前崩溃了，会发生什么？" class="headerlink" title="Q4：leader commit 日志之前崩溃了，会发生什么？"></a>Q4：leader commit 日志之前崩溃了，会发生什么？</h2><p>该问题在 raft 论文中有论述，是关于如何处理前任 leader 复制的日志。<strong>我当时的疑问是</strong>：leader 将最新的日志复制到了一部分节点后，或许是还未满足大多数原则，或许是 commit 之前就崩溃了，这些最新的日志会被怎么处理？</p><p>后来再次仔细阅读论文，发现并理解了更加细微的点。借助论文中对该问题的讨论部分的图示，再来复盘一下：</p><p><img src="/img/raft/Q4_1.png"></p><p>图中方块中的数字标识 Term，上方的数字标识 Index。</p><ul><li><p>(a)中，S1 为 Term2 的 leader，在将 Index&#x3D;2 的日志复制到 S2 后崩溃。</p></li><li><p>(b)中，S1 崩溃后，S5 在 Term3 当选 leader（根据选举限制，在最后一条日志中，S5 具有更大的 Term，所以能够当选）</p></li><li><p>(c)中，S5 崩溃，S1 在 Term4 当选 leader，并继续复制日志，此时它将 Index&#x3D;2 的日志成功复制到了大多数节点上，但还未提交。</p></li><li><p>(d1) 和 (d2) 描述两种情况：<br>  第一种是(d1)，以前任期复制的日志（未提交）被后面新的日志覆盖。客户端等待响应超时，会重新发起请求（我之前还在担心，这会不会造成数据丢失，太天真了）。对应的是 S1 再次崩溃，在 (c) 的局面下，S5 再次当选 leader（图中未画出新的任期，S5 可以获得 S2-4 的选票），由于复制日志以 leader 的日志为准，所以 Index&#x3D;2 的以前任期的日志会被 S5 的 Index&#x3D;2 的日志覆盖。</p><p>  第二种是(d2)，以前任期复制的日志可以被后面新的 leader 提交（属于被动提交，因为 raft 中，提交一条日志，就表示该条日志之前的所有日志都已被提交）。对应的是，S1 在崩溃之前将日志复制到了大多数节点上，此时 S5 已经不可能再当选，新的 leader 只能在 S1-3 之中。假设，S1 未崩溃，那么，S1 通过提交 Index&#x3D;3 的日志，之前的日志也就一起被动提交了；就算 S1 在提交之前崩溃了，新的 leader 通过提交当前任期的日志也能提交以前任期的所有日志。</p></li></ul><p><strong>从这里，应当认识到两点</strong>：</p><ul><li>复制的日志可能会被覆盖，客户端会重试。</li><li>raft 算法中，leader 会强制要求其他节点的日志与自己一致，对安全性的考虑应该结合选举限制一起理解。</li></ul><h2 id="Q5：Raft-成员变更过程如何理解？"><a href="#Q5：Raft-成员变更过程如何理解？" class="headerlink" title="Q5：Raft 成员变更过程如何理解？"></a>Q5：Raft 成员变更过程如何理解？</h2><p><font color=dark-green>Raft 成员变更（Cluster Membership Change）</font><br>raft 集群中的每一个节点都会记录集群节点的信息，即 peer 配置，如果节点发起选举或是 leader 复制日志都需要该配置，以便将请求发送给其他节点。<u>成员变更本质上就是 peer 配置的变化</u>：向集群中新增节点，就是向 peer 配置中新增节点信息；从集群中移除节点，就是从 peer 配置中删除节点信息。</p><p>raft 博士毕业论文中设计了两种算法来处理成员变更：</p><ul><li>方法一：一次变更只包括一个节点加入集群或从集群中移除；</li><li>方法二：一次变更包括多个节点加入集群或从集群中移除。</li></ul><p>由于对安全性的考虑，第二种方法会引入额外的复杂度，不如第一种简单，考虑如下情况（见下图）：<br><img src="/img/raft/Q5-1.png"><br>当集群中成员发生变更时，该变更不可能同时应用在所以节点上，上图集群中有 3 个节点，现新增 2 个，在变更的过程中，有些节点更新了自己的 peer 配置，感知到的是 5 个节点，如 server 3，而其他节点可能还未得到更新，记录的仍是 3 个节点，如 server 1 和 server 2。<u>如果此时发生选举，就可能会选出两个 leader</u>：</p><ul><li>server 3：获得 server 3、4、5 的选票。</li><li>server 1：获得 server 1、2 的选票。</li></ul><p>由于 server 1 的 peer 配置还未更新，认为集群中只有 3 个节点，那么它已经得到大多数的选票了，可以当选为 leader；而 server 3，根据它的 peer 配置同样也得到了大多数的选票，可以当选为 leader. 同时移除多个节点也会导致该问题。也就是说一次涉及多个节点加入或移除的变更是不安全的，不能保证唯一的 leader。</p><p>为了解决这个问题（会产生两个大多数群体），第二种方法会引入一个过渡状态，被称为 joint consensus（共同一致），有时间再讨论这种方法。<font color=red>本节只讨论第一种方法</font>。</p><p><u>为什么一次变更只包含一个节点的情况（方法一）不会引发上述问题呢？</u>考虑如下场景：<br><img src="/img/raft/Q5-2.png"></p><p><strong>如上图 case 1 所示（case 2 类似）</strong>，集群中刚开始有 A、B、C 三个节点，$T_0$ 时刻节点 D 加入了该集群。</p><p>$T_1$ 时刻，节点 C 感知到新节点的加入，更新了 peer 配置，此时，集群分为了两部分：更新了 peer 的节点（C、D）和未更新 peer 的节点（A、B），但这并不会将集群分裂成两个大多数群体，C 或 D 要想当选 leader，至少需要 3 票（包含自身选票），A 或 B 要想当选，至少需要获得 2 两票（包含自身选票），但这两种情况不可能同时发生，前者的 3 票与后者的 2 票存在重合。</p><p>$T_2$ 时刻，B 节点也感知到了新节点的加入，更新了 peer 配置，此时集群中依然存在两个部分：更新了 peer 的节点（B、C、D）和未更新 peer 的节点（A），这种情况下同样不会产生两个 leader.</p><p>$T_3$ 时刻，A 节点也感知到了新节点，至此整个集群都完成了 peer 配置的更新。</p><p><strong>再来看 case 4 所示（case 3 类似）</strong>，集群中刚开始有 A、B、C、D 四个节点，$T_0$ 时刻节点 D 被移除了该集群。</p><p>$T_1$ 时刻，节点 C 感知到了节点的移除，更新了 peer 配置，此时集群分为两部分：更新了 peer 配置的节点（C）和未更新 peer 配置的节点（A、B），同样，如果此时需要选举 leader，不会产生两个，若 C 想要当选 leader，至少需要 2 票（包含自身选票），若 A 或 B 想要当选，至少需要 3 票（包含自身选票），前者的 2 票与后者的 3 票存在重合，不可能同时满足。</p><p>$T_2$ 时刻，节点 B 感知到了节点的移除，更新了 peer 配置，虽然集群中还是存在两个部分，同样也不会产生两个 leader.</p><p>$T_3$ 时刻，节点 A 也感知到了节点的移除，至此整个集群都完成了 peer 配置的更新。</p><p><font color=red>因此，若一次变更只包含单个节点的加入或删除，那么不会造成集群的分裂，这种情况下变更是安全的。</font></p><br><p><strong>如果存在多个节点需要加入或删除怎么办？</strong></p><p>论文指出，对于这种情况，可以一次一个节点地变更。由上面的讨论可知，若一次变更只涉及一个节点，那么当其他节点感知到变更时，就可以更新并应用新的 peer 配置，并且不会产生安全性问题。下面是论文描述的单个节点的变更过程：</p><blockquote><ul><li>leader 接收到成员变更请求（一种特殊的 entry：$C_{new}$）后，将该 entry 存在日志中，并应用该配置，同时将 $C_{new}$ 复制给其他 follower 节点。</li><li>follower 节点收到 $C_{new}$ 后，同样存入日志中，并应用该新配置。</li><li>当一个节点（candidate 或 leader）需要给其他所有节点发送请求时，使用的是从日志中检索到的最新的配置。</li><li>当 $C_{new}$ 在所有节点中达成一致后被提交，此次变更就完成了，leader 反馈变更成功，可以开始下一次的变更。</li></ul></blockquote><p>上述过程其实和普通的 raft 过程没什么区别，主要的不同在与配置是即时生效的。<u>这里有一个隐藏的细节</u>：在 $C_{new}$ 复制的过程中（还未提交），若 leader 崩了，新的 leader 选了出来（Q4 描述的场景），那么 $C_{new}$ 就有可能会被丢弃，当然节点变更也就失败了，最后会超时重试。事实上，我在做 TinyKV 项目时发现，$C_{new}$ 并不是即时生效的，而是上层模块处理 committedEntries 时若发现 $C_{new}$ 被提交了，才会更新节点的配置。</p><h2 id="Q6：如何提升成员变更中的集群可用性？"><a href="#Q6：如何提升成员变更中的集群可用性？" class="headerlink" title="Q6：如何提升成员变更中的集群可用性？"></a>Q6：如何提升成员变更中的集群可用性？</h2><p>要想解答问题首先得了解问题，成员变更过程中集群的可用性为何会受影响？分为三个部分：</p><ol><li><u>新加入一个节点到集群中时</u>，该新节点的日志大概率非常落后于 leader（甚至为空），在追赶上（catch up）leader 之前，这个新节点不能参与任何新日志提交的决策，甚至是拖累。比如一个初始有 3 个节点的集群新加入了一个节点，在新节点追赶的过程中，初始 3 个节点中的某个节点崩溃下线了，那么该集群在一段时间内都处于不可用（无法提交任何新的日志），因为达成一致需要大多数节点（$\frac{3}{4}$）成功复制了该日志条目，而新节点还在追赶中。</li><li><u>当 leader 节点收到移除自己的请求时</u>。论文中到了该问题（因为 Q5 中方法二需要处理该问题），其实不算一个大问题，解决方法很多，后面会提到。</li><li><u>被移除的节点可能会发起选举干扰集群的正常流程</u>。当 leader 收到移除某个节点的请求后，leader 不再给被移除的节点发送任何日志（包括移除请求日志 $C_{new}$），同样也不会发送心跳包。被移除的节点感到很无辜啊，它并不知道自己已经不属于该集群了，所以等待它的比如是超时，它自然会发起选举请求，也就干扰到了集群。</li></ol><p><font color=red>对于第一个问题</font>，解决方法很简单，新节点刚开始加入时，将其当作 non-voter，即不被算作大多数，当其追赶上 leader 的日志后，由 leader 再一次的将其作为正式成员加入到集群中。</p><p>还有一个问题需要解决：<u>如何定义“追赶上”？</u>当新节点还在复制日志时，新的日志又会到达 leader，如果是完全相同，基本不可能。细想一下，新节点加入集群影响可用性主要是因为它需要较长的时间复制日志，那如果这个时间比较短，对可用性的影响自然就可以接受，论文中建议这个时间低于 election timeout，所以当新节点与 leader 的日志差距小到复制的时间开销低于 election timeout，那么就可以将其正式加入集群了。<u>具体算法是这样的</u>：</p><blockquote><p>新节点从 leader 处复制日志被划分为多个 round. 在每个 round 中复制 leader 在该次 round 开始时具有的所有日志，复制过程中新到达的日志会在下一个 round 中复制。leader 可能会等待几个 round（比如 10），若最后一次 round 所花费的时间少于 election timeout，那么 leader 就可以将该新节点正式加入集群中。<br>如果新节点出问题了，或者太慢了（无论是哪种都会超时），那么 leader 会主动中止本次成员更改，返回失败，后面节点可以继续请求加入，这时，它已经具有一部分日志了，成功的概率大大地增加。</p></blockquote><br><p><font color=red>对于第二个问题</font>，有两种方法可以解决：</p><ol><li>如 Q5 中提到的，可以在该 $C_{new}$ 提交后再应用新的配置。但需要注意一点，当 leader 接受到移除自己的请求 $C_{new}$ 后，leader 不应该参与任何日志的提交决策，即统计大多数时不应该算上自己，比如集群中只有两个节点时，当 leader 提交 $C_{new}$ 后移除自己，另外一个节点很可能根本没有复制 $C_{new}$，集群就陷入不可用状态了。</li><li>使用 Q7 中提到的 leadership transfer.</li></ol><br><p><font color=red>对于第三个问题</font>，粗看起来有点像 Q3 的问题一，采用 PreVote 优化就能解决。实际中，很有可能当一个节点被移除后超时发起选举时，在大多数节点中它依然具有较新的日志，也就是它依然能够当选 leader. 有意思的事情发生了，被移除的节点居然还能自己回归（当然移除节点请求会超时返回失败，稍后可能会重试）。如果移除请求会不停重试，该节点始终会被移除集群，然而这个过程就耗费了太多时间，而且旧 leader 的一些日志还可能会被覆盖，导致客户端请求超时失败。</p><p>PreVote 能够阻止部分的干扰，但显然还不够，<u>因此 raft 引入了租期（Lease）的概念</u>：如果一个 follower 能够在 election timeout 时间内收到 leader 的信息（AppendEntries 或 HeartBeat），则该 follower 处在租期内。当一个 follower 收到一个选举投票请求时，如该 follower 还处在租期内，那么 follower 会拒绝或者直接忽视该投票请求。</p><p>下面代码是 etcd 对 Lease 的实现。引入上面的方法后，尽管被移除的节点在大多数节点中具有较新的日志，只要 leader 正常工作，被移除的节点就不可能当选，自然也就不会干扰到集群。然而，Lease 可能会影响 Q7 中的 leadership transfer，所以代码中有一个特殊的变量 force，来判断是不是处于 leadership transfer.</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *raft)</span></span> Step(m pb.Message) <span class="hljs-type">error</span> &#123;<br><span class="hljs-comment">// Handle the message term, which may result in our stepping down to a follower.</span><br><span class="hljs-keyword">switch</span> &#123;<br><span class="hljs-keyword">case</span> m.Term == <span class="hljs-number">0</span>:<br><span class="hljs-comment">// local message</span><br><span class="hljs-keyword">case</span> m.Term &gt; r.Term:<br><span class="hljs-keyword">if</span> m.Type == pb.MsgVote || m.Type == pb.MsgPreVote &#123;<br>force := bytes.Equal(m.Context, []<span class="hljs-type">byte</span>(campaignTransfer))<br>inLease := r.checkQuorum &amp;&amp; r.lead != None &amp;&amp;<br>                       r.electionElapsed &lt; r.electionTimeout<br><span class="hljs-keyword">if</span> !force &amp;&amp; inLease &#123;<br><span class="hljs-comment">// If a server receives a RequestVote request within the minimum election timeout</span><br><span class="hljs-comment">// of hearing from a current leader, it does not update its term or grant its vote</span><br>r.logger.Infof(<span class="hljs-string">&quot;%x [logterm: %d, index: %d, vote: %x] ignored %s from %x [logterm: %d,</span><br><span class="hljs-string">                index: %d] at term %d: lease is not expired (remaining ticks: %d)&quot;</span>,<br>r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), r.Vote, m.Type, m.From, m.LogTerm,<br>                    m.Index, r.Term, r.electionTimeout-r.electionElapsed)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br>            <span class="hljs-comment">// ...</span><br>&#125;<br>        <span class="hljs-comment">// ...</span><br>    &#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>其实这里我有个疑问</strong>：实现 Lease 后，岂不是会增加正常选举的时长？假设某时刻 leader 维修下线了，一段时间后，某个 follower 率先超时向其他所有发送选举请求，由于其他节点都还未超时（还在租期内），该 follower 一定不会当选，且只有当集群中超过半数的节点都超时后才可能会有 leader 产生。</p><h2 id="Q7：Leadership-Transfer-过程是怎样的？"><a href="#Q7：Leadership-Transfer-过程是怎样的？" class="headerlink" title="Q7：Leadership Transfer 过程是怎样的？"></a>Q7：Leadership Transfer 过程是怎样的？</h2>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
    </categories>
    
    
    <tags>
      
      <tag>raft</tag>
      
      <tag>分布式一致性协议</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo 博客使用记录</title>
    <link href="/2022/06/13/Hexo%20%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
    <url>/2022/06/13/Hexo%20%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>终于搬新博客了</p><h3 id="Record-1：文章内部图片死活加载不出来"><a href="#Record-1：文章内部图片死活加载不出来" class="headerlink" title="Record 1：文章内部图片死活加载不出来"></a>Record 1：文章内部图片死活加载不出来</h3><p>刚使用 fluid 主题时，文章封面图片可以设置，但文章内的图片无法显示，markdown 语法和 HTML 语法都无济于事。最后死马当活马医，将hexo-asset-image卸载了，重试，居然可以了。</p><h3 id="Record-2：图片存放问题"><a href="#Record-2：图片存放问题" class="headerlink" title="Record 2：图片存放问题"></a>Record 2：图片存放问题</h3><p>有两个文件夹可以存放：&#x2F;source&#x2F;img 和 &#x2F;public&#x2F;img，但图片不能放在后者中，因为 &#x2F;public 目录下的东西都是执行 hexo g 生成的（从 &#x2F;source 目录获取相关内容），若执行 hexo clean 就全部被删除了。</p><h3 id="Record-3-引用本地文章"><a href="#Record-3-引用本地文章" class="headerlink" title="Record 3: 引用本地文章"></a>Record 3: 引用本地文章</h3><p>使用这样的格式 (明白为什么要用图片吧 -_- )：<br><img src="/img/usage-records/cite-local-paper.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>2022年(23届)秋招记录</title>
    <link href="/2022/06/07/2022%E5%B9%B4-23%E5%B1%8A-%E7%A7%8B%E6%8B%9B%E8%AE%B0%E5%BD%95/"/>
    <url>/2022/06/07/2022%E5%B9%B4-23%E5%B1%8A-%E7%A7%8B%E6%8B%9B%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>本文对秋招做一个记录，算是面经。</p><p><img src="/img/2022%E5%B9%B4%E7%A7%8B%E6%8B%9B/1.jpg" alt="简历"></p><h3 id="1-华为-软件开发工程师，数据库开发"><a href="#1-华为-软件开发工程师，数据库开发" class="headerlink" title="1. 华为-软件开发工程师，数据库开发"></a>1. 华为-软件开发工程师，数据库开发</h3><p>投递：<br>笔试：<br>面试：</p><blockquote></blockquote><h3 id="2-SHEIN-后台开发工程师，云平台"><a href="#2-SHEIN-后台开发工程师，云平台" class="headerlink" title="2. SHEIN-后台开发工程师，云平台"></a>2. SHEIN-后台开发工程师，云平台</h3><p>投递<br>笔试：<br>面试：</p><blockquote></blockquote><h3 id="3-字节-后端开发工程师，基础架构"><a href="#3-字节-后端开发工程师，基础架构" class="headerlink" title="3. 字节-后端开发工程师，基础架构"></a>3. 字节-后端开发工程师，基础架构</h3><p>投递：<br>笔试：<br>面试：</p><blockquote></blockquote><h3 id="4-PingCAP"><a href="#4-PingCAP" class="headerlink" title="4. PingCAP"></a>4. PingCAP</h3><p>投递：<br>笔试：<br>面试：</p><blockquote></blockquote><style>    .markdown-body {        display: none    }</style>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
