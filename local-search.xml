<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>事务篇五：两阶段提交协议</title>
    <link href="/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/"/>
    <url>/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>事务篇三：事务的并发控制</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a>中介绍了事务基本概念和具有特性，在<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a>中介绍了调度要具备何种性质，才能在并发事务执行后使系统处于一致性状态和事务失败后能让系统回到安全状态，还论证了隔离性与一致性的关系。</p><p><strong>并发控制的工作就是，在事务并发时，确保只会产生具备这种性质的调度方案</strong>，具体地，并发控制策略的目标是同时保证两点：</p><ul><li>使系统具有较高的事务并发度</li><li>确保生成的调度方案，同时是冲突可串行化（conflict serializable），可恢复（recoverable）和非失败连锁式（cascadeless）的。</li></ul><p>并发控制策略有许多，<font color=red>本文主要集中在两阶段加锁（two-phase locking）和快照隔离（snapshot isolation）</font>。介绍完这两个并发控制协议后，再讨论各个隔离级别的实现，如何避免事务总论中提到的现象。</p><h2 id="Two-Phase-Locking-Protocol"><a href="#Two-Phase-Locking-Protocol" class="headerlink" title="Two-Phase Locking Protocol"></a>Two-Phase Locking Protocol</h2><p>为了简单，目前讨论的事务只有两种操作：访问（read）和更新（write），实际上还有如：插入（insert）、删除（delete）和含条件的读取（predicate read），它们会放在以后慢慢讨论。</p><p>同时，现在只讨论操作针对的是单个数据（如元组），这其实是锁的粒度问题，后面会讨论大粒度锁的情况。</p><p>为此，最简单的锁具有两种模式：</p><ul><li>共享锁（shared-mode lock: S） </li><li>排他锁（exclusive-mode lock: X）</li></ul><p>在基于锁的的协议中，执行 read 操作时会对数据加共享锁，执行 write 操作时会对数据加排他锁，两种锁的相容性（compatibility）如下：</p><table><thead><tr><th align="center"></th><th align="center">S</th><th align="center">X</th></tr></thead><tbody><tr><td align="center">S</td><td align="center">true</td><td align="center">false</td></tr><tr><td align="center">X</td><td align="center">false</td><td align="center">false</td></tr></tbody></table><p><img src="/img/concurrency-control/txn-3.jpg" alt="事务3"></p><h3 id="两阶段加锁协议"><a href="#两阶段加锁协议" class="headerlink" title="两阶段加锁协议"></a>两阶段加锁协议</h3><p><font color=dark-green>基础版两阶段加锁协议</font><br>该协议要求一个事务请求加锁和释放锁分别集中在两个阶段：</p><ul><li>growing phase. 该阶段事务可以申请加锁，而不能释放任何锁</li><li>shrinking phase. 该阶段事务可以释放锁，但不能申请新的锁</li></ul><p>如图事务3满足两阶段加锁要求，需要注意，基本的两阶段加锁协议并不要求所有的释放锁操作都必须放在最后，如事务3将 unlock(B) 可以移到 lock-X(A) 后面，仍然满足两阶段加锁的定义。</p><br><p><font color=dark-green>基础版两阶段加锁协议的特点</font>：</p><ul><li><u>保证了 conflict serializability</u>. 假设一个调度内的事务都遵守该两阶段假设协议，同时将事务 growing phase 获取到的最后一个锁的位置定义为 lock-point，那么各个事务就可以根据其 lock-point 排序，该顺序一定是可串行化的。（证明：可以构造该调度的先行图，先行图有两种可能，要么无环，要么有环。若无环调度冲突可串行化；若有环，事务必然是相互等待对方到达 shrinking phase 释放冲突的锁，这是死锁。）</li><li><u>可能存在死锁</u>。如上</li><li><u>不能保证 recoverable 和 cascadeless rollback</u>.</li></ul><p>先行图、recoverable 和 cascadeless 的定义见<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a></p><br><p><font color=dark-green>如何保证 recoverable 和 cascadeless？</font><br>这里引入两种升级版两阶段加锁协议，一步到位之间使得满足这些协议的调度满足 cascadeless（自然就满足了 recoverable）。</p><p><strong>Strict Two-Phase Locking Protocol</strong>:<br>在基础版两阶段加锁协议上，增加了这样一条要求，所有的 exclusive-mode lock 必须要等到事务提交之后再释放。</p><p><strong>Rigorous Two-Phase Locking Protocol</strong>：<br>在基础版两阶段加锁协议上，增加了这样一条要求，所有的 lock 必须要等到事务提交之后再释放。</p><p>依据 cascadeless 的定义，它们显然满足。这两种方式被广泛用于商业数据库系统中，也就是锁的释放通常要等到事务完成（提交或终止）。</p><br><p><font color=dark-green>如何处理死锁？</font><br>事务存在循环等待即进入了死锁，处理死锁一般有两种思路，这两种思路都会涉及到事务的回滚：</p><ul><li><u>死锁避免</u>。该思路是让系统永远不会进入死锁状态，一般用于死锁发送频率高的场景中。</li><li><u>死锁检测 + 恢复</u>。该思路是在死锁发生后在人为干预，解除死锁状态，一般用于死锁发生频率较低的场景。</li></ul><h3 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h3><p>实现死锁避免也有许多思路：</p><ul><li><u>破除循环等待条件</u>。实现方法有 1)通过给所有数据规定一个次序，加锁只能按该顺序加锁；2)事务开始之前完成对所有数据的加锁。最大的问题在于，如何准确的知道事务会涉及哪些数据。</li><li><u>基于抢占和回滚事务方式</u>。抢占很好理解，当两个事务发生冲突了，优先级高的事务可以让优先级低的事务回滚，下面主要介绍这种方式。</li></ul><p><font color=dark-green>基于抢占和回滚事务方式的死锁避免算法</font><br>抢占需要确定优先级，下面的两个算法采用时间戳（timestamp）的方式决定优先级的大小。每个事务在其开始时得到一个时间戳，时间戳越小，代表事务越老，若事务回滚重新开始，那么它保持时间戳不变。注意，该时间戳只用于两个事务发生锁冲突时：</p><ul><li><u>wait-die 算法</u>。当事务 TA 对某个数据加锁时，发现事务 TB 已经已经持有该数据的排它锁，若 TA 时间戳小于 TB 的时间戳，那么 TA 更老，选择等待；若 TA 的时间戳更大，则 TA 回滚。很明显，该算法不涉及抢占。该算法为何能避免死锁？因为它避免了循环等待的条件，假设形成了环，环首持有环尾需要的锁，那么环尾会直接回滚，矛盾。</li><li><u>wound-wait</u>。和上个算法不同的是，该算法涉及抢占，若 TA 的时间戳更小（TA 更老），那么 TA 会直接抢占该锁，TB 则回滚；若 TA 的时间戳更大，则 TA 等待。其避免死锁的证明同上。</li></ul><p>以上两个算法最大的缺点在于可能会造成不必要的回滚，一个简单的优化是在回滚之前先等待一段时间，不过这段时间对每个事务最好是随机的，而且其长度难以确定。</p><p>死锁检测与恢复不想写了，有时间再写吧。</p><h3 id="更丰富的锁粒度"><a href="#更丰富的锁粒度" class="headerlink" title="更丰富的锁粒度"></a>更丰富的锁粒度</h3><p>一个简单的例子，当锁粒度只有元组时，一个事务想更新一张表，那它就必须对表的所有元组加锁，显然不合适，反之，若只更新几个元组，用不上对整张表加锁。</p><p>多粒度一般采用分级结构实现，该结构可以称为 multiple-granularity tree，如下图：<br><img src="/img/concurrency-control/multiple-granularity.jpg"></p><p>（未完待续）</p><h2 id="Snapshot-Isolation"><a href="#Snapshot-Isolation" class="headerlink" title="Snapshot Isolation"></a>Snapshot Isolation</h2><p>快照隔离（snapshot isolation）属于多版本并发控制技术的一种。多版本并发技术通过维护一份数据的多个版本，让事务可以访问（read）数据的上一个版本（当前版本可能正在被修改），而不是当前未提交事务正在修改的版本（该未提交事务可能更老，即开始于当前事务之前；也可能发生在未来，即如果按照串行执行，当前事务本应该看不到后面事务的更新，但并发执行，就有可能了）。</p><p>版本的概念并不容易理解，后面详细介绍 snapshot 后，再理解它在 percolator 的实现，这样就清楚得多了。</p><br><p><font color=dark-green>基本概念</font><br>当事务开始执行时，数据库给该事务一个 snapshot，里面包含的是该事务需要的、已被提交的数据，之后该事务对数据的所有操作都在该 snapshot 上操作（该 snapshot 暂存在事务私有内存中，其他事务不可见）。对于只读事务，操作完就可以结束了（提交或者失败），不需要等待，也不会因为并发而被回滚；对于更新事务，它还需要将更新写入数据库（写入应当是一个原子操作），因此还涉及到一个验证步骤，后面会详细结束。</p><p>在快照隔离中，只读事务不需要等待，也不会 abort，<font color=red>那会不会有不可重复读的可能呢？毕竟多次读取，可不可能在某一次就读到了刚刚被更新的值呢？</font>不会的，因为读取到快照后，该快照被该事务独享（或者说被只读事务共享），不会涉及到更新。</p><br><p><font color=dark-green>实现细节</font><br>该协议给进入系统的每个事务两个时间戳：</p><ul><li><u>startTS</u>. 在事务开始时获取</li><li><u>commitTS</u>. 在事务准备写入更新到数据库的时间（或者是开始验证阶段的时间）</li></ul><p>每个被更新的数据都带有一个时间戳（即数据的版本号），即执行该更新事务的 commitTS，也就是说同一份数据可能存在多个版本。时间可以是系统时间，也可以是逻辑时间（计数器），只要保证不会存在相同的时间戳即可。</p><p>当一个事务 TA 读取一个数据时，读取到的数据具有这样的特点：在所有版本号小于等于 startTS(TA) 的数据中具有最大的版本号。从事务角度理解，一个事务看不到，任何在该事务开始之后提交的事务所作的更新。</p><br><p><font color=dark-green>更新验证阶段</font><br>按照上面所说，事务读取数据后各自操作，其他事务感知不到，那么就可能会出现两个并发的事务读取了同一个数据的同一个版本，那么在更新时就会产生冲突，如果不做限制，就会发生更新丢失（lost update）。如何解决冲突，是验证阶段的工作。</p><p>一般有两种方法可以防止更新丢失，介绍之前先看一下 snapshot isolation 中的并发定义，当下面情况中任一发生时，就说事务 TA 和 TB 并发：</p><ul><li>startTS(TA) &lt;&#x3D; startTS(TB) &lt;&#x3D; commitTS(TA)</li><li>startTS(TB) &lt;&#x3D; startTS(TA) &lt;&#x3D; commitTS(TB)</li></ul><p><strong>第一种方法称为 first committer wins</strong>. 假设同一时间只有一个事务在验证。当事务 TA 准备写入更新时，执行如下步骤：<br><u>1.检测是否存在冲突</u>。检查 TA 更新的每一个数据，看是某个数据否存在一个版本号落在 [startTS(TA), commitTS(TA)].<br><u>2.若有</u>，则 TA abort.<br><u>3.若无</u>，正常写入更新和后续步骤，比如提交。</p><p><strong>第二种方法称为 first updater wins</strong>. 该方法会应用到锁机制，当 TA 准备写入更新时，会申请需要写入数据的写锁，根据锁是否申请成功，有两种情况：<br><u>写锁申请成功</u>，若当前数据已经被并发的事务更新了（检测方法同上），则 TA abort，否则执行正常的写入更新和后续步骤，比如提交。<br><u>写锁申请失败</u>，假如该锁被 TB 占有，TA 等待 TB abort 或者 commit。若 TB abort，TA 获取到该锁，后续操作同上一种情况。若 TB commit，那么 TA 只能 abort.</p><br><p><font color=dark-green>Snapshot Isolation 问题与解决</font><br>snapshot isolation 吸入的地方在于，读写分离，互不干扰。<strong>但它有一个致命的缺陷：不保证可串行化（serializability）</strong>，如下例子：<br><img src="/img/concurrency-control/snapshot-isolation-1.jpg"></p><p>如果构造该例子的先行图，会发现存在环：该调度非冲突可串行化。在两阶段加锁协议中该调度会产生死锁，不可能顺利执行下去。然而，在快照隔离策略中却可以执行、提交。如果这两个事务以可串行化的调度方案并发，那么 A、B 的值是一样的，具体的值取决于谁先执行，而快照隔离的结果却是将它们的值交换。</p><p><strong>这种现象被称为写偏斜（write skew）</strong>：两个事务读取了对方会更新的数据，但这两个事务更新的数据却没有交集，如 Ti 读取了会被 Tj 更新的 B，Tj 读取了会被 Ti 更新的 A，而它们分别更新 A 和 B，没有交集，所以在验证阶段无法被检测出来。</p><p>发生写偏斜的根本原因是什么？借助上面例子可以发现，发生写偏斜时，调度方案的先行图存在两条 read-write 冲突边，一条入边，一条出边：对于事务 Ti 写入了 A 的一个新版本，而 Tj 读取了 A 之前的一个版本（Tj -&gt; Ti, read-write edge）；对于事务 Tj 写入了 B 的一个新版本，而 Ti 读取了 B 之前的一个版本（Ti -&gt; Tj, read-write edge）：<br><img src="/img/concurrency-control/snapshot-isolation-2.jpg"></p><p><strong>由此，一种新的技术称为 Serializable Snapshot Isolation(SSI) 被提出以保证快照隔离可串行化</strong>。SSI 会追踪并发事务之间的所有 read-write 冲突，检查是否同时存在出边和入边，如果存在，则其中一个事务会被回滚。</p><p>趁热打铁读一读 snapshot isolation 的工业级实现：<a href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/" title="事务篇六：Percolator 随笔">事务篇六：Percolator 随笔</a>呀。</p><h2 id="隔离级别的实现"><a href="#隔离级别的实现" class="headerlink" title="隔离级别的实现"></a>隔离级别的实现</h2><p><font color=dark-green>如何实现读已提交级别？</font><br>读已提交有如下特点：</p><ul><li>从数据库中读时，只会读到已经提交了的数据，即<u>没有脏读（dirty read）</u></li><li>写入数据库时，只会覆盖已经写入的数据，即<u>没有脏写（dirty write）</u></li><li>可能发送不可重复读异常</li></ul><p><strong>何为脏读？</strong>能读取到尚未 commit&#x2F;abort 事务所做的更新，就叫脏读。脏读可能引发只能看到部分更新的问题，比如转账，很可能会出现账户余额蒸发的怪现象。</p><p><strong>何为脏写？</strong>当两个事务更新同一对象时，通常后者会覆盖前者所做的更新，但若是覆盖的是尚未 commit&#x2F;abort 事务的更新，就叫做脏写。脏写同样会产生问题，考虑自动发送邮件例子，一个事务刚刚填入收件人地址，还未提交，另一个事务却覆盖了这个邮件地址，邮件就会发往错误的地方。</p><p>一般数据库通过行锁（row-level lock）<strong>防止脏写</strong>。当事务想要修改特定对象时，它必须首先获得该对象的排它锁，持有到事务提交或中止。可以通过读锁<strong>防止脏读</strong>，然而该方法中，写事务会阻塞读事务，所以数据库通常会为数据保留一个最近的旧值，和正在更新的新值，若新值还未提交，任何读事务都会拿到旧值，若新值提交了，就会读到新值（发生不可重复读异常）。</p><br><p><font color=dark-green>如何实现读未提交级别？</font><br>读未提交有如下特点：</p><ul><li>写入数据库时，只会覆盖已经写入的数据，即<u>没有脏写（dirty write）</u></li><li>可能发生脏读</li><li>可能发生不可重复读异常</li></ul><p>防止脏写的实现方式和读已提交级别一致。</p><br><p><font color=dark-green>如何实现可重复读级别？</font><br>可重复读有如下特点：</p><ul><li>没有脏读</li><li>没有脏写</li><li>不会发生不可重复读异常</li><li>若使用快照隔离提供稳定的视图，则不会出现幻读</li></ul><p>读已提交维护数据的两个版本，但可能会读取到已提交的新值，而对于可重复读级别，可以使用快照隔离，让事务始终读取最开始读取的那一个版本，维持稳定的视图，来<strong>防止不可重复读异常，同时也解决了幻读异常</strong>。</p><p>这里再解释下幻读。下面是一张教师（instructor）薪资表：</p><table><thead><tr><th align="center">ID</th><th align="center">name</th><th align="center">salary</th></tr></thead><tbody><tr><td align="center">001</td><td align="center">Jery</td><td align="center">1000</td></tr><tr><td align="center">002</td><td align="center">Peter</td><td align="center">2000</td></tr><tr><td align="center">003</td><td align="center">Tim</td><td align="center">3000</td></tr></tbody></table><p>现在执行如下的 SQL 语句：</p><p><strong>select</strong> <em>ID</em>, <em>name</em><br><strong>from</strong> <em>instructor</em><br><strong>where</strong> <em>salary</em> &gt; 1000</p><p>假设当前系统采用可重复读隔离级别，一个用户执行了如上的查询，同时另外一个用户执行了如下的插入语句（删除语句也有如下的效果）：<br><strong><center>insert</strong> <strong>into</strong> <em>instructor</em> <strong>values</strong> (004, James, 4000)</center><br>查询事务读取到多少条的数据取决于它和插入事务的先后顺序，若查询事务多次读取，就有可能发生某次读到的内容比上次多，即发生了幻读。</p><br><p><font color=dark-green>如何实现可串行化级别？</font><br>采用 SSI 和 两阶段加锁都可以实现。</p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
      <tag>mvcc</tag>
      
      <tag>snapshot</tag>
      
      <tag>two-phase-locking</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇二：事务的可串行化</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该文章的内容来自 database system concepts 17.5-7，建议先看文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a></p><p>为什么会有事务的串行化？因为数据库系统会允许事务的并发（concurrency），事务并发有如下两个优点：<br><strong>提升吞吐量和资源利用率</strong>。事务通常包含多个操作（步骤），一些操作需要更多的 CPU 计算，而另外一些需要更多的 I&#x2F;O. CPU 和 I&#x2F;O 设备通常可以并行（parallel），如果同时允许多个事务并发，能减少 CPU 和 I&#x2F;O 的空闲时间，那么可以提升吞吐量和资源利用率。</p><p><strong>减少等待时间</strong>。如果所有事务只能串行，那么短事务只能等待长事务结束才能执行。如果事务能够并发，则能在一定程度上共享 CPU 和磁盘资源，减少响应时间。</p><p><strong>并发很有好处，但它可能会破坏事务的隔离性，破坏数据的一致性</strong>。因此有必要研究各个并发的事务需要满足什么关系，才能保证隔离性和数据库的一致性。并发的具体实现方案，参见文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a>。</p><h2 id="Schedules"><a href="#Schedules" class="headerlink" title="Schedules"></a>Schedules</h2><p><strong>考虑一个例子</strong>：有 A 和 B 两个账户，其初始值分别为 1000 和 2000，事务 T1 从账户 A 转账 50 到账户 B，而事务 T2 从账户 A 转账 10% 到账户 B。这两个事务包含许多指令（instructions），当它们并发时，这些指令可能以各种顺序执行（但属于同一个事务的的各指令间相对顺序一定，而属于不同事务的指令间顺序可能变化），这样具体的一个顺序称为一个调度（schedule），如下面的调度1和调度2<br><img src="/img/schedule-serializable/schedule1.jpg" alt="schedule 1"><br><img src="/img/schedule-serializable/schedule2.jpg" alt="schedule 2"></p><p><strong>串行调度（serial schedule）</strong>：在调度中，属于同一个事务的指令出现在一起。如 schedule 1 是串行的调度，而 schedule 2 不是。对于 n 个事务，可以组成最多 n! 个不同的串行调度。</p><p>很明显，串行的调度方案一定能保证隔离性和一致性，但在并发时，不同事务的指令可能交叉，调度并不总是串行的，但如果能保证一个调度方案对数据库的修改与某个串行调度方案的修改结果一致，那么该调度方案就保证了一致性，如上图调度1执行后，A+B&#x3D;3000，调度2执行后，A+B&#x3D;3000，这样的调度等价于串行调度，被称为<strong>可串行化调度（serializable schedule）</strong>。</p><h2 id="Conflict-Serializability"><a href="#Conflict-Serializability" class="headerlink" title="Conflict Serializability"></a>Conflict Serializability</h2><p>但我们不能将并发事务的调度方案执行后，根据结果来判断这样的并发是否能保证数据库的一致性。需要采取其他办法判断。</p><p>这里先介绍冲突可串行化（conflict serializability）的概念。</p><p>冲突的定义为：假设，指令 I 和 J 属于不同的事务且对相同的数据执行操作，当 I 和 J 之中至少有一个是修改操作（write）时，I，J 冲突。</p><p>对于非冲突的指令，我们可以交换它们的顺序而不会影响调度最终的结果（如交换调度2中 T1 的 read(B) 和 T2 的 write(A)），而对于冲突的指令，交换它们则会产生影响。如果一个调度方案 S1 能够通过交换一系列不冲突指令后，变成调度方案 S2，那么 S1 和 S2 冲突等价（conflict equivalent），如果 S2 恰好是一个串行调度方案，那么 S1 就可以称为<strong>冲突可串行化</strong>（与串行调度冲突等价）。</p><p>由上可知，如果一个调度方案冲突可串行化，那么它能保证数据的一致性，现在问题是如何判断一个调度是否冲突可串行化？构造<strong>先行图（precedence graph）</strong>后可以很容易的判断。</p><p>先行图是一个有向图，其顶点表示并发的事务，假设在一个调度方案 S 中有 TA 和 TB 两个并发事务，Q 表示某个数据。当且仅当以下任何一种情况发生时，TA 到 TB 有一条边 （TA -&gt; TB）：</p><ul><li>TA 执行 write(Q) 之后 TB 会执行 read(Q)</li><li>TA 执行 read(Q) 之后 TB 会执行 write(Q)</li><li>TA 执行 write(Q) 之后 TB 会执行 write(Q)</li></ul><p>如果边 TA -&gt; TB 存在，那么在任何与 S 等价的串行调度方案中，TA 都要先于 TB 执行。</p><p><strong>假如某个调度方案的先行图含有环，那么，该调度就不是冲突可串行化的，若没有环，则该调度是冲突可串行化的</strong>。</p><p><font color=red>调度可串行化就万事大吉了吗？</font>上面讨论的内容都没有提到事务失败的情况，见下图：<br><img src="/img/schedule-serializable/schedule3.jpg" alt="schedule 3"></p><p>很明显，调度3是冲突可串行化的。如果 T6 执行完 read(B) 后提交前失败了，按照原子性定义，T6 需要回滚，需要注意，T7 读取的数据正是 T6 更新的数据，而 T6 回滚时，T7 已经提交了。</p><p>一个允许事务并发的系统，为保证原子性，若一个事务失败了，依赖于这个事务的其他事务（即这些事务读取了失败事务所做的更新）都需要失败终止和回滚。在调度3中，T7 依赖于 T6，所以 T7 也需要回滚，但已经不可能了。</p><p>像调度3这样的调度方案属于不可恢复调度（nonrecoverable schedule）。</p><p><strong>可恢复调度（recoverable schedule）</strong>是指，在一个调度中，对于任一两个事务 TA, TB，若 TA 读取了 TB 所做的更新，那么 TB 的提交操作必须出现在 TA 的<strong>提交</strong>操作之前。若 T7 的提交操作延迟到 T6 提交之后，那么调度3就是可恢复的。</p><p>为了提升性能，还需要介绍一种调度：cascadeless schedule (不知道怎么翻译，暂译作非失败连锁式调度)</p><p>考虑下面调度4的情况：<br><img src="/img/schedule-serializable/schedule4.jpg" alt="schedule 4"></p><p>调度4中，T9 依赖 T8，T10 依赖 T9，若 T8 失败终止，那么 T9 和 T10 都需要失败终止。调度4同时满足冲突可串行化和可恢复，但单个事务的失败引起太多的事务失败，这显然会降低系统的性能，这样的调度被称为失败连锁式调度（cascade schedule）。</p><p>这里给出 <strong>cascadeless schedule 的定义</strong>：在一个调度中，对于任一两个事务 TA, TB，若 TA 读取了 TB 所做的更新，那么 TB 的提交操作必须出现在 TA 的<strong>读</strong>操作之前，显然，所有的 cascadeless 调度都是可恢复的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>为了保证事务并发执行和单个按次序执行之间结果的一致性，提出了可串行化调度的定义。为了更加容易判断一个调度是否可串行化，引出了冲突可串行化的定义。可串行化调度能保证事务都成功执行后的一致性，但未能保证事务失败后的原子性，为了保证原子性，提出了可恢复调度的概念。再进一步，为了减少事务失败所引起的回滚操作，在可恢复之上再做限定，给出了 cascadeless 调度的定义。</p><p><font color=red>文首提到，并发很有好处，但它可能会破坏事务的隔离性，破坏数据的一致性，但通篇只提到了一致性，并没有涉及隔离性，难道是事务并发破坏了事务的隔离性？</p><p>假设系统初始处于一致状态，事务编写正确，即每个事务执行后，系统仍处于一致状态，那么事务的隔离性实际上就保证了：如果所有事务按照串行执行，最终系统仍处于一致性状态。</p><p>事务的一致性和隔离性的区别在哪？一致性需要满足系统初始处于一致状态，事务的逻辑正确，那么保证了隔离性就能保证事务的一致性。</p><p>再次回忆，可串行化实际上就是按照事务并发后其系统所处的状态是否和事务按串行顺序执行后的状态一致来定义的，也就是说，满足了可串行化，即满足了隔离性！！！</font></p><p>在文章 <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a>，会介绍采取何种方法，能生成满足这些定义的调度方案。</p>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇四：Log-Based Recovery System</title>
    <link href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/"/>
    <url>/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><img src="/img/log-based_recovery_system/1.png" alt="磁盘交互模型"><br>事务的原子性和一致性（数据库数据一致性）的保证，除了在事务代码的编写（逻辑）、事务的调度（并发）上面下功夫外，还需要考虑系统故障的发生。当系统从故障中恢复后，应当正确处理这些异常，以保证事务的原子性和数据的一致性。</p><p><font color=dark-green>一个例子</font></p><p>考虑银行有两个账户 A 和 B，初始账户分别为 1000 和 2000，一个事务负责从账户 A 转账 50 到账户 B。假设故障发生在事务执行过程中，那么有如下可能发生：</p><ol><li>A &#x3D; 950, B &#x3D; 2050</li><li>A &#x3D; 950, B &#x3D; 2000</li><li>A &#x3D; 1000, B &#x3D; 2050</li><li>A &#x3D; 1000, B &#x3D; 2000</li></ol><p>根据磁盘交互模型图可知，事务需要先将需要的数据从 buffer pool 读入自己的私有内存，等到操作完成再将更新后的数据写入 buffer pool，至于这些数据何时会写入磁盘与 buffer pool 页面置换算法有关。上面情况 1 表示一切正常；情况 2 表示含有 A 新值的 block 被正常写入磁盘后系统崩溃；情况 3 表示含有 B 新值的 block 被正常写入磁盘后系统崩溃；情况 4 表示两个 block 都还未写入磁盘就发生故障（当然 A 和 B 可能存在于同一个 block 中）。在情况 2 和 3 中数据库一致性都已被破坏，事务的原子性也未能保证。</p><h2 id="Log-Based-Recovery-System"><a href="#Log-Based-Recovery-System" class="headerlink" title="Log-Based Recovery System"></a>Log-Based Recovery System</h2><p>为了保证事务的原子性和数据的一致性，需要一种方法从故障中恢复被部分修改的数据，目前最广泛使用的方式是：<strong>在更新数据库之前，先将描述修改的信息记录到 stable storage 中的 log 文件里</strong>，等到系统重启后，根据这些信息来恢复数据。这些信息被称为 log records，这个方法被称作 log-based recovery.</p><h3 id="Log-Records-amp-事务流程"><a href="#Log-Records-amp-事务流程" class="headerlink" title="Log Records &amp; 事务流程"></a>Log Records &amp; 事务流程</h3><p>当事务开始时，会向日志中追加一条事务<strong>开始记录</strong> &lt;$T_i\ $start&gt;，$T_i$ 表示事务标识符；在将更新写入数据库之前，会向日志中追加一条<strong>更新记录</strong> &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt;，$X_i,\ V_1,\ V_2$ 分别表示数据标识符（通常用磁盘块号和偏移量）、该数据的旧值和新值；等到事务提交时，会向日志中追加一条<strong>提交记录</strong> &lt;$T_i\ $commit&gt;，如下面例子：<br><img src="/img/log-based_recovery_system/2.png" alt="图 2"><br>一旦这些记录被写入了日志文件中，系统就可以将事务的修改应用到数据库中了，就算故障发生，也能根据日志文件重放（replay）这些操作，恢复它们，正因为如此，日志文件必须持久化（写入 stable storage，定义见<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/" title="事务篇一：事务总论">事务篇一：事务总论</a>），以保证永久不丢失。需要注意，$T_0,\ T_1$ 的日志记录可能是相互交叉的。</p><p>现在在细节上更进一步，所谓的将记录追加到日志文件中、更新写入数据库中这些操作实际上包含两个步骤：</p><ol><li>将事务私有内存空间的数据写入某个 buffer block；</li><li>将 buffer block 写到磁盘上。</li></ol><p>第二步由数据库系统执行，发生的时机是不定的，<strong>所以当第一步发生后，就算是对文件和数据库做出了修改</strong>。需要明白，buffer block 中的数据可能会在内存中存在很长一段时间才会被写入磁盘（当然可以让第二步即时生效，但磁盘 I&#x2F;O 代价昂贵，非必须，一般是推迟写磁盘），在这段时间内系统完全有可能发生故障，所以事务提交流程中有一些<font color=red>原则（write-ahead logging, WAL, rule）必须要遵守，以保证事务的原子性</font>：</p><ul><li>事务已提交是指（可以回复客户端了），提交记录 &lt;$T_i\ $commit&gt; 已经被 <em>output</em> 到 stable storage 中。</li><li>在上一步执行之前，该事务的所有其他记录（存在于 log buffer 中）都被 <em>output</em> 到 stable storage 中。</li><li>在数据被 <em>output</em> 到数据库之前，与这些数据更新相关的所有日志记录都已经被 <em>output</em> 到 stable storage 中。</li></ul><p>只要遵守了以上原则，无论是系统崩溃后恢复还是事务的正常 abort，系统都可以根据这些记录将系统恢复到一致状态，事务自然也就保证了原子性。因此日志记录在 log 文件（stable storage）中的<strong>顺序十分重要</strong>，必须和写入 log buffer（buffer block）的顺序一致。</p><h3 id="Recovery-Algorithm"><a href="#Recovery-Algorithm" class="headerlink" title="Recovery Algorithm"></a>Recovery Algorithm</h3><p>在介绍恢复算法之前，先看看事务在正常情况下的流程，第一种是事务因失败需要 abort 回滚；第二种是事务正常提交。</p><p><strong>Case 1：正常 abort，事务回滚</strong>。当事务 $T_i$ 执行到一半时因为某些原因失败需要 abort 回滚（rollback 之后才能算 aborted）。回滚也就是将该事务对数据库所做的修改撤销，系统做如下操作（如图 2 示）：</p><ul><li>系统向后扫描 log 文件（注意 log 文件是只追加的，向前是指追加的方向），对于每一条属于 $T_i$ 的更新记录 &lt;$T_i,\ X_j,\ V_1,\ V_2$&gt;，系统将使用旧值 $V_1$ 更新 $X_i$ （即撤销修改），同时向日志追加一条 redo-only 记录 $&lt;T_i,\ X_j,\ V_1$&gt;。</li><li>当遇到记录 &lt;$T_i\ $start&gt;，系统向日志追加一条 &lt;$T_i\ $abort&gt; 记录，对于该事务的回滚也就结束了。</li></ul><p>系统回滚所添加的这些日志记录又被称为 compensation log records. 其实可以这样理解，事务中止处于未完成状态，系统创建一个互补事务，负责撤销事务所做的更改，并补充完整日志记录。</p><p><img src="/img/log-based_recovery_system/3.png" alt="图 3"></p><p><strong>Case 2: 事务正常提交</strong>。这种情况下事务不需要回滚，日志文件就像图 2 所示那样。</p><br><p><font color=dark-green>恢复算法思想</font><br>系统重启后是很懵逼的，它不清楚执行过的事务的具体情况，所以它需要查看 log 文件，根据日志记录的完整性，可以将事务分为两种：</p><ul><li><p><u>事务执行完成（committed 或 aborted）</u>。当系统重启后，发现某事务在 log 文件中包含了完整的日志记录，同时具有 &lt;$T_i\ $start&gt; 和 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt;。根据 WAL rule，尽管日志表明该事务已经完成，无论是事务自己正常提交还是系统所做的互补操作，但它不能确定这些操作已经写入数据库中（很可能系统在将 log buffer <em>output</em> 到 stable storage 之后就失败了，还未来得及将 data buffer <em>output</em> 到数据库），<strong>因此它必须依据日志 redo 这些操作</strong>。</p></li><li><p><u>事务执行未完成</u>。当系统重启后，发现某事务在 log 文件中没有包含完整的日志记录，即缺少了 &lt;$T_i\ $commit&gt; 或 &lt;$T_i\ $abort&gt; 记录，为保证事务的原子性和数据的一致性，该事务所做的操作必须撤销，<strong>也就是必须 undo 这些操作</strong>。当然，系统有可能在 rollback 未完成时就发生了故障，也就是日志中含有不完整的 redo-only 记录，undo 这些记录实际上就是 redo 它们。</p></li></ul><p>上面简单描述了恢复算法的的思想，很朴素，<font color=red>但却存在很大的性能问题</font>，试想在系统重启之前可能存在成千上百的事务执行大量的操作，这些操作都记录在了 log 文件中，log 文件可能变得十分巨大，若系统重启后对这些操作全都执行 redo 或 undo，系统可用性就会很差，何况，有很多操作已经实实在在写入了数据库，不需要在做额外的操作。为了减少恢复时间，数据库系统通常采用一种简单的方法，称作 checkpoints. </p><p><font color=dark-green>Checkpoints 思想</font></p><p><font color=dark-green>恢复算法详细步骤</font></p><br><p>总结以下，log record 有这些类型：</p><ul><li>&lt;$T_i,\ X_j,\ V_1,\ V_2$&gt;. 更新记录（update log record）</li><li>&lt;$T_i,\ X_j,\ V_2$&gt;. 回滚记录（redo only record）</li><li>&lt;$T_i\ $start&gt;. 事务开始记录（transaction start record）</li><li>&lt;$T_i\ $commit&gt;. 事务提交记录（transaction commit record）</li><li>&lt;$T_i\ $abort&gt;. 事务中止记录（transaction abort record）</li><li>&lt;checkpoint L&gt;. checkpoint record</li></ul>]]></content>
    
    
    <categories>
      
      <category>事务</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>事务篇一：事务总论</title>
    <link href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/"/>
    <url>/2022/06/22/%E4%BA%8B%E5%8A%A1%E6%80%BB%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文内容来自 database system concepts 第七版第十七章，主要是大概介绍事务的基本概念。更进一步的内容会引用其他文章，所以该文相当于一篇索引。</p><p><strong>单机事务部分（local transactions）：</strong></p><a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务篇二：事务的可串行化">事务篇二：事务的可串行化</a> <br /> <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务篇三：事务的并发控制">事务篇三：事务的并发控制</a> <br /><a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="事务篇四：Log-Based Recovery System">事务篇四：Log-Based Recovery System</a> <br /><br /><p><strong>分布式事务部分（global transactions）：</strong></p><a href="/2022/06/25/%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%E5%8D%8F%E8%AE%AE/" title="事务篇五：两阶段提交协议">事务篇五：两阶段提交协议</a> <br /><a href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/" title="事务篇六：Percolator 随笔">事务篇六：Percolator 随笔</a> <br /><br /><p><font color=red>分布式事务部分和单机事务部分的不同主要在于如何在多个节点保证事务的原子性。</font></p><h2 id="Transaction-Basic-Concept"><a href="#Transaction-Basic-Concept" class="headerlink" title="Transaction Basic Concept"></a>Transaction Basic Concept</h2><p>事务是一个逻辑单元，它包含了一组操作，这些操作可能访问或修改不同的数据。对用户来说，这组操作（事务）是一个单一的、不可分割的部分（比如，用户发起转账请求，这个请求对用户来说单一的操作，而在实际执行时分为多个步骤：安全性检查、出账、入账等），也就是说事务要么完成（其包含的所有步骤全部执行完成）或者失败（所有步骤皆失败），这个 all-or-none 特性被称为<font color=red>原子性（atomicity）</font>。</p><p>因为事务是一个不可分割的单元，所以它内含的步骤不能被数据库的其他操作分隔开。即使多个事务并发（concurrence），数据库系统也需要保证如此，即对于每两个事务 TA，TB，TA 要么开始于 TB 结束之后，要么结束于 TB 开始之前。这个特性被称为<font color=red>隔离性（isolation）</font>。</p><p>当事务执行完成提交后，它对数据库的修改不会丢失，即使系统从崩溃中恢复。这个特性被称为<font color=red>持久性（durability）</font>。</p><p>另外，事务还需要保证数据库的<font color=red>一致性（consistency）</font>，即，事务开始之前，数据库满足一致性，事务结束之后，数据库仍满足一致性。一致性的要求无法完成由数据库系统本身保证，它可能与上层逻辑有关（如出账入账，总账平衡）。</p><p>以上四个特性被简称位 ACID。</p><h2 id="Storage-Structure"><a href="#Storage-Structure" class="headerlink" title="Storage Structure"></a>Storage Structure</h2><p>这里需要清楚一些存储的概念，比如易失性存储（volatile storage）、非易失性存储（non-volatile storage）、稳定性存储（stable storage）。</p><p><strong>主要是 stable storage 的概念</strong>：只要数据写入 stable storage，那么它们就“永远”不可能丢失，要实现这样的存储，需要将数据备份（replicate）到多个非易失性存储介质上，这些非易失性存储分别独立，有自己的错误处理模块。<a href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/" title="关于多从节点如何安全的备份，参考分布式一致性协议 raft">关于多从节点如何安全的备份，参考分布式一致性协议 raft</a>。</p><h2 id="Transaction-Atomicity-and-Durability"><a href="#Transaction-Atomicity-and-Durability" class="headerlink" title="Transaction Atomicity and Durability"></a>Transaction Atomicity and Durability</h2><p><strong>需要明白，原子性的挑战在哪里？</strong></p><p>事务并不总是成功执行。结束的事务（terminated txn）有两种可能：</p><ul><li>committed，事务成功完成了所有操作，所有更新都已经写入了数据库；</li><li>aborted，事务因各种原因失败，事务造成的修改都已经恢复（rolled back），数据库回到了事务开始之前的状态。</li></ul><p>对失败的事务造成的修改如何恢复，是保证原子性的一个难点。通常采用日志的方式实现（事务对数据库的每一个修改都先写入日志中），维护日志不仅可以重新事务的修改操作（保证原子性和持久性），还能在事务失败后撤销修改，以保证原子性。数据库的 recovery system 负责保证原子性和持久性，<a href="/2022/06/22/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E7%B3%BB%E7%BB%9F/" title="参见文章：数据库恢复系统">参见文章：数据库恢复系统</a>。</p><p>对于，原子性和持久化还有一个部分需要注意：外部可见更新（observable external writes），如更新显示到屏幕、发送邮件或者网上购物等场景。如果事务半途中断，这些更新难以撤回。一般的解决方法是，先将更新存储到数据库的某个地方，等到事务提交后，在将这些更新应用到外部，另外，如果系统在事务提交后，应用更新到外部之前崩溃，那么等到系统重启后依然可以应用更新到外部。</p><h2 id="Transaction-Isolation"><a href="#Transaction-Isolation" class="headerlink" title="Transaction Isolation"></a>Transaction Isolation</h2><p>SQL 标准将隔离级别（isolation level）分为四类：</p><p><strong>可串行化-serializable</strong>，即事务之间的执行顺序可串行化，其结果等价于串行执行，能够保证数据库的一致性。该隔离级别允许一定程度的并发，属于最高的隔离级别。一般，数据库为了提升性能，实现时不会完全遵循其标准。关于串行化的定义，可以参见文章：<a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%8F%AF%E4%B8%B2%E8%A1%8C%E5%8C%96/" title="事务的串行化定义">事务的串行化定义</a>。</p><p><strong>可重复读-repeatable read</strong>，该级别规定了两点：1）只读已经提交了的数据；2）在事务执行期间多次读取一个数据之间，不允许其他任何事务更新该数据。该隔离级别保证了在<u>一个事务中，多次读取同一个数据，总会得到同样的值</u>。但注意，这里只是规定了不允许更新已存在的数据，对于其他事务插入新的数据却未做规定，这就导致了，遵循该级别的事务在两次读取中，有可能第二次读取的数据中有一些不存在于第一次读取中的新数据（共同拥有的数据还是相同的），即幻读，若使用快照隔离提供稳定的视图，则不会出现幻读。</p><p><strong>读已提交-read committed</strong>，该级别比上一级别更弱，由上一级别的叙述可知，遵循该级别的事务在两次读取同一数据，这两次的数据可能不同，因为其他事务在这段时间内更新了该数据，即不可重复读。</p><p><strong>读未提交-read uncommitted</strong>，这是最低的级别了，该级别甚至允许一个事务读取另外一个事务的中间结果，即脏读。</p><p>解释下读已提交&#x2F;未提交中的提交的含义：提交是指事务的提交。假设有两个事务： TA 读取数据 S，TB 修改数据 S。读未提交允许 TA 读取被 TB 修改了的数据 S，尽管 TB 还未提交。这里存在的可能隐患是，若 TB 失败终止了，所有修改都会回滚，也就是说，TA 读取到了无效的值。</p><p>区分下幻读和不可重复读现象的区别：幻读是指，本次读出的数据中，有一部分在之前读取的结果中不存在，幻读存在于范围读取中；不可重复读是指，本次读出的数据和之前的值不相等。</p><p>总结下，各种隔离级别可能发生的<font color=red>现象</font>：</p><table><thead><tr><th align="center"></th><th align="center">脏读</th><th align="center">幻读</th><th align="center">不可重复读</th><th align="center">脏写</th></tr></thead><tbody><tr><td align="center">可串行化</td><td align="center">禁止</td><td align="center">禁止</td><td align="center">禁止</td><td align="center">禁止</td></tr><tr><td align="center">可重复读</td><td align="center">禁止</td><td align="center">？</td><td align="center">禁止</td><td align="center">禁止</td></tr><tr><td align="center">读已提交</td><td align="center">禁止</td><td align="center">允许</td><td align="center">允许</td><td align="center">禁止</td></tr><tr><td align="center">读未提交</td><td align="center">允许</td><td align="center">允许</td><td align="center">允许</td><td align="center">禁止</td></tr></tbody></table><h2 id="Implementation-of-Isolation-Level"><a href="#Implementation-of-Isolation-Level" class="headerlink" title="Implementation of Isolation Level"></a>Implementation of Isolation Level</h2><p>该部分内容广且复杂，在文章 <a href="/2022/06/22/%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/" title="事务的并发控制中有详细介绍。">事务的并发控制中有详细介绍。</a></p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>事务</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>事务篇六：Percolator 随笔</title>
    <link href="/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/"/>
    <url>/2022/06/18/Percolator-%E9%9A%8F%E7%AC%94/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>事务</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式事务</tag>
      
      <tag>事务</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>记 CMU-15445 课程所得</title>
    <link href="/2022/06/16/%E8%AE%B0-CMU-15445-%E8%AF%BE%E7%A8%8B%E6%89%80%E5%BE%97/"/>
    <url>/2022/06/16/%E8%AE%B0-CMU-15445-%E8%AF%BE%E7%A8%8B%E6%89%80%E5%BE%97/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CMU-15445</tag>
      
      <tag>关系型数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TinyKV Snapshot 流程探秘</title>
    <link href="/2022/06/16/TinyKV-Snapshot-%E6%B5%81%E7%A8%8B/"/>
    <url>/2022/06/16/TinyKV-Snapshot-%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在写 TinyKV 时，有一个部分很难理解，那就 Snapshot 的收发过程。<br>为了防止内存中的日志条目无限扩张，Raft 会定时&#x2F;定量清理日志（如已经提交了的日志），一些节点可能由于是新加入或者网络等原因，其想要复制的日志已经被 leader 清理出内存了，此时，leader 会给该节点发送一份 Snapshot 使其快速跟上。在实现代码时，Raft 只是使用 Snapshot 的元数据来更新了一些状态，并没有涉及的日志的追加等操作，深感疑惑。而且，Snapshot 一般很大，虽然可以作为普通消息处理，但可能会阻塞正常的流程，所以对它的收发过程也很感兴趣。为了搞清楚这些问题，追踪代码调用，总算是搞清楚了。下面分为 Snapshot 的发送、接收、处理几个方面解密。</p><h2 id="Snapshot-流程总览"><a href="#Snapshot-流程总览" class="headerlink" title="Snapshot 流程总览"></a>Snapshot 流程总览</h2><p>这里先给出 snapshot 各个部分的流程示意图，下面会对各个部分详细分析<br><img src="/img/tinykv_snapshot/tinykv_arch.png" alt="TinyKV 的整体架构（代码层面）"><br><img src="/img/tinykv_snapshot/fig_for_create.png" alt="snapshot 创建流程"><br><img src="/img/tinykv_snapshot/fig_for_send.png" alt="snapshot 发送流程"><br><img src="/img/tinykv_snapshot/fig_for_recv.png" alt="snapshot 接收流程"><br><img src="/img/tinykv_snapshot/fig_for_apply.png" alt="snapshot 应用流程"></p><h2 id="Part-1：Snapshot-的创建"><a href="#Part-1：Snapshot-的创建" class="headerlink" title="Part 1：Snapshot 的创建"></a>Part 1：Snapshot 的创建</h2><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Raft)</span></span> sendAppend(to <span class="hljs-type">uint64</span>) &#123;<br>    term, err := r.RaftLog.Term(r.Prs[to].Next - <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// the peer left too far behind  (or newly join), </span><br>        <span class="hljs-comment">// send it a snapshot to catch-up</span><br>        r.trySendSnapshot(to)<br>        <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-comment">// something else</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Raft)</span></span> trySendSnapshot(to <span class="hljs-type">uint64</span>) &#123;<br>    snapshot, err := r.RaftLog.storage.snapshot()<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span><br>    &#125;<br><br>    r.msgs = <span class="hljs-built_in">append</span>(r.msgs, SnapshotMessage&#123;...&#125;)<br><br>&#125;<br></code></pre></td></tr></table></figure><p>第一个函数表明了 Raft 发送 Snapshot 的时机，第二个函数表明了 Snapshot 来自 storage。</p><blockquote><p>这个 storage 在 2A部分和之后的部分是不一样的，值得分析下。在 2A 中，storage 接口由 MemoryStorage 实现，这货存在于内存中，文档中写着，放入 storage 的日志是持久化的（stabled），当时很不理解，因为它也是存在内存中的啊，做到后面才发现，这里的 MemoryStorage 主要起着测试的作用，你只需要闭只眼假装它真的持久化了就行。而在后面的部分，storage 接口由 badger.DB（engines.Raft）实现，是实打实的写入磁盘。</p></blockquote><p>那么，调用 storage.snapshot() 实际做了什么？</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> Snapshot() (eraftpb.Snapshot, <span class="hljs-type">error</span>) &#123;<br>    <span class="hljs-keyword">var</span> snapshot eraftpb.Snapshot<br>    <span class="hljs-keyword">if</span> snapshot_is_generating &#123;<br>        snapshot &lt;- ps.snapState.Receiver<br>        <span class="hljs-keyword">return</span> snapshot, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// something else</span><br><br>    ch := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> *eraftpb.Snapshot, <span class="hljs-number">1</span>)<br>ps.snapState = snap.SnapState&#123;<br>StateType: snap.SnapState_Generating,<br>Receiver:  ch,<br>&#125;<br><span class="hljs-comment">// schedule snapshot generate task</span><br>ps.regionSched &lt;- &amp;runner.RegionTaskGen&#123;<br>RegionId: ps.region.GetId(),<br>Notifier: ch,<br>&#125;<br><br>    <span class="hljs-keyword">return</span> snapshot, raft.ErrSnapshotTemporarilyUnavailable<br>&#125;<br><br></code></pre></td></tr></table></figure><p>代码很清楚，如果当前正在生成 snapshot，那么就等待它生成完成并返回，否则，就创建一个 RegionTaskGen 任务发送给 regionSched 通道，并返回暂时不可用错误。那么是谁在接收该任务呢？</p><p>上面说到 Raft 调用生成 snapshot 的接口，该接口的实现（PeerStorage）会创建一个 RegionTaskGen 任务发送给 regionSched 通道。该通道的消费者实际上是 regionWorker：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(w *Worker)</span></span> Start(handler TaskHandler) &#123;<br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-keyword">for</span> &#123;<br>Task := &lt;-w.receiver<br><span class="hljs-keyword">if</span> _, ok := Task.(TaskStop); ok &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br>handler.Handle(Task)<br>&#125;<br>&#125;()<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *regionTaskHandler)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *RegionTaskGen:<br>task := t.(*RegionTaskGen)<br><span class="hljs-comment">// It is safe for now to handle generating and applying snapshot concurrently,</span><br><span class="hljs-comment">// but it may not when merge is implemented.</span><br>r.ctx.handleGen(task.RegionId, task.Notifier)<br><span class="hljs-keyword">case</span> *RegionTaskApply:<br>task := t.(*RegionTaskApply)<br>        <span class="hljs-comment">// apply received snapshot</span><br>r.ctx.handleApply(task.RegionId, task.Notifier, task.StartKey, task.EndKey, task.SnapMeta)<br><span class="hljs-keyword">case</span> *RegionTaskDestroy:<br>task := t.(*RegionTaskDestroy)<br>r.ctx.cleanUpRange(task.RegionId, task.StartKey, task.EndKey)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>regionWorker 创建一个协程，处理接收到的各种任务。其中有两种任务是本文需要关注的：<br>1）RegionTaskGen（生成 snapshot）<br>2）RegionTaskApply（应用从其他 peer 接收来的 snapshot）</p><p>继续看 RegionTaskGen 任务是如何执行的：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> handleGen(...) &#123;<br>snap, err := doSnapshot(...)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>notifier &lt;- <span class="hljs-literal">nil</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">// notify task done to task creator</span><br>notifier &lt;- snap<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">doSnapshot</span><span class="hljs-params">(...)</span></span> (*eraftpb.Snapshot, <span class="hljs-type">error</span>) &#123;<br>log.Debugf(<span class="hljs-string">&quot;begin to generate a snapshot. [regionId: %d]&quot;</span>, regionId)<br>    <span class="hljs-comment">// kvDB !!!</span><br>txn := engines.Kv.NewTransaction(<span class="hljs-literal">false</span>)<br>    <span class="hljs-comment">//...</span><br>err = s.Build(txn, ...)<br>    <span class="hljs-comment">//...</span><br><span class="hljs-keyword">return</span> snapshot, err<br>&#125;<br></code></pre></td></tr></table></figure><p>该 Build() 函数最终会调用 snapBuilder.build()，函数会扫描 PeerStorage.kvDB 的数据，创建一份快照：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(b *snapBuilder)</span></span> build() <span class="hljs-type">error</span> &#123;<br><span class="hljs-keyword">defer</span> b.txn.Discard()<br>startKey, endKey := b.region.StartKey, b.region.EndKey<br><br>    <span class="hljs-comment">// all data will store in b.cfFiles</span><br><span class="hljs-keyword">for</span> _, file := <span class="hljs-keyword">range</span> b.cfFiles &#123;<br>cf := file.CF<br>sstWriter := file.SstWriter<br><br>it := engine_util.NewCFIterator(cf, b.txn)<br><span class="hljs-keyword">for</span> it.Seek(startKey); it.Valid(); it.Next() &#123;<br>item := it.Item()<br>key := item.Key()<br><br>value, err := item.Value()<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br><br>cfKey := engine_util.KeyWithCF(cf, key)<br>            <span class="hljs-comment">// store data</span><br><span class="hljs-keyword">if</span> err := sstWriter.Add(cfKey, y.ValueStruct&#123;<br>Value: value,<br>&#125;); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br>it.Close()<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>自此，Snapshot 的创建过程分析完成</p><h2 id="Part-2：Snapshot-的发送"><a href="#Part-2：Snapshot-的发送" class="headerlink" title="Part 2：Snapshot 的发送"></a>Part 2：Snapshot 的发送</h2><p>当 sendAppend() 函数获取到创建的 snapshot 后，会将其封装在 pb.MessageType_MsgSnapshot 消息中，等待 RaftStorage 层调用 rawNode.Ready()：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(d *peerMsgHandler)</span></span> HandleRaftReady() &#123;<br><span class="hljs-comment">// Your Code Here (2B).</span><br>rd := d.RaftGroup.Ready()<br>d.sendMessageToPeers(rd.Messages)<br>    <span class="hljs-comment">// ...</span><br>d.RaftGroup.Advance(rd)<br>&#125;<br></code></pre></td></tr></table></figure><p>sendMessageToPeers() 最终会调用 WirteData() 函数：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *ServerTransport)</span></span> WriteData(...) &#123;<br><span class="hljs-keyword">if</span> msg.GetMessage().GetSnapshot() != <span class="hljs-literal">nil</span> &#123;<br>t.SendSnapshotSock(addr, msg)<br><span class="hljs-keyword">return</span><br>&#125;<br><span class="hljs-keyword">if</span> err := t.raftClient.Send(storeID, addr, msg); err != <span class="hljs-literal">nil</span> &#123;<br>log.Errorf(<span class="hljs-string">&quot;send raft msg err. err: %v&quot;</span>, err)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，在 WriteData 中对 snapshot 消息做了一个拦截，采用另外的方式单独处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(t *ServerTransport)</span></span> SendSnapshotSock(...) &#123;<br>t.snapScheduler &lt;- &amp;sendSnapTask&#123;<br>addr:     addr,<br>msg:      msg,<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>这下明白了，由于 snapshot 比较大，会采用分块传输，对它的发送操作与普通的消息分开，由 sendSnapTask 异步完成。<br>继续探寻该任务是如何被执行的，该任务被 snapWorker 接收，并调用 Handle() 处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *sendSnapTask:<br>r.send(t.(*sendSnapTask))<br><span class="hljs-keyword">case</span> *recvSnapTask:<br>r.recv(t.(*recvSnapTask))<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> send(t *sendSnapTask) &#123;<br>t.callback(r.sendSnap(t.addr, t.msg))<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *snapRunner)</span></span> sendSnap(...) <span class="hljs-type">error</span> &#123;<br>    <span class="hljs-comment">// ...</span><br>buf := <span class="hljs-built_in">make</span>([]<span class="hljs-type">byte</span>, snapChunkLen) <span class="hljs-comment">// snapChunkLen = 1024 * 1024</span><br><span class="hljs-keyword">for</span> remain := snap.TotalSize(); remain &gt; <span class="hljs-number">0</span>; remain -= <span class="hljs-type">uint64</span>(<span class="hljs-built_in">len</span>(buf)) &#123;<br>_, err := io.ReadFull(snap, buf)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> errors.Errorf(<span class="hljs-string">&quot;failed to read snapshot chunk: %v&quot;</span>, err)<br>&#125;<br>err = stream.Send(&amp;raft_serverpb.SnapshotChunk&#123;Data: buf&#125;)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到最终 snapshot 以 snapChunkLen 为单位分块发送出去的，后面的事情就是 gRPC 的工作了，探秘自此结束。 </p><h2 id="Part-3：Snapshot-的接收"><a href="#Part-3：Snapshot-的接收" class="headerlink" title="Part 3：Snapshot 的接收"></a>Part 3：Snapshot 的接收</h2><p>当使用 gRPC 发送 snapshot 时，对应 peer 也就进入了接收流程。上面提到的 snapWorker 也会处理接收操作，这里不再赘述。当所有的 snapshot 分块都接受完成后，就会给 raftWorker 监听的管道发送消息，最后调用 rawNode.Step() 让 raft 调用 handleSnapshot() 处理。 </p><h2 id="Part-4：应用来自其他-Peer-的-Snapshot"><a href="#Part-4：应用来自其他-Peer-的-Snapshot" class="headerlink" title="Part 4：应用来自其他 Peer 的 Snapshot"></a>Part 4：应用来自其他 Peer 的 Snapshot</h2><p>handleSnapshot() 接收到 snapshot 后只是更新了一些元数据，并将 snapshot 赋值给 pendingSnapshot，等待上层调用 Ready() 获取 pendingSnapshot：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(d *peerMsgHandler)</span></span> HandleRaftReady() &#123;<br><span class="hljs-comment">// Your Code Here (2B).</span><br>rd := d.RaftGroup.Ready()<br>applySnapResult, _ := d.peerStorage.SaveReadyState(&amp;rd)<br><span class="hljs-comment">//...</span><br>d.RaftGroup.Advance(rd)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> SaveReadyState(ready *raft.Ready) &#123;<br><span class="hljs-comment">// Your Code Here (2B/2C).</span><br>applySnapResult, err := ps.ApplySnapshot(&amp;ready.Snapshot, ...)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br>    <span class="hljs-comment">// ...</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ps *PeerStorage)</span></span> ApplySnapshot(snapshot *eraftpb.Snapshot, ...) &#123;<br>    <span class="hljs-comment">// ...</span><br><span class="hljs-comment">// send runner.RegionTaskApply task to region worker through </span><br>    <span class="hljs-comment">// PeerStorage.regionSched and</span><br><span class="hljs-comment">// wait until region worker finishes</span><br>ch := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-type">bool</span>, <span class="hljs-number">1</span>)<br>ps.regionSched &lt;- &amp;runner.RegionTaskApply&#123;<br>RegionId: snapData.Region.GetId(),<br>Notifier: ch,<br>SnapMeta: snapshot.Metadata,<br>StartKey: snapData.Region.StartKey,<br>EndKey:   snapData.Region.EndKey,<br>&#125;<br><br><span class="hljs-comment">// waiting</span><br>&lt;-ch<br>    <span class="hljs-keyword">return</span> ...<br>&#125;<br></code></pre></td></tr></table></figure><p>上面代码很明显了，上层获取到 Ready.Snapshot 后，会创建 RegionTaskApply 任务通过 regionSched 通道发送给 RegionWorker -&gt; handle() -&gt; handleApply() 处理：</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *regionTaskHandler)</span></span> Handle(t worker.Task) &#123;<br><span class="hljs-keyword">switch</span> t.(<span class="hljs-keyword">type</span>) &#123;<br><span class="hljs-keyword">case</span> *RegionTaskGen:<br>task := t.(*RegionTaskGen)<br><span class="hljs-comment">// It is safe for now to handle generating and applying snapshot concurrently,</span><br><span class="hljs-comment">// but it may not when merge is implemented.</span><br>r.ctx.handleGen(task.RegionId, task.Notifier)<br><span class="hljs-keyword">case</span> *RegionTaskApply:<br>task := t.(*RegionTaskApply)<br>r.ctx.handleApply(task.RegionId, task.Notifier, task.StartKey, task.EndKey, task.SnapMeta)<br><span class="hljs-keyword">case</span> *RegionTaskDestroy:<br>task := t.(*RegionTaskDestroy)<br>r.ctx.cleanUpRange(task.RegionId, task.StartKey, task.EndKey)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> handleApply(...) &#123;<br>err := snapCtx.applySnap(regionId, startKey, endKey, snapMeta)<br><span class="hljs-comment">// ...</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(snapCtx *snapContext)</span></span> applySnap(...) &#123;<br>applyOptions := snap.NewApplyOptions(snapCtx.engines.Kv, &amp;metapb.Region&#123;<br>Id:       regionId,<br>StartKey: startKey,<br>EndKey:   endKey,<br>&#125;)<br><span class="hljs-keyword">if</span> err := snapshot.Apply(*applyOptions); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，上面一步步调用，最后调用 snapshot.Apply()，注意这里传入的是 badger.kvDB。<br>snapshot.Apply() 和上面提到的 snapshot 创建过程的 snapBuilder.build() 执行的是相反的步骤，即，将 snapshot 中的内容写入到磁盘:</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs golang"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *Snap)</span></span> Apply(opts ApplyOptions) <span class="hljs-type">error</span> &#123;<br>externalFiles := <span class="hljs-built_in">make</span>([]*os.File, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(s.CFFiles))<br><span class="hljs-keyword">for</span> _, cfFile := <span class="hljs-keyword">range</span> s.CFFiles &#123;<br>file, err := os.Open(cfFile.Path)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>log.Errorf(<span class="hljs-string">&quot;open ingest file %s failed: %s&quot;</span>, cfFile.Path, err)<br><span class="hljs-keyword">return</span> err<br>&#125;<br>externalFiles = <span class="hljs-built_in">append</span>(externalFiles, file)<br>&#125;<br>    <span class="hljs-comment">// write to DB</span><br>n, err := opts.DB.IngestExternalFiles(externalFiles)<br>    <span class="hljs-comment">// ...</span><br>&#125;<br></code></pre></td></tr></table></figure><p>snapshot 的应用分析自此结束。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面的分析，可以得到以下信息：</p><ol><li>snapshot 的创建、发送、接收和处理都与 Raft 无关，它无需关系具体数据（除了元数据）。</li><li>snapshot 的发送和接收都采取了单独的 RPC 异步处理。</li><li>生成 snapshot 需要从 kvDB 中读取数据，然后返回给 Raft，最后通过 Ready 交给上层发送。</li><li>snapshot 接收后，需要先交给 Raft 更新一些元数据，然后通过 Ready 交给上层写到 kvDB 中。</li></ol>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TinyKV</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 算法问答录</title>
    <link href="/2022/06/14/raft-%E6%9D%82%E8%AE%B0/"/>
    <url>/2022/06/14/raft-%E6%9D%82%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前段时间学习了 CMU-15445 的课程，也写完了 project，了解了数据库内核的基本知识。这段时间在做 TinyKV，刚好看了 raft，细节很多，所以来总结下。</p><p>关于 raft 网上有很多资料：<br><a href="https://raft.github.io/raft.pdf">raft 小论文</a><br><a href="http://files.catwell.info/misc/mirror/2014-ongaro-raft-phd.pdf">raft 博士论文</a><br><a href="https://tanxinyu.work/raft/">raft 博客</a><br><a href="https://www.codedump.info/post/20180922-etcd-raft/">etcd raft 实现</a></p><p>所以，这里并不是对 raft 算法本身的细节记录，而是自己阅读、实现时的一些疑问和解答。</p><h2 id="Q1：raft算法解决了什么问题？"><a href="#Q1：raft算法解决了什么问题？" class="headerlink" title="Q1：raft算法解决了什么问题？"></a>Q1：raft算法解决了什么问题？</h2><p>raft 是一个分布式共识协议（算法），其主要作用是让集群中的节点对某件事情达成一致，如客户端发起更新请求，为了保证多个节点的数据状态一致，就需要让该更新请求在所有节点上都应用成功，否则更新请求失败。raft 算法可以看作一个黑匣子，当某个节点接收到客户端的请求后，首先将该请求交给 raft 模块，由 raft 模块负责节点间的协商，最后将结果返回给节点，节点再反馈客户端。如下图的复制状态机所示。</p><p><img src="/img/raft/rsm.png"></p><p>图中的 consensus module 就相当于 raft 黑匣子，state machine 可以理解为 kv 键值数据库。还需要注意两点：1）日志记录和协商同步进行；2）图中是分层的，表示多个客户端和集群节点。</p><h2 id="Q2：在实际实现中，整个分布式系统的流程如何？"><a href="#Q2：在实际实现中，整个分布式系统的流程如何？" class="headerlink" title="Q2：在实际实现中，整个分布式系统的流程如何？"></a>Q2：在实际实现中，整个分布式系统的流程如何？</h2><p>这个问题实际是关于</p><ol><li>集群节点如何与客户端交互？</li><li>集群节点如何与 raft 模块交互？</li><li>raft 模块如何与其他 raft 模块交互？</li></ol><p>要解答这些问题，需要借助现有的成熟的工业系统，比如 etcd、tikv 等等，因为比较熟悉 TinyKV，所以以它为例。</p><p>TinyKV 的设计参照了 etcd，将 raft 模块设计成独立的部分，raft 需要的网络、存储服务由上层（非 raft)提供，比较具有灵活性，具体流程如下：</p><ol><li>客户端向节点发送请求（put&#x2F;get&#x2F;delete…)</li><li>节点准备 WAL(write-ahead-log)</li><li>节点将请求封装成 entry 发送给 raft 模块</li><li>节点获取 raft 算法的输出，主要有一下部分：<br> 1）raft 需要存储的日志记录（unstable entry 和一些 raft 自身状态信息）<br> 2）committed entry（已经在大多数节点间达成一致的 entry）<br> 3）messages，需要发往其他 raft 模块的消息</li><li>节点获取到 raft 的输出后，按其类型做一些操作，对于 unstable entry 和状态信息，执行持久化操作；对于 committed entry，它们已处于安全状态，可以应用于数据库了，并且可以就这些 entry 中的请求向客户端反馈成功；对于messages，将其发送到对应的 raft 模块。完成以上操作后，通知 raft 模块。</li></ol><p><em>为什么 raft 会存储 unstable entry 和状态信息，比如 peers，是为了从崩溃中恢复</em></p><p><strong>总结一下</strong>，raft 模块被独立实现，其算法输入来自上层（这里的输入可能是客户端请求，也可能是其他 raft 模块的消息），其算法输出由上层负责处理（存储、发送等），从这里也能知道，raft 根本不关心 entry 中的具体请求，那是上层逻辑的责任，它只需要采取办法能够唯一标识一条 entry 即可（Term、Index）。</p><h2 id="Q3：当网络分区发生时，raft算法有什么表现？"><a href="#Q3：当网络分区发生时，raft算法有什么表现？" class="headerlink" title="Q3：当网络分区发生时，raft算法有什么表现？"></a>Q3：当网络分区发生时，raft算法有什么表现？</h2><p>该问题比较大，需要分类讨论：</p><ol><li>单个 follower 节点被隔离，恢复后，会发生什么？</li><li>网络分区发生时，leader 处在少数部分，恢复后会发生什么？</li><li>网络分区发生时，leader 处在多数部分，恢复后会发生什么？</li></ol><p><font color=red>对于第一个问题</font>，先看被隔离节点的表现：因为是 follower，它只能被动应答，在一段时间内没有异常发生。等到 election_timeout 后，它自增 Term，发起选举请求，由于网络问题，其他节点接收不到该请求，然后再次等到 election_timeout，再次自增 Term，发起选举请求……它重复该操作，直到从隔离中恢复。</p><p>该节点（记为 A）从隔离中恢复后，可能会先收到 leader 发来的 AppenEntriesRPC 或者 HeartBeatRPC，但 leader.Term &lt; A.Term，根据算法，这些 RPC 对 A 没有影响，但 leader 会受到影响，leader 会转变为 follower。集群中先超时的节点会率先发起选举请求，由于存在选举限制：<strong>要获取到大多数的选票，就必须具有最新的日志记录（通过比较最后一份日志的 Term 和 Index 判断）</strong>，这样的选举可能会持续多次，但无论如何节点 A 都不可能当选 leader，因为它被隔离，没有后面新追加的日志。也就是说，节点 A 的重新加入造成了系统不必要的抖动，其原因在于，节点 A 在隔离期间盲目地自增 Term。</p><p>etcd 是如何解决该问题的呢？采用 preVote 机制，即当一个节点超时后，它并不急于自增 Term，而是先发起选举请求，如果能获取到大多数的选票，再自增 Term 重新发起选举。</p><br><font color=red>对于第二个问题</font>，leader 处在少数节点分区部分，根据 raft 要求，一条 entry 能被提交，该entry 至少需要被 N/2 + 1 个节点安全复制，因此上层交付的任务 proposal 都无法被提交，自然无法被应用到数据库和反馈客户端，客户端会出现请求超时。后面的客户端请求可能会被路由到另外一个节点，直到请求能够被正常执行。<p>当网络分区恢复后，该 leader 会接受来自新 leader 的 RPC 请求，转换成 follower，开始正常的日志复制。该种情况下，是否会出现第一个问题中的场景呢？是有可能的，比如四个节点，每个分区中存在两个节点，包含 leader 的分区不会触发新的选举，但另外一个分区会发起多次的选举（或预选举），<strong>这种情况下，整个系统瘫痪，无法对外服务</strong>。</p><br><font color=red>对于第三个问题</font>，这种情况相对较简单，系统可正常对外服务，少数分区可能存在多次选举，但分区恢复后，可以开始正常的日志复制，具体过程在前两个问题中已经提及。<h2 id="Q4：leader-commit日志之前崩溃了，会发生什么？"><a href="#Q4：leader-commit日志之前崩溃了，会发生什么？" class="headerlink" title="Q4：leader commit日志之前崩溃了，会发生什么？"></a>Q4：leader commit日志之前崩溃了，会发生什么？</h2><p>该问题在 raft 论文中有论述，是关于如何处理前任 leader 复制的日志。<strong>我当时的疑问是</strong>：leader 将最新的日志复制到了一部分节点后，或许是还未满足大多数原则，或许是 commit 之前就崩溃了，这些最新的日志会被怎么处理？</p><p>后来再次仔细阅读论文，发现并理解了更加细微的点。借助论文中对该问题的讨论部分的图示，再来复盘一下：</p><p><img src="/img/raft/Q4_1.png"></p><p>图中方块中的数字标识 Term，上方的数字标识 Index。</p><ol><li><p>(a)中，S1 为 Term2 的 leader，在将 Index&#x3D;2 的日志复制到 S2 后崩溃。</p></li><li><p>(b)中，S1 崩溃后，S5 在 Term3 当选 leader（根据选举限制，在最后一条日志中，S5 具有更大的 Term，所以能够当选）</p></li><li><p>(c)中，S5 崩溃，S1 在 Term4 当选 leader，并继续复制日志，此时它将 Index&#x3D;2 的日志成功复制到了大多数节点上，但还未提交。</p></li><li><p>(d1) 和 (d2) 描述两种情况：<br> 第一种是(d1)，以前任期复制的日志（未提交）被后面新的日志覆盖。客户端等待响应超时，会重新发起请求（我之前还在担心，这会不会造成数据丢失，太天真了）。对应的是 S1 再次崩溃，在 (c) 的局面下，S5 再次当选 leader（图中未画出新的任期，S5 可以获得 S2-4 的选票），由于复制日志以 leader 的日志为准，所以Index&#x3D;2的以前任期的日志会被 S5 的 Index&#x3D;2 的日志覆盖。</p><p> 第二种是(d2)，以前任期复制的日志可以被后面新的 leader 提交（属于被动提交，因为 raft 中，提交一条日志，就表示该条日志之前的所有日志都已被提交）。对应的是，S1 在崩溃之前将日志复制到了大多数节点上，此时 S5 已经不可能再当选，新的 leader 只能在 S1-3 之中。假设，S1 未崩溃，那么，S1 通过提交 Index&#x3D;3 的日志，之前的日志也就一起被动提交了；就算 S1 在提交之前崩溃了，新的 leader 通过提交当前任期的日志也能提交以前任期的所有日志。</p></li></ol><p><strong>从这里，应当认识到两点</strong>：<br>1）复制的日志可能会被覆盖，客户端会重试。<br>2）raft 算法中，leader 会强制要求其他节点的日志与自己一致，对安全性的考虑应该结合选举限制一起理解。</p>]]></content>
    
    
    <categories>
      
      <category>分布式</category>
      
    </categories>
    
    
    <tags>
      
      <tag>raft</tag>
      
      <tag>分布式一致性协议</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo 博客使用记录</title>
    <link href="/2022/06/13/Hexo%20%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
    <url>/2022/06/13/Hexo%20%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>终于搬新博客了</p><h3 id="Record-1：文章内部图片死活加载不出来"><a href="#Record-1：文章内部图片死活加载不出来" class="headerlink" title="Record 1：文章内部图片死活加载不出来"></a>Record 1：文章内部图片死活加载不出来</h3><p>刚使用 fluid 主题时，文章封面图片可以设置，但文章内的图片无法显示，markdown 语法和 HTML 语法都无济于事。最后死马当活马医，将hexo-asset-image卸载了，重试，居然可以了。</p><h3 id="Record-2：图片存放问题"><a href="#Record-2：图片存放问题" class="headerlink" title="Record 2：图片存放问题"></a>Record 2：图片存放问题</h3><p>有两个文件夹可以存放：&#x2F;source&#x2F;img 和 &#x2F;public&#x2F;img，但图片不能放在后者中，因为 &#x2F;public 目录下的东西都是执行 hexo g 生成的（从 &#x2F;source 目录获取相关内容），若执行 hexo clean 就全部被删除了。</p><h3 id="Record-3-引用本地文章"><a href="#Record-3-引用本地文章" class="headerlink" title="Record 3: 引用本地文章"></a>Record 3: 引用本地文章</h3><p>使用这样的格式 (明白为什么要用图片吧 -_- )：<br><img src="/img/usage-records/cite-local-paper.png"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
